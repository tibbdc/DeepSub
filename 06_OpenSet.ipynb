{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 13:25:32.302124: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-22 13:25:32.302582: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-22 13:25:32.302725: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-22 13:25:32.332549: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 13:25:34.598076: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from io import StringIO\n",
    "import requests\n",
    "import esm\n",
    "from trainer import reshape_features\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tool import model as md\n",
    "from tool.att import Attention\n",
    "from tool import config as cfg\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "from featurizer import set_model, get_rep_seq\n",
    "model,batch_converter,alphabet = set_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./DATA/openset.csv\")\n",
    "data = data[data[\"seq\"]!=\"-\"]\n",
    "data = data[data[\"comple_portal_num\"].isin([1,2,3,4,5,6,7,8,10,12])]\n",
    "data.comple_portal_num.value_counts()\n",
    "data.seq = data.seq.apply(lambda x:x[:1000])\n",
    "df_data = list(zip(data.index,data.seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Embedding\n",
    "# stride = 20\n",
    "# num_iterations = len(df_data) // stride\n",
    "# if len(df_data) % stride != 0:\n",
    "#     num_iterations += 1\n",
    "    \n",
    "# all_results = pd.DataFrame()\n",
    "# all_emb = []\n",
    "# for i in tqdm(range(num_iterations)):\n",
    "#     start = i * stride\n",
    "#     end = start + stride\n",
    "\n",
    "#     current_data = df_data[start:end]\n",
    "#     rep33 = get_rep_seq(current_data,model,batch_converter,alphabet)\n",
    "#     all_emb.append(rep33.values)\n",
    "# data['emb'] = np.concatenate(all_emb).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_feather(\"DATA/openset_with_feat.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th>comple_portal_num</th>\n",
       "      <th>uniprot_num</th>\n",
       "      <th>Subunit_structure</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A1D5P2B5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>SUBUNIT: Homodimer. Heterodimer; with a rar mo...</td>\n",
       "      <td>MDTKHFLPLDFSNQVNSTSLNSPTSRGPMATPSLHPSIGPGIGSSL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A287A1S6</td>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>MFSFVDLRLLLLLAATALLTHGQEEGQEEGQQGQEEDIPPVTCVQN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A2P9E102</td>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>MMPHPIQDAVLRVDRLSVVYPGGVTALRDTSIAFRRGEFTVLLGLS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A5B9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Alpha-beta TR is a heterodimer composed of an ...</td>\n",
       "      <td>DLKNVFPPKVAVFEPSEAEISHTQKATLVCLATGFYPDHVELSWWV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A0A8C0MYN9</td>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>MTSFVQKGTWLLLALLQPAVISAQQQAIDGGCSHLGQSYADRDVWK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3669</th>\n",
       "      <td>Q9Z266</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Component of the biogenesis of lysosome-relate...</td>\n",
       "      <td>MAAAGSAAVSGAGTPVAGPTGRDLFAEGLLEFLRPAVQQLDSHVHA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3670</th>\n",
       "      <td>Q9Z321</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>MKTVLMVAEKPSLAQSIAKILSRGNMSSHKGLNGACSVHKYTGTFA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3671</th>\n",
       "      <td>Q9ZZW7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Forms a ribonucleoprotein complex composed of ...</td>\n",
       "      <td>MAFRKSNVYLSLVNSYIIDSPQPSSINYWWNMGSLLGLCLVIQIVT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3672</th>\n",
       "      <td>V6CLJ5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Component of a complex consisting of at least ...</td>\n",
       "      <td>MFSWRRGDCESQKEENRSEERKGEETIRFPTRSPFHCVLFLLTDGF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3673</th>\n",
       "      <td>W5NTT7</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>MLSFVDTRTLLLLAVTSCLATCQSLQEATARKGPSGDRGPRGERGP...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3628 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           entry  comple_portal_num uniprot_num  \\\n",
       "1     A0A1D5P2B5                  1           2   \n",
       "2     A0A287A1S6                  2           -   \n",
       "3     A0A2P9E102                  2           -   \n",
       "4         A0A5B9                  1           0   \n",
       "5     A0A8C0MYN9                  3           -   \n",
       "...          ...                ...         ...   \n",
       "3669      Q9Z266                  1           0   \n",
       "3670      Q9Z321                  1           -   \n",
       "3671      Q9ZZW7                  1           0   \n",
       "3672      V6CLJ5                  1           0   \n",
       "3673      W5NTT7                  1           -   \n",
       "\n",
       "                                      Subunit_structure  \\\n",
       "1     SUBUNIT: Homodimer. Heterodimer; with a rar mo...   \n",
       "2                                                     -   \n",
       "3                                                     -   \n",
       "4     Alpha-beta TR is a heterodimer composed of an ...   \n",
       "5                                                     -   \n",
       "...                                                 ...   \n",
       "3669  Component of the biogenesis of lysosome-relate...   \n",
       "3670                                                  -   \n",
       "3671  Forms a ribonucleoprotein complex composed of ...   \n",
       "3672  Component of a complex consisting of at least ...   \n",
       "3673                                                  -   \n",
       "\n",
       "                                                    seq  \n",
       "1     MDTKHFLPLDFSNQVNSTSLNSPTSRGPMATPSLHPSIGPGIGSSL...  \n",
       "2     MFSFVDLRLLLLLAATALLTHGQEEGQEEGQQGQEEDIPPVTCVQN...  \n",
       "3     MMPHPIQDAVLRVDRLSVVYPGGVTALRDTSIAFRRGEFTVLLGLS...  \n",
       "4     DLKNVFPPKVAVFEPSEAEISHTQKATLVCLATGFYPDHVELSWWV...  \n",
       "5     MTSFVQKGTWLLLALLQPAVISAQQQAIDGGCSHLGQSYADRDVWK...  \n",
       "...                                                 ...  \n",
       "3669  MAAAGSAAVSGAGTPVAGPTGRDLFAEGLLEFLRPAVQQLDSHVHA...  \n",
       "3670  MKTVLMVAEKPSLAQSIAKILSRGNMSSHKGLNGACSVHKYTGTFA...  \n",
       "3671  MAFRKSNVYLSLVNSYIIDSPQPSSINYWWNMGSLLGLCLVIQIVT...  \n",
       "3672  MFSWRRGDCESQKEENRSEERKGEETIRFPTRSPFHCVLFLLTDGF...  \n",
       "3673  MLSFVDTRTLLLLAVTSCLATCQSLQEATARKGPSGDRGPRGERGP...  \n",
       "\n",
       "[3628 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2902, 1, 1280)\n",
      "X_val shape: (726, 1, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 13:26:22.829941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10565 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:73:00.0, compute capability: 8.6\n",
      "2024-02-22 13:26:23.367530: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 13:26:33.041649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2024-02-22 13:26:33.461063: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fdf9c00df30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-22 13:26:33.461101: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2024-02-22 13:26:33.469446: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-02-22 13:26:33.715984: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 12s 1s/step - loss: 2.1270 - accuracy: 0.3022 - val_loss: 1.4922 - val_accuracy: 0.7534\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 1.4779 - accuracy: 0.7536 - val_loss: 1.1201 - val_accuracy: 0.7534\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 1.1400 - accuracy: 0.7536 - val_loss: 1.0194 - val_accuracy: 0.7534\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 1.0325 - accuracy: 0.7536 - val_loss: 0.9900 - val_accuracy: 0.7534\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.9879 - accuracy: 0.7536 - val_loss: 0.9432 - val_accuracy: 0.7534\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.9238 - accuracy: 0.7536 - val_loss: 0.8846 - val_accuracy: 0.7534\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.8790 - accuracy: 0.7536 - val_loss: 0.8597 - val_accuracy: 0.7534\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.8573 - accuracy: 0.7519 - val_loss: 0.8770 - val_accuracy: 0.7507\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.8826 - accuracy: 0.7409 - val_loss: 0.8730 - val_accuracy: 0.7534\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.8676 - accuracy: 0.7481 - val_loss: 0.8459 - val_accuracy: 0.7548\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.8389 - accuracy: 0.7564 - val_loss: 0.8306 - val_accuracy: 0.7534\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.8217 - accuracy: 0.7553 - val_loss: 0.8294 - val_accuracy: 0.7534\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.8147 - accuracy: 0.7543 - val_loss: 0.8248 - val_accuracy: 0.7534\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.8151 - accuracy: 0.7536 - val_loss: 0.8105 - val_accuracy: 0.7534\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.8010 - accuracy: 0.7553 - val_loss: 0.7942 - val_accuracy: 0.7534\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.7848 - accuracy: 0.7571 - val_loss: 0.7854 - val_accuracy: 0.7534\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.7849 - accuracy: 0.7619 - val_loss: 0.7824 - val_accuracy: 0.7576\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.7767 - accuracy: 0.7653 - val_loss: 0.7785 - val_accuracy: 0.7576\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 178ms/step - loss: 0.7739 - accuracy: 0.7622 - val_loss: 0.7733 - val_accuracy: 0.7576\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.7623 - accuracy: 0.7636 - val_loss: 0.7698 - val_accuracy: 0.7576\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.7558 - accuracy: 0.7626 - val_loss: 0.7687 - val_accuracy: 0.7576\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.7555 - accuracy: 0.7657 - val_loss: 0.7673 - val_accuracy: 0.7590\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.7508 - accuracy: 0.7636 - val_loss: 0.7636 - val_accuracy: 0.7590\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.7475 - accuracy: 0.7636 - val_loss: 0.7587 - val_accuracy: 0.7645\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 190ms/step - loss: 0.7441 - accuracy: 0.7688 - val_loss: 0.7553 - val_accuracy: 0.7562\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.7381 - accuracy: 0.7681 - val_loss: 0.7523 - val_accuracy: 0.7562\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.7311 - accuracy: 0.7719 - val_loss: 0.7488 - val_accuracy: 0.7576\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.7308 - accuracy: 0.7715 - val_loss: 0.7458 - val_accuracy: 0.7590\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.7275 - accuracy: 0.7677 - val_loss: 0.7428 - val_accuracy: 0.7576\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.7223 - accuracy: 0.7722 - val_loss: 0.7389 - val_accuracy: 0.7576\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.7097 - accuracy: 0.7733 - val_loss: 0.7346 - val_accuracy: 0.7603\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.7058 - accuracy: 0.7764 - val_loss: 0.7312 - val_accuracy: 0.7617\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.7019 - accuracy: 0.7771 - val_loss: 0.7281 - val_accuracy: 0.7617\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6979 - accuracy: 0.7746 - val_loss: 0.7253 - val_accuracy: 0.7631\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.7040 - accuracy: 0.7760 - val_loss: 0.7224 - val_accuracy: 0.7603\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.6901 - accuracy: 0.7822 - val_loss: 0.7198 - val_accuracy: 0.7617\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 265ms/step - loss: 0.6884 - accuracy: 0.7805 - val_loss: 0.7171 - val_accuracy: 0.7603\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6842 - accuracy: 0.7843 - val_loss: 0.7144 - val_accuracy: 0.7631\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6831 - accuracy: 0.7802 - val_loss: 0.7117 - val_accuracy: 0.7645\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.6696 - accuracy: 0.7802 - val_loss: 0.7092 - val_accuracy: 0.7645\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.6716 - accuracy: 0.7829 - val_loss: 0.7067 - val_accuracy: 0.7658\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 182ms/step - loss: 0.6677 - accuracy: 0.7833 - val_loss: 0.7036 - val_accuracy: 0.7631\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6640 - accuracy: 0.7853 - val_loss: 0.6998 - val_accuracy: 0.7631\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6594 - accuracy: 0.7932 - val_loss: 0.6964 - val_accuracy: 0.7658\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.6559 - accuracy: 0.7884 - val_loss: 0.6934 - val_accuracy: 0.7672\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.6449 - accuracy: 0.7919 - val_loss: 0.6909 - val_accuracy: 0.7658\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6489 - accuracy: 0.7908 - val_loss: 0.6882 - val_accuracy: 0.7672\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.6456 - accuracy: 0.7884 - val_loss: 0.6845 - val_accuracy: 0.7700\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.6375 - accuracy: 0.7926 - val_loss: 0.6815 - val_accuracy: 0.7769\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6335 - accuracy: 0.7950 - val_loss: 0.6791 - val_accuracy: 0.7769\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6357 - accuracy: 0.7960 - val_loss: 0.6769 - val_accuracy: 0.7782\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6315 - accuracy: 0.7998 - val_loss: 0.6753 - val_accuracy: 0.7782\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6225 - accuracy: 0.7929 - val_loss: 0.6730 - val_accuracy: 0.7796\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.6181 - accuracy: 0.7981 - val_loss: 0.6704 - val_accuracy: 0.7810\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.6204 - accuracy: 0.7967 - val_loss: 0.6681 - val_accuracy: 0.7824\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 176ms/step - loss: 0.6157 - accuracy: 0.7960 - val_loss: 0.6662 - val_accuracy: 0.7796\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.6033 - accuracy: 0.8015 - val_loss: 0.6640 - val_accuracy: 0.7796\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6047 - accuracy: 0.7991 - val_loss: 0.6613 - val_accuracy: 0.7796\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6010 - accuracy: 0.7981 - val_loss: 0.6576 - val_accuracy: 0.7824\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5977 - accuracy: 0.8036 - val_loss: 0.6552 - val_accuracy: 0.7824\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5940 - accuracy: 0.8046 - val_loss: 0.6528 - val_accuracy: 0.7865\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5916 - accuracy: 0.8050 - val_loss: 0.6515 - val_accuracy: 0.7851\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.5909 - accuracy: 0.8036 - val_loss: 0.6475 - val_accuracy: 0.7865\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5858 - accuracy: 0.8053 - val_loss: 0.6443 - val_accuracy: 0.7865\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5813 - accuracy: 0.8050 - val_loss: 0.6416 - val_accuracy: 0.7879\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5720 - accuracy: 0.8032 - val_loss: 0.6402 - val_accuracy: 0.7893\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5678 - accuracy: 0.8074 - val_loss: 0.6404 - val_accuracy: 0.7879\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5673 - accuracy: 0.8077 - val_loss: 0.6357 - val_accuracy: 0.7906\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5631 - accuracy: 0.8119 - val_loss: 0.6322 - val_accuracy: 0.7851\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.5583 - accuracy: 0.8074 - val_loss: 0.6292 - val_accuracy: 0.7865\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.5569 - accuracy: 0.8122 - val_loss: 0.6287 - val_accuracy: 0.7920\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5507 - accuracy: 0.8101 - val_loss: 0.6257 - val_accuracy: 0.7920\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5540 - accuracy: 0.8094 - val_loss: 0.6211 - val_accuracy: 0.7906\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5496 - accuracy: 0.8181 - val_loss: 0.6187 - val_accuracy: 0.7893\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5456 - accuracy: 0.8143 - val_loss: 0.6177 - val_accuracy: 0.7934\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5526 - accuracy: 0.8063 - val_loss: 0.6168 - val_accuracy: 0.7920\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5457 - accuracy: 0.8115 - val_loss: 0.6123 - val_accuracy: 0.7893\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.5353 - accuracy: 0.8232 - val_loss: 0.6099 - val_accuracy: 0.7879\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.5340 - accuracy: 0.8222 - val_loss: 0.6104 - val_accuracy: 0.7961\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5342 - accuracy: 0.8184 - val_loss: 0.6116 - val_accuracy: 0.7948\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.5279 - accuracy: 0.8222 - val_loss: 0.6047 - val_accuracy: 0.7920\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5318 - accuracy: 0.8156 - val_loss: 0.6042 - val_accuracy: 0.7948\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5179 - accuracy: 0.8256 - val_loss: 0.6012 - val_accuracy: 0.7920\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5252 - accuracy: 0.8218 - val_loss: 0.6022 - val_accuracy: 0.7961\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5118 - accuracy: 0.8198 - val_loss: 0.5969 - val_accuracy: 0.7989\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 207ms/step - loss: 0.5224 - accuracy: 0.8191 - val_loss: 0.5938 - val_accuracy: 0.7920\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5059 - accuracy: 0.8236 - val_loss: 0.5936 - val_accuracy: 0.7961\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5012 - accuracy: 0.8218 - val_loss: 0.5942 - val_accuracy: 0.7961\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4937 - accuracy: 0.8301 - val_loss: 0.5919 - val_accuracy: 0.7948\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5070 - accuracy: 0.8236 - val_loss: 0.5898 - val_accuracy: 0.7948\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.4935 - accuracy: 0.8256 - val_loss: 0.5880 - val_accuracy: 0.7975\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4950 - accuracy: 0.8291 - val_loss: 0.5865 - val_accuracy: 0.7961\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.4895 - accuracy: 0.8332 - val_loss: 0.5841 - val_accuracy: 0.8017\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4902 - accuracy: 0.8329 - val_loss: 0.5838 - val_accuracy: 0.7989\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.4911 - accuracy: 0.8274 - val_loss: 0.5824 - val_accuracy: 0.7989\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4917 - accuracy: 0.8291 - val_loss: 0.5803 - val_accuracy: 0.7948\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4874 - accuracy: 0.8353 - val_loss: 0.5795 - val_accuracy: 0.7934\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4877 - accuracy: 0.8325 - val_loss: 0.5797 - val_accuracy: 0.7989\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4716 - accuracy: 0.8380 - val_loss: 0.5805 - val_accuracy: 0.7989\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4822 - accuracy: 0.8374 - val_loss: 0.5743 - val_accuracy: 0.8058\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.4829 - accuracy: 0.8301 - val_loss: 0.5726 - val_accuracy: 0.8017\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4745 - accuracy: 0.8336 - val_loss: 0.5746 - val_accuracy: 0.8030\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4709 - accuracy: 0.8336 - val_loss: 0.5741 - val_accuracy: 0.8044\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4747 - accuracy: 0.8391 - val_loss: 0.5700 - val_accuracy: 0.8017\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.4549 - accuracy: 0.8398 - val_loss: 0.5686 - val_accuracy: 0.8044\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4681 - accuracy: 0.8429 - val_loss: 0.5680 - val_accuracy: 0.8044\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.4575 - accuracy: 0.8456 - val_loss: 0.5677 - val_accuracy: 0.8030\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 217ms/step - loss: 0.4699 - accuracy: 0.8329 - val_loss: 0.5660 - val_accuracy: 0.8058\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4662 - accuracy: 0.8374 - val_loss: 0.5632 - val_accuracy: 0.8072\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.4595 - accuracy: 0.8363 - val_loss: 0.5609 - val_accuracy: 0.8099\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4506 - accuracy: 0.8460 - val_loss: 0.5598 - val_accuracy: 0.8127\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4552 - accuracy: 0.8398 - val_loss: 0.5588 - val_accuracy: 0.8127\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.4461 - accuracy: 0.8377 - val_loss: 0.5582 - val_accuracy: 0.8140\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.4449 - accuracy: 0.8501 - val_loss: 0.5580 - val_accuracy: 0.8113\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4459 - accuracy: 0.8384 - val_loss: 0.5569 - val_accuracy: 0.8113\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4429 - accuracy: 0.8439 - val_loss: 0.5555 - val_accuracy: 0.8113\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4633 - accuracy: 0.8349 - val_loss: 0.5553 - val_accuracy: 0.8127\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4458 - accuracy: 0.8477 - val_loss: 0.5539 - val_accuracy: 0.8127\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.4452 - accuracy: 0.8391 - val_loss: 0.5517 - val_accuracy: 0.8127\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 174ms/step - loss: 0.4449 - accuracy: 0.8418 - val_loss: 0.5510 - val_accuracy: 0.8085\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4452 - accuracy: 0.8473 - val_loss: 0.5531 - val_accuracy: 0.8099\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4406 - accuracy: 0.8432 - val_loss: 0.5536 - val_accuracy: 0.8085\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4385 - accuracy: 0.8446 - val_loss: 0.5463 - val_accuracy: 0.8154\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4313 - accuracy: 0.8442 - val_loss: 0.5448 - val_accuracy: 0.8196\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4410 - accuracy: 0.8487 - val_loss: 0.5499 - val_accuracy: 0.8113\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 170ms/step - loss: 0.4312 - accuracy: 0.8480 - val_loss: 0.5512 - val_accuracy: 0.8099\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4280 - accuracy: 0.8460 - val_loss: 0.5465 - val_accuracy: 0.8140\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4301 - accuracy: 0.8522 - val_loss: 0.5472 - val_accuracy: 0.8127\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.4232 - accuracy: 0.8529 - val_loss: 0.5567 - val_accuracy: 0.8085\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.4206 - accuracy: 0.8501 - val_loss: 0.5482 - val_accuracy: 0.8154\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.4274 - accuracy: 0.8415 - val_loss: 0.5426 - val_accuracy: 0.8113\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4184 - accuracy: 0.8563 - val_loss: 0.5425 - val_accuracy: 0.8154\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4162 - accuracy: 0.8535 - val_loss: 0.5517 - val_accuracy: 0.8113\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4180 - accuracy: 0.8487 - val_loss: 0.5466 - val_accuracy: 0.8127\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 223ms/step - loss: 0.4235 - accuracy: 0.8498 - val_loss: 0.5444 - val_accuracy: 0.8099\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 200ms/step - loss: 0.4219 - accuracy: 0.8473 - val_loss: 0.5430 - val_accuracy: 0.8196\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4169 - accuracy: 0.8511 - val_loss: 0.5461 - val_accuracy: 0.8099\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4179 - accuracy: 0.8573 - val_loss: 0.5416 - val_accuracy: 0.8127\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4102 - accuracy: 0.8522 - val_loss: 0.5380 - val_accuracy: 0.8209\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4130 - accuracy: 0.8522 - val_loss: 0.5395 - val_accuracy: 0.8196\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4088 - accuracy: 0.8508 - val_loss: 0.5432 - val_accuracy: 0.8154\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.4117 - accuracy: 0.8473 - val_loss: 0.5470 - val_accuracy: 0.8127\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.4048 - accuracy: 0.8532 - val_loss: 0.5418 - val_accuracy: 0.8154\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.3935 - accuracy: 0.8529 - val_loss: 0.5361 - val_accuracy: 0.8237\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4078 - accuracy: 0.8570 - val_loss: 0.5346 - val_accuracy: 0.8237\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3982 - accuracy: 0.8573 - val_loss: 0.5382 - val_accuracy: 0.8168\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4175 - accuracy: 0.8518 - val_loss: 0.5349 - val_accuracy: 0.8237\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4011 - accuracy: 0.8580 - val_loss: 0.5364 - val_accuracy: 0.8196\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4072 - accuracy: 0.8508 - val_loss: 0.5422 - val_accuracy: 0.8168\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.3956 - accuracy: 0.8560 - val_loss: 0.5432 - val_accuracy: 0.8154\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.3912 - accuracy: 0.8549 - val_loss: 0.5363 - val_accuracy: 0.8182\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4005 - accuracy: 0.8525 - val_loss: 0.5329 - val_accuracy: 0.8182\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.3847 - accuracy: 0.8591 - val_loss: 0.5396 - val_accuracy: 0.8168\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3925 - accuracy: 0.8560 - val_loss: 0.5408 - val_accuracy: 0.8209\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.3918 - accuracy: 0.8625 - val_loss: 0.5327 - val_accuracy: 0.8223\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3858 - accuracy: 0.8642 - val_loss: 0.5334 - val_accuracy: 0.8237\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.3854 - accuracy: 0.8604 - val_loss: 0.5383 - val_accuracy: 0.8182\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.3799 - accuracy: 0.8577 - val_loss: 0.5283 - val_accuracy: 0.8223\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.3860 - accuracy: 0.8632 - val_loss: 0.5274 - val_accuracy: 0.8264\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3971 - accuracy: 0.8522 - val_loss: 0.5334 - val_accuracy: 0.8168\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3753 - accuracy: 0.8584 - val_loss: 0.5373 - val_accuracy: 0.8113\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.3760 - accuracy: 0.8646 - val_loss: 0.5291 - val_accuracy: 0.8264\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.3918 - accuracy: 0.8635 - val_loss: 0.5279 - val_accuracy: 0.8251\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.3825 - accuracy: 0.8587 - val_loss: 0.5307 - val_accuracy: 0.8196\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.3822 - accuracy: 0.8642 - val_loss: 0.5336 - val_accuracy: 0.8209\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.3749 - accuracy: 0.8625 - val_loss: 0.5260 - val_accuracy: 0.8223\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.3788 - accuracy: 0.8649 - val_loss: 0.5286 - val_accuracy: 0.8264\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 204ms/step - loss: 0.3742 - accuracy: 0.8656 - val_loss: 0.5335 - val_accuracy: 0.8209\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.3797 - accuracy: 0.8629 - val_loss: 0.5380 - val_accuracy: 0.8182\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.3764 - accuracy: 0.8587 - val_loss: 0.5271 - val_accuracy: 0.8264\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3809 - accuracy: 0.8660 - val_loss: 0.5238 - val_accuracy: 0.8306\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.3705 - accuracy: 0.8663 - val_loss: 0.5260 - val_accuracy: 0.8264\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3771 - accuracy: 0.8573 - val_loss: 0.5266 - val_accuracy: 0.8237\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.3772 - accuracy: 0.8587 - val_loss: 0.5233 - val_accuracy: 0.8320\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.3703 - accuracy: 0.8694 - val_loss: 0.5241 - val_accuracy: 0.8264\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.3534 - accuracy: 0.8735 - val_loss: 0.5265 - val_accuracy: 0.8196\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.3613 - accuracy: 0.8646 - val_loss: 0.5262 - val_accuracy: 0.8223\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.3640 - accuracy: 0.8642 - val_loss: 0.5182 - val_accuracy: 0.8375\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 207ms/step - loss: 0.3660 - accuracy: 0.8691 - val_loss: 0.5164 - val_accuracy: 0.8347\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.3645 - accuracy: 0.8684 - val_loss: 0.5188 - val_accuracy: 0.8237\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.3685 - accuracy: 0.8622 - val_loss: 0.5176 - val_accuracy: 0.8223\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3630 - accuracy: 0.8635 - val_loss: 0.5162 - val_accuracy: 0.8223\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.3607 - accuracy: 0.8742 - val_loss: 0.5178 - val_accuracy: 0.8209\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 0.3582 - accuracy: 0.8704 - val_loss: 0.5223 - val_accuracy: 0.8168\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 0.3509 - accuracy: 0.8697 - val_loss: 0.5223 - val_accuracy: 0.8154\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.3500 - accuracy: 0.8718 - val_loss: 0.5217 - val_accuracy: 0.8209\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3431 - accuracy: 0.8777 - val_loss: 0.5196 - val_accuracy: 0.8223\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.3506 - accuracy: 0.8694 - val_loss: 0.5199 - val_accuracy: 0.8223\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 0.3499 - accuracy: 0.8749 - val_loss: 0.5172 - val_accuracy: 0.8278\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3453 - accuracy: 0.8753 - val_loss: 0.5161 - val_accuracy: 0.8320\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3540 - accuracy: 0.8732 - val_loss: 0.5146 - val_accuracy: 0.8347\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.3471 - accuracy: 0.8756 - val_loss: 0.5132 - val_accuracy: 0.8361\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.3497 - accuracy: 0.8739 - val_loss: 0.5169 - val_accuracy: 0.8264\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3435 - accuracy: 0.8684 - val_loss: 0.5222 - val_accuracy: 0.8182\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.3505 - accuracy: 0.8732 - val_loss: 0.5116 - val_accuracy: 0.8264\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.3527 - accuracy: 0.8701 - val_loss: 0.5081 - val_accuracy: 0.8278\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.3492 - accuracy: 0.8739 - val_loss: 0.5135 - val_accuracy: 0.8196\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3477 - accuracy: 0.8687 - val_loss: 0.5145 - val_accuracy: 0.8223\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.3431 - accuracy: 0.8718 - val_loss: 0.5104 - val_accuracy: 0.8375\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3470 - accuracy: 0.8742 - val_loss: 0.5140 - val_accuracy: 0.8320\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the dataset.\n",
    "dataset = pd.read_feather(\"DATA/openset_with_feat.feather\")\n",
    "dataset['label'] = LabelEncoder().fit_transform(dataset['comple_portal_num'])\n",
    "train_data, vali_data = train_test_split(dataset, test_size=cfg.TRAIN_TEST_SPLIT_SIZE,stratify=dataset['label'])\n",
    "\n",
    "X_train = reshape_features(pd.DataFrame(train_data['emb'].tolist()))\n",
    "X_val = reshape_features(pd.DataFrame(vali_data['emb'].tolist()))\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_val shape: {X_val.shape}')\n",
    "\n",
    "# Train the model.\n",
    "gru_attention_model = md.GRUWithAttentionModel(input_shape=cfg.INPUT_SHAPE, num_classes=cfg.NUM_CLASSES)\n",
    "gru_attention_model.compile_model()\n",
    "\n",
    "history = gru_attention_model.train(\n",
    "    X_train, \n",
    "    train_data['label'],\n",
    "    X_val, \n",
    "    vali_data['label'],\n",
    "    batch_size=cfg.BATCH_SIZE, \n",
    "    epochs=cfg.EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.853420</td>\n",
       "      <td>0.957952</td>\n",
       "      <td>0.902670</td>\n",
       "      <td>547.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.446281</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>121.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.831956</td>\n",
       "      <td>0.831956</td>\n",
       "      <td>0.831956</td>\n",
       "      <td>0.831956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.623617</td>\n",
       "      <td>0.383163</td>\n",
       "      <td>0.463080</td>\n",
       "      <td>726.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.817357</td>\n",
       "      <td>0.831956</td>\n",
       "      <td>0.814542</td>\n",
       "      <td>726.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "1              0.853420  0.957952  0.902670  547.000000\n",
       "2              0.650602  0.446281  0.529412  121.000000\n",
       "3              0.857143  0.545455  0.666667   22.000000\n",
       "4              0.875000  0.437500  0.583333   16.000000\n",
       "5              1.000000  0.500000  0.666667    4.000000\n",
       "6              1.000000  0.444444  0.615385    9.000000\n",
       "7              0.000000  0.000000  0.000000    1.000000\n",
       "8              1.000000  0.500000  0.666667    2.000000\n",
       "10             0.000000  0.000000  0.000000    1.000000\n",
       "12             0.000000  0.000000  0.000000    3.000000\n",
       "accuracy       0.831956  0.831956  0.831956    0.831956\n",
       "macro avg      0.623617  0.383163  0.463080  726.000000\n",
       "weighted avg   0.817357  0.831956  0.814542  726.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_predictions = gru_attention_model.model.predict(X_val, batch_size=cfg.BATCH_SIZE)\n",
    "ground_truth_labels = vali_data['label'].values\n",
    "predicted_labels = np.argmax(val_predictions, axis=1)\n",
    "report = pd.DataFrame(classification_report(ground_truth_labels, predicted_labels, zero_division=0, output_dict=True, target_names=[1,2,3,4,5,6,7,8,10,12])).T\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Replace 'your_model_path.h5' with the actual path to your saved model file\n",
    "loaded_model = load_model(\"./model/deepsub.h5\",custom_objects={\"Attention\": Attention},compile=False)\n",
    "dataset = pd.read_feather(\"DATA/openset_with_feat.feather\")\n",
    "dataset['label'] = LabelEncoder().fit_transform(dataset['comple_portal_num'])\n",
    "X_test = reshape_features(pd.DataFrame(dataset['emb'].tolist()))\n",
    "predicted = loaded_model.predict(X_test)\n",
    "predicted_labels = np.argmax(predicted, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.256035</td>\n",
       "      <td>0.393258</td>\n",
       "      <td>2734.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.183135</td>\n",
       "      <td>0.711921</td>\n",
       "      <td>0.291328</td>\n",
       "      <td>604.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.392523</td>\n",
       "      <td>0.286689</td>\n",
       "      <td>107.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.132653</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.147727</td>\n",
       "      <td>78.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.203390</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.142012</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.332415</td>\n",
       "      <td>0.332415</td>\n",
       "      <td>0.332415</td>\n",
       "      <td>0.332415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.313967</td>\n",
       "      <td>0.255631</td>\n",
       "      <td>0.214694</td>\n",
       "      <td>3628.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.684262</td>\n",
       "      <td>0.332415</td>\n",
       "      <td>0.360646</td>\n",
       "      <td>3628.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "1              0.847458  0.256035  0.393258  2734.000000\n",
       "2              0.183135  0.711921  0.291328   604.000000\n",
       "3              0.225806  0.392523  0.286689   107.000000\n",
       "4              0.132653  0.166667  0.147727    78.000000\n",
       "5              0.153846  0.300000  0.203390    20.000000\n",
       "6              0.096774  0.266667  0.142012    45.000000\n",
       "7              0.000000  0.000000  0.000000     7.000000\n",
       "8              0.000000  0.000000  0.000000    12.000000\n",
       "10             1.000000  0.400000  0.571429     5.000000\n",
       "12             0.500000  0.062500  0.111111    16.000000\n",
       "accuracy       0.332415  0.332415  0.332415     0.332415\n",
       "macro avg      0.313967  0.255631  0.214694  3628.000000\n",
       "weighted avg   0.684262  0.332415  0.360646  3628.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.DataFrame(classification_report(dataset.label.values, predicted_labels, zero_division=0, output_dict=True, target_names=[1,2,3,4,5,6,7,8,10,12])).T\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
