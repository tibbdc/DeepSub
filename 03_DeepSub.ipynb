{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python featurizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 04:27:22.852090: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-05 04:27:22.852165: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-05 04:27:22.853689: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-05 04:27:23.010281: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-05 04:27:26.103306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tool import model as md\n",
    "from tool import config as cfg\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"\n",
    "    Load and preprocess the dataset.\n",
    "\n",
    "    Args:\n",
    "        None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed dataset containing the following columns:\n",
    "            - 'uniprot_id': UniProt ID.\n",
    "            - 'seq': Sequence.\n",
    "            - 'f1': Feature value.\n",
    "            - 'new_label': Encoded label value using LabelEncoder.\n",
    "            - 'label': Original label value.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(cfg.DATA_PATH)\n",
    "    feature = pd.read_feather(f'{cfg.FEATURE_PATH}feature_esm2.feather')\n",
    "    dataset = dataset.rename(columns={'Entry': 'uniprot_id', 'Sequence': 'seq'})\n",
    "    data_df = dataset.merge(feature, on='uniprot_id', how='left')\n",
    "    data_df = data_df[~data_df.f1.isnull()]\n",
    "    data_df['label'] = LabelEncoder().fit_transform(data_df['label'])\n",
    "\n",
    "    return data_df\n",
    "\n",
    "def reshape_features(data):\n",
    "    \"\"\"\n",
    "    Reshape input data to have 3 dimensions.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Input data to be reshaped.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Reshaped data with shape (n_samples, 1, n_features).\n",
    "    \"\"\"\n",
    "    return np.array(data).reshape(data.shape[0],1,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python trainner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CV Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "accuracies, f1_scores, recalls = [], [], []\n",
    "all_ground_truth_labels = []\n",
    "all_predicted_labels = []\n",
    "\n",
    "for fold in range(10):\n",
    "    fold_data = pd.read_excel('output/cv_labels.xlsx', sheet_name=f'fold{fold}')\n",
    "\n",
    "    ground_truth_labels = fold_data['GroundTruth'].values\n",
    "    predicted_labels = fold_data['PredictedLabels'].values\n",
    "    all_ground_truth_labels.append(ground_truth_labels)\n",
    "    all_predicted_labels.append(predicted_labels)\n",
    "    \n",
    "    f1 = f1_score(ground_truth_labels, predicted_labels, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "f1_scores.index(max(f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.966938</td>\n",
       "      <td>0.958215</td>\n",
       "      <td>0.962557</td>\n",
       "      <td>2106.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.972325</td>\n",
       "      <td>0.982861</td>\n",
       "      <td>0.977564</td>\n",
       "      <td>5076.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.976440</td>\n",
       "      <td>0.949109</td>\n",
       "      <td>0.962581</td>\n",
       "      <td>393.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.969109</td>\n",
       "      <td>0.956446</td>\n",
       "      <td>0.962736</td>\n",
       "      <td>1148.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.980231</td>\n",
       "      <td>0.973813</td>\n",
       "      <td>0.977011</td>\n",
       "      <td>611.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.934211</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>76.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980583</td>\n",
       "      <td>0.990196</td>\n",
       "      <td>103.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.946237</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.971657</td>\n",
       "      <td>0.971657</td>\n",
       "      <td>0.971657</td>\n",
       "      <td>0.971657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.977411</td>\n",
       "      <td>0.963762</td>\n",
       "      <td>0.970427</td>\n",
       "      <td>9632.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.971654</td>\n",
       "      <td>0.971657</td>\n",
       "      <td>0.971615</td>\n",
       "      <td>9632.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "0              0.966938  0.958215  0.962557  2106.000000\n",
       "1              0.972325  0.982861  0.977564  5076.000000\n",
       "2              0.976440  0.949109  0.962581   393.000000\n",
       "3              0.969109  0.956446  0.962736  1148.000000\n",
       "4              0.971831  0.985714  0.978723    70.000000\n",
       "5              0.980231  0.973813  0.977011   611.000000\n",
       "6              1.000000  1.000000  1.000000     1.000000\n",
       "7              0.959459  0.934211  0.946667    76.000000\n",
       "8              1.000000  0.980583  0.990196   103.000000\n",
       "9              0.977778  0.916667  0.946237    48.000000\n",
       "accuracy       0.971657  0.971657  0.971657     0.971657\n",
       "macro avg      0.977411  0.963762  0.970427  9632.000000\n",
       "weighted avg   0.971654  0.971657  0.971615  9632.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groundtruth = all_ground_truth_labels[4]\n",
    "predict = all_predicted_labels[4]\n",
    "report = pd.DataFrame(classification_report(groundtruth, predict, zero_division=0, output_dict=True)).T\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.to_csv(\"best_cv_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "cm = confusion_matrix(groundtruth, predict)\n",
    "sns.set(font_scale=1.2) \n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted', size=16)\n",
    "plt.ylabel('GroundTruth', size=16)\n",
    "plt.title('Confusion Matrix', size=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.960685</td>\n",
       "      <td>0.945632</td>\n",
       "      <td>0.953099</td>\n",
       "      <td>4212.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.967294</td>\n",
       "      <td>0.981680</td>\n",
       "      <td>0.974434</td>\n",
       "      <td>10153.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.968545</td>\n",
       "      <td>0.941401</td>\n",
       "      <td>0.954780</td>\n",
       "      <td>785.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.967613</td>\n",
       "      <td>0.950327</td>\n",
       "      <td>0.958892</td>\n",
       "      <td>2295.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.971223</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>141.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.978776</td>\n",
       "      <td>0.981178</td>\n",
       "      <td>0.979975</td>\n",
       "      <td>1222.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.985612</td>\n",
       "      <td>0.895425</td>\n",
       "      <td>0.938356</td>\n",
       "      <td>153.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990244</td>\n",
       "      <td>0.995098</td>\n",
       "      <td>205.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.927083</td>\n",
       "      <td>0.951872</td>\n",
       "      <td>96.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.967246</td>\n",
       "      <td>0.967246</td>\n",
       "      <td>0.967246</td>\n",
       "      <td>0.967246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.977777</td>\n",
       "      <td>0.890375</td>\n",
       "      <td>0.917079</td>\n",
       "      <td>19265.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.967247</td>\n",
       "      <td>0.967246</td>\n",
       "      <td>0.967141</td>\n",
       "      <td>19265.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score       support\n",
       "0              0.960685  0.945632  0.953099   4212.000000\n",
       "1              0.967294  0.981680  0.974434  10153.000000\n",
       "2              0.968545  0.941401  0.954780    785.000000\n",
       "3              0.967613  0.950327  0.958892   2295.000000\n",
       "4              0.971223  0.957447  0.964286    141.000000\n",
       "5              0.978776  0.981178  0.979975   1222.000000\n",
       "6              1.000000  0.333333  0.500000      3.000000\n",
       "7              0.985612  0.895425  0.938356    153.000000\n",
       "8              1.000000  0.990244  0.995098    205.000000\n",
       "9              0.978022  0.927083  0.951872     96.000000\n",
       "accuracy       0.967246  0.967246  0.967246      0.967246\n",
       "macro avg      0.977777  0.890375  0.917079  19265.000000\n",
       "weighted avg   0.967247  0.967246  0.967141  19265.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.read_csv(\"./output/groundtruth_and_labels.csv\")\n",
    "groundtruth = result[\"GroundTruth\"]\n",
    "predict = result[\"PredictedLabels\"]\n",
    "report = pd.DataFrame(classification_report(result[\"GroundTruth\"], result[\"PredictedLabels\"], zero_division=0, output_dict=True)).T\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.to_csv(\"1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(groundtruth, predict)\n",
    "sns.set(font_scale=1.2) \n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='viridis')\n",
    "plt.xlabel('Predicted', size=16)\n",
    "plt.ylabel('GroundTruth', size=16)\n",
    "plt.title('Confusion Matrix', size=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "groundtruth = np.array(groundtruth)\n",
    "predict = np.array(predict)\n",
    "\n",
    "cm = confusion_matrix(groundtruth, predict)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Ground Truth')\n",
    "plt.title('Blast')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1:\n",
      "X_train shape: (86691, 1, 1280)\n",
      "X_val shape: (9633, 1, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 16:37:53.251188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3152 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-02-22 16:37:53.253064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10083 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:25:00.0, compute capability: 8.6\n",
      "2024-02-22 16:37:53.254631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 10083 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:41:00.0, compute capability: 8.6\n",
      "2024-02-22 16:37:53.256212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 10083 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:61:00.0, compute capability: 8.6\n",
      "2024-02-22 16:37:53.257782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 10083 MB memory:  -> device: 4, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2024-02-22 16:37:53.259341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 10083 MB memory:  -> device: 5, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:a1:00.0, compute capability: 8.6\n",
      "2024-02-22 16:37:53.261137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 10083 MB memory:  -> device: 6, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:c1:00.0, compute capability: 8.6\n",
      "2024-02-22 16:37:53.263163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 10083 MB memory:  -> device: 7, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:e1:00.0, compute capability: 8.6\n",
      "2024-02-22 16:37:53.492224: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 16:37:59.034495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2024-02-22 16:37:59.230987: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f0b1402f2d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-22 16:37:59.231046: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\n",
      "2024-02-22 16:37:59.231053: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\n",
      "2024-02-22 16:37:59.231059: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\n",
      "2024-02-22 16:37:59.231063: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\n",
      "2024-02-22 16:37:59.231067: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (4): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\n",
      "2024-02-22 16:37:59.231071: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (5): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\n",
      "2024-02-22 16:37:59.231075: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (6): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\n",
      "2024-02-22 16:37:59.231079: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (7): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\n",
      "2024-02-22 16:37:59.239413: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-02-22 16:37:59.403890: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 7s 23ms/step - loss: 1.3088 - accuracy: 0.5606 - val_loss: 0.9859 - val_accuracy: 0.6559\n",
      "Epoch 2/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.8242 - accuracy: 0.7314 - val_loss: 0.6250 - val_accuracy: 0.8163\n",
      "Epoch 3/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.5809 - accuracy: 0.8250 - val_loss: 0.4549 - val_accuracy: 0.8686\n",
      "Epoch 4/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.4791 - accuracy: 0.8597 - val_loss: 0.3821 - val_accuracy: 0.8893\n",
      "Epoch 5/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.4325 - accuracy: 0.8718 - val_loss: 0.3343 - val_accuracy: 0.9065\n",
      "Epoch 6/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.4043 - accuracy: 0.8789 - val_loss: 0.3076 - val_accuracy: 0.9153\n",
      "Epoch 7/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3853 - accuracy: 0.8839 - val_loss: 0.2903 - val_accuracy: 0.9176\n",
      "Epoch 8/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3686 - accuracy: 0.8880 - val_loss: 0.2747 - val_accuracy: 0.9220\n",
      "Epoch 9/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3556 - accuracy: 0.8898 - val_loss: 0.2650 - val_accuracy: 0.9236\n",
      "Epoch 10/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3430 - accuracy: 0.8943 - val_loss: 0.2567 - val_accuracy: 0.9265\n",
      "Epoch 11/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3334 - accuracy: 0.8964 - val_loss: 0.2460 - val_accuracy: 0.9292\n",
      "Epoch 12/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3240 - accuracy: 0.9001 - val_loss: 0.2391 - val_accuracy: 0.9316\n",
      "Epoch 13/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3146 - accuracy: 0.9025 - val_loss: 0.2314 - val_accuracy: 0.9320\n",
      "Epoch 14/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3049 - accuracy: 0.9057 - val_loss: 0.2332 - val_accuracy: 0.9296\n",
      "Epoch 15/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2963 - accuracy: 0.9077 - val_loss: 0.2182 - val_accuracy: 0.9346\n",
      "Epoch 16/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2891 - accuracy: 0.9107 - val_loss: 0.2130 - val_accuracy: 0.9389\n",
      "Epoch 17/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2812 - accuracy: 0.9142 - val_loss: 0.2071 - val_accuracy: 0.9383\n",
      "Epoch 18/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2732 - accuracy: 0.9155 - val_loss: 0.2010 - val_accuracy: 0.9396\n",
      "Epoch 19/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2670 - accuracy: 0.9176 - val_loss: 0.1967 - val_accuracy: 0.9426\n",
      "Epoch 20/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2593 - accuracy: 0.9199 - val_loss: 0.1938 - val_accuracy: 0.9423\n",
      "Epoch 21/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2545 - accuracy: 0.9221 - val_loss: 0.1890 - val_accuracy: 0.9445\n",
      "Epoch 22/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2451 - accuracy: 0.9248 - val_loss: 0.1860 - val_accuracy: 0.9452\n",
      "Epoch 23/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2400 - accuracy: 0.9250 - val_loss: 0.1823 - val_accuracy: 0.9452\n",
      "Epoch 24/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2335 - accuracy: 0.9280 - val_loss: 0.1769 - val_accuracy: 0.9484\n",
      "Epoch 25/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2254 - accuracy: 0.9313 - val_loss: 0.1728 - val_accuracy: 0.9482\n",
      "Epoch 26/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.2190 - accuracy: 0.9324 - val_loss: 0.1687 - val_accuracy: 0.9490\n",
      "Epoch 27/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2133 - accuracy: 0.9348 - val_loss: 0.1669 - val_accuracy: 0.9503\n",
      "Epoch 28/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2078 - accuracy: 0.9356 - val_loss: 0.1639 - val_accuracy: 0.9508\n",
      "Epoch 29/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2032 - accuracy: 0.9380 - val_loss: 0.1612 - val_accuracy: 0.9504\n",
      "Epoch 30/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1996 - accuracy: 0.9395 - val_loss: 0.1600 - val_accuracy: 0.9514\n",
      "Epoch 31/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1947 - accuracy: 0.9406 - val_loss: 0.1561 - val_accuracy: 0.9520\n",
      "Epoch 32/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1909 - accuracy: 0.9416 - val_loss: 0.1541 - val_accuracy: 0.9529\n",
      "Epoch 33/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1857 - accuracy: 0.9435 - val_loss: 0.1535 - val_accuracy: 0.9526\n",
      "Epoch 34/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1795 - accuracy: 0.9449 - val_loss: 0.1487 - val_accuracy: 0.9544\n",
      "Epoch 35/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1777 - accuracy: 0.9449 - val_loss: 0.1472 - val_accuracy: 0.9544\n",
      "Epoch 36/200\n",
      "43/43 [==============================] - 0s 12ms/step - loss: 0.1742 - accuracy: 0.9468 - val_loss: 0.1461 - val_accuracy: 0.9548\n",
      "Epoch 37/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1711 - accuracy: 0.9476 - val_loss: 0.1479 - val_accuracy: 0.9548\n",
      "Epoch 38/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1679 - accuracy: 0.9482 - val_loss: 0.1438 - val_accuracy: 0.9559\n",
      "Epoch 39/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1642 - accuracy: 0.9496 - val_loss: 0.1424 - val_accuracy: 0.9567\n",
      "Epoch 40/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1613 - accuracy: 0.9505 - val_loss: 0.1380 - val_accuracy: 0.9581\n",
      "Epoch 41/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1583 - accuracy: 0.9514 - val_loss: 0.1373 - val_accuracy: 0.9579\n",
      "Epoch 42/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1542 - accuracy: 0.9522 - val_loss: 0.1358 - val_accuracy: 0.9579\n",
      "Epoch 43/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1529 - accuracy: 0.9523 - val_loss: 0.1348 - val_accuracy: 0.9591\n",
      "Epoch 44/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1505 - accuracy: 0.9530 - val_loss: 0.1359 - val_accuracy: 0.9591\n",
      "Epoch 45/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1484 - accuracy: 0.9538 - val_loss: 0.1323 - val_accuracy: 0.9599\n",
      "Epoch 46/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1443 - accuracy: 0.9550 - val_loss: 0.1314 - val_accuracy: 0.9597\n",
      "Epoch 47/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1444 - accuracy: 0.9553 - val_loss: 0.1315 - val_accuracy: 0.9597\n",
      "Epoch 48/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1412 - accuracy: 0.9549 - val_loss: 0.1293 - val_accuracy: 0.9600\n",
      "Epoch 49/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1384 - accuracy: 0.9566 - val_loss: 0.1292 - val_accuracy: 0.9603\n",
      "Epoch 50/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1382 - accuracy: 0.9572 - val_loss: 0.1271 - val_accuracy: 0.9610\n",
      "Epoch 51/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1342 - accuracy: 0.9580 - val_loss: 0.1284 - val_accuracy: 0.9602\n",
      "Epoch 52/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1344 - accuracy: 0.9581 - val_loss: 0.1255 - val_accuracy: 0.9614\n",
      "Epoch 53/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1313 - accuracy: 0.9591 - val_loss: 0.1270 - val_accuracy: 0.9612\n",
      "Epoch 54/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.1294 - accuracy: 0.9592 - val_loss: 0.1272 - val_accuracy: 0.9602\n",
      "Epoch 55/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.1279 - accuracy: 0.9595 - val_loss: 0.1229 - val_accuracy: 0.9614\n",
      "Epoch 56/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1253 - accuracy: 0.9600 - val_loss: 0.1240 - val_accuracy: 0.9615\n",
      "Epoch 57/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1243 - accuracy: 0.9604 - val_loss: 0.1233 - val_accuracy: 0.9613\n",
      "Epoch 58/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1241 - accuracy: 0.9606 - val_loss: 0.1215 - val_accuracy: 0.9626\n",
      "Epoch 59/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1241 - accuracy: 0.9603 - val_loss: 0.1239 - val_accuracy: 0.9618\n",
      "Epoch 60/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1208 - accuracy: 0.9608 - val_loss: 0.1237 - val_accuracy: 0.9621\n",
      "Epoch 61/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1189 - accuracy: 0.9621 - val_loss: 0.1207 - val_accuracy: 0.9633\n",
      "Epoch 62/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1189 - accuracy: 0.9616 - val_loss: 0.1195 - val_accuracy: 0.9630\n",
      "Epoch 63/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1163 - accuracy: 0.9626 - val_loss: 0.1194 - val_accuracy: 0.9623\n",
      "Epoch 64/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1146 - accuracy: 0.9631 - val_loss: 0.1220 - val_accuracy: 0.9625\n",
      "Epoch 65/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1127 - accuracy: 0.9643 - val_loss: 0.1179 - val_accuracy: 0.9625\n",
      "Epoch 66/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1123 - accuracy: 0.9645 - val_loss: 0.1204 - val_accuracy: 0.9635\n",
      "Epoch 67/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1118 - accuracy: 0.9637 - val_loss: 0.1209 - val_accuracy: 0.9630\n",
      "Epoch 68/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1101 - accuracy: 0.9647 - val_loss: 0.1195 - val_accuracy: 0.9635\n",
      "Epoch 69/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1090 - accuracy: 0.9646 - val_loss: 0.1154 - val_accuracy: 0.9652\n",
      "Epoch 70/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1076 - accuracy: 0.9648 - val_loss: 0.1155 - val_accuracy: 0.9645\n",
      "Epoch 71/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1047 - accuracy: 0.9658 - val_loss: 0.1181 - val_accuracy: 0.9647\n",
      "Epoch 72/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1062 - accuracy: 0.9658 - val_loss: 0.1163 - val_accuracy: 0.9640\n",
      "Epoch 73/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1034 - accuracy: 0.9665 - val_loss: 0.1165 - val_accuracy: 0.9648\n",
      "Epoch 74/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1037 - accuracy: 0.9666 - val_loss: 0.1186 - val_accuracy: 0.9643\n",
      "Epoch 75/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1020 - accuracy: 0.9667 - val_loss: 0.1167 - val_accuracy: 0.9638\n",
      "Epoch 76/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1018 - accuracy: 0.9672 - val_loss: 0.1169 - val_accuracy: 0.9644\n",
      "Epoch 77/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0991 - accuracy: 0.9683 - val_loss: 0.1145 - val_accuracy: 0.9644\n",
      "Epoch 78/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0984 - accuracy: 0.9671 - val_loss: 0.1133 - val_accuracy: 0.9651\n",
      "Epoch 79/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0969 - accuracy: 0.9677 - val_loss: 0.1147 - val_accuracy: 0.9651\n",
      "Epoch 80/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0977 - accuracy: 0.9682 - val_loss: 0.1162 - val_accuracy: 0.9652\n",
      "Epoch 81/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0964 - accuracy: 0.9684 - val_loss: 0.1129 - val_accuracy: 0.9665\n",
      "Epoch 82/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0954 - accuracy: 0.9683 - val_loss: 0.1138 - val_accuracy: 0.9653\n",
      "Epoch 83/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0941 - accuracy: 0.9691 - val_loss: 0.1143 - val_accuracy: 0.9651\n",
      "Epoch 84/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0927 - accuracy: 0.9697 - val_loss: 0.1146 - val_accuracy: 0.9657\n",
      "Epoch 85/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0928 - accuracy: 0.9691 - val_loss: 0.1141 - val_accuracy: 0.9649\n",
      "Epoch 86/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0914 - accuracy: 0.9695 - val_loss: 0.1150 - val_accuracy: 0.9650\n",
      "Epoch 87/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0911 - accuracy: 0.9703 - val_loss: 0.1131 - val_accuracy: 0.9654\n",
      "Epoch 88/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0900 - accuracy: 0.9704 - val_loss: 0.1132 - val_accuracy: 0.9656\n",
      "Epoch 89/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0897 - accuracy: 0.9698 - val_loss: 0.1121 - val_accuracy: 0.9660\n",
      "Epoch 90/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0882 - accuracy: 0.9705 - val_loss: 0.1119 - val_accuracy: 0.9654\n",
      "Epoch 91/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0871 - accuracy: 0.9709 - val_loss: 0.1118 - val_accuracy: 0.9656\n",
      "Epoch 92/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0868 - accuracy: 0.9717 - val_loss: 0.1102 - val_accuracy: 0.9670\n",
      "Epoch 93/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0858 - accuracy: 0.9716 - val_loss: 0.1126 - val_accuracy: 0.9656\n",
      "Epoch 94/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0843 - accuracy: 0.9715 - val_loss: 0.1112 - val_accuracy: 0.9664\n",
      "Epoch 95/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0835 - accuracy: 0.9719 - val_loss: 0.1123 - val_accuracy: 0.9670\n",
      "Epoch 96/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0839 - accuracy: 0.9722 - val_loss: 0.1102 - val_accuracy: 0.9657\n",
      "Epoch 97/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0829 - accuracy: 0.9726 - val_loss: 0.1094 - val_accuracy: 0.9671\n",
      "Epoch 98/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0829 - accuracy: 0.9717 - val_loss: 0.1109 - val_accuracy: 0.9670\n",
      "Epoch 99/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0816 - accuracy: 0.9729 - val_loss: 0.1119 - val_accuracy: 0.9653\n",
      "Epoch 100/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0817 - accuracy: 0.9729 - val_loss: 0.1118 - val_accuracy: 0.9673\n",
      "Epoch 101/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0787 - accuracy: 0.9737 - val_loss: 0.1112 - val_accuracy: 0.9667\n",
      "Epoch 102/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0789 - accuracy: 0.9734 - val_loss: 0.1133 - val_accuracy: 0.9661\n",
      "Epoch 103/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0773 - accuracy: 0.9744 - val_loss: 0.1114 - val_accuracy: 0.9667\n",
      "Epoch 104/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0776 - accuracy: 0.9740 - val_loss: 0.1090 - val_accuracy: 0.9678\n",
      "Epoch 105/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0782 - accuracy: 0.9737 - val_loss: 0.1110 - val_accuracy: 0.9670\n",
      "Epoch 106/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0778 - accuracy: 0.9735 - val_loss: 0.1127 - val_accuracy: 0.9657\n",
      "Epoch 107/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0759 - accuracy: 0.9746 - val_loss: 0.1091 - val_accuracy: 0.9680\n",
      "Epoch 108/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0752 - accuracy: 0.9742 - val_loss: 0.1114 - val_accuracy: 0.9666\n",
      "Epoch 109/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0734 - accuracy: 0.9746 - val_loss: 0.1095 - val_accuracy: 0.9671\n",
      "Epoch 110/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0752 - accuracy: 0.9746 - val_loss: 0.1096 - val_accuracy: 0.9692\n",
      "Epoch 111/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0739 - accuracy: 0.9752 - val_loss: 0.1121 - val_accuracy: 0.9673\n",
      "Epoch 112/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0728 - accuracy: 0.9752 - val_loss: 0.1099 - val_accuracy: 0.9679\n",
      "Epoch 113/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0727 - accuracy: 0.9750 - val_loss: 0.1104 - val_accuracy: 0.9673\n",
      "Epoch 114/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0724 - accuracy: 0.9756 - val_loss: 0.1087 - val_accuracy: 0.9677\n",
      "Epoch 115/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0713 - accuracy: 0.9758 - val_loss: 0.1092 - val_accuracy: 0.9675\n",
      "Epoch 116/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0708 - accuracy: 0.9757 - val_loss: 0.1091 - val_accuracy: 0.9681\n",
      "Epoch 117/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0690 - accuracy: 0.9771 - val_loss: 0.1094 - val_accuracy: 0.9683\n",
      "Epoch 118/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0707 - accuracy: 0.9760 - val_loss: 0.1118 - val_accuracy: 0.9672\n",
      "Epoch 119/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0686 - accuracy: 0.9765 - val_loss: 0.1092 - val_accuracy: 0.9680\n",
      "Epoch 120/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0683 - accuracy: 0.9768 - val_loss: 0.1113 - val_accuracy: 0.9672\n",
      "Epoch 121/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0686 - accuracy: 0.9771 - val_loss: 0.1092 - val_accuracy: 0.9686\n",
      "Epoch 122/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0673 - accuracy: 0.9765 - val_loss: 0.1107 - val_accuracy: 0.9677\n",
      "Epoch 123/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0657 - accuracy: 0.9771 - val_loss: 0.1131 - val_accuracy: 0.9673\n",
      "Epoch 124/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0660 - accuracy: 0.9781 - val_loss: 0.1098 - val_accuracy: 0.9682\n",
      "Epoch 125/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0655 - accuracy: 0.9776 - val_loss: 0.1128 - val_accuracy: 0.9680\n",
      "Epoch 126/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0651 - accuracy: 0.9779 - val_loss: 0.1109 - val_accuracy: 0.9673\n",
      "Epoch 127/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0649 - accuracy: 0.9773 - val_loss: 0.1122 - val_accuracy: 0.9678\n",
      "Epoch 128/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0645 - accuracy: 0.9779 - val_loss: 0.1117 - val_accuracy: 0.9684\n",
      "Epoch 129/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0643 - accuracy: 0.9778 - val_loss: 0.1149 - val_accuracy: 0.9676\n",
      "Epoch 130/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0631 - accuracy: 0.9783 - val_loss: 0.1116 - val_accuracy: 0.9681\n",
      "Epoch 131/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0627 - accuracy: 0.9779 - val_loss: 0.1113 - val_accuracy: 0.9682\n",
      "Epoch 132/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0618 - accuracy: 0.9792 - val_loss: 0.1143 - val_accuracy: 0.9675\n",
      "Epoch 133/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0628 - accuracy: 0.9785 - val_loss: 0.1123 - val_accuracy: 0.9685\n",
      "Epoch 134/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0617 - accuracy: 0.9788 - val_loss: 0.1117 - val_accuracy: 0.9685\n",
      "Epoch 135/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0625 - accuracy: 0.9787 - val_loss: 0.1132 - val_accuracy: 0.9682\n",
      "Epoch 136/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0613 - accuracy: 0.9788 - val_loss: 0.1138 - val_accuracy: 0.9674\n",
      "Epoch 137/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0609 - accuracy: 0.9790 - val_loss: 0.1114 - val_accuracy: 0.9681\n",
      "Epoch 138/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0604 - accuracy: 0.9786 - val_loss: 0.1109 - val_accuracy: 0.9686\n",
      "Epoch 139/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0593 - accuracy: 0.9794 - val_loss: 0.1122 - val_accuracy: 0.9678\n",
      "Epoch 140/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0585 - accuracy: 0.9800 - val_loss: 0.1141 - val_accuracy: 0.9688\n",
      "Epoch 141/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0590 - accuracy: 0.9798 - val_loss: 0.1104 - val_accuracy: 0.9681\n",
      "Epoch 142/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0585 - accuracy: 0.9799 - val_loss: 0.1145 - val_accuracy: 0.9681\n",
      "Epoch 143/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0583 - accuracy: 0.9795 - val_loss: 0.1137 - val_accuracy: 0.9692\n",
      "Epoch 144/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0573 - accuracy: 0.9801 - val_loss: 0.1127 - val_accuracy: 0.9685\n",
      "Epoch 145/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0573 - accuracy: 0.9801 - val_loss: 0.1116 - val_accuracy: 0.9686\n",
      "Epoch 146/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0573 - accuracy: 0.9799 - val_loss: 0.1118 - val_accuracy: 0.9692\n",
      "Epoch 147/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0551 - accuracy: 0.9812 - val_loss: 0.1119 - val_accuracy: 0.9692\n",
      "Epoch 148/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0562 - accuracy: 0.9808 - val_loss: 0.1116 - val_accuracy: 0.9697\n",
      "Epoch 149/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0555 - accuracy: 0.9805 - val_loss: 0.1157 - val_accuracy: 0.9685\n",
      "Epoch 150/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0546 - accuracy: 0.9807 - val_loss: 0.1115 - val_accuracy: 0.9685\n",
      "Epoch 151/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0560 - accuracy: 0.9807 - val_loss: 0.1109 - val_accuracy: 0.9686\n",
      "Epoch 152/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0545 - accuracy: 0.9809 - val_loss: 0.1166 - val_accuracy: 0.9679\n",
      "Epoch 153/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0541 - accuracy: 0.9807 - val_loss: 0.1149 - val_accuracy: 0.9683\n",
      "Epoch 154/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0525 - accuracy: 0.9815 - val_loss: 0.1145 - val_accuracy: 0.9689\n",
      "Epoch 155/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0541 - accuracy: 0.9811 - val_loss: 0.1140 - val_accuracy: 0.9688\n",
      "Epoch 156/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0531 - accuracy: 0.9818 - val_loss: 0.1117 - val_accuracy: 0.9688\n",
      "Epoch 157/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0517 - accuracy: 0.9823 - val_loss: 0.1149 - val_accuracy: 0.9688\n",
      "Epoch 158/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0530 - accuracy: 0.9816 - val_loss: 0.1149 - val_accuracy: 0.9685\n",
      "Epoch 159/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0514 - accuracy: 0.9822 - val_loss: 0.1124 - val_accuracy: 0.9697\n",
      "Epoch 160/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0522 - accuracy: 0.9818 - val_loss: 0.1128 - val_accuracy: 0.9694\n",
      "Epoch 161/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0512 - accuracy: 0.9823 - val_loss: 0.1117 - val_accuracy: 0.9689\n",
      "Epoch 162/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0498 - accuracy: 0.9828 - val_loss: 0.1120 - val_accuracy: 0.9692\n",
      "Epoch 163/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0495 - accuracy: 0.9827 - val_loss: 0.1135 - val_accuracy: 0.9695\n",
      "Epoch 164/200\n",
      "43/43 [==============================] - 0s 12ms/step - loss: 0.0520 - accuracy: 0.9822 - val_loss: 0.1167 - val_accuracy: 0.9696\n",
      "Epoch 165/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0498 - accuracy: 0.9824 - val_loss: 0.1177 - val_accuracy: 0.9690\n",
      "Epoch 166/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0508 - accuracy: 0.9827 - val_loss: 0.1160 - val_accuracy: 0.9697\n",
      "Epoch 167/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0498 - accuracy: 0.9824 - val_loss: 0.1153 - val_accuracy: 0.9695\n",
      "Epoch 168/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0492 - accuracy: 0.9830 - val_loss: 0.1169 - val_accuracy: 0.9689\n",
      "Epoch 169/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0487 - accuracy: 0.9830 - val_loss: 0.1138 - val_accuracy: 0.9700\n",
      "Epoch 170/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0472 - accuracy: 0.9835 - val_loss: 0.1141 - val_accuracy: 0.9700\n",
      "Epoch 171/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0477 - accuracy: 0.9836 - val_loss: 0.1152 - val_accuracy: 0.9701\n",
      "Epoch 172/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0472 - accuracy: 0.9838 - val_loss: 0.1144 - val_accuracy: 0.9693\n",
      "Epoch 173/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0481 - accuracy: 0.9829 - val_loss: 0.1170 - val_accuracy: 0.9693\n",
      "Epoch 174/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0474 - accuracy: 0.9834 - val_loss: 0.1153 - val_accuracy: 0.9694\n",
      "Epoch 175/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0480 - accuracy: 0.9830 - val_loss: 0.1166 - val_accuracy: 0.9698\n",
      "Epoch 176/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0458 - accuracy: 0.9838 - val_loss: 0.1158 - val_accuracy: 0.9698\n",
      "Epoch 177/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0463 - accuracy: 0.9837 - val_loss: 0.1172 - val_accuracy: 0.9693\n",
      "Epoch 178/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0453 - accuracy: 0.9838 - val_loss: 0.1178 - val_accuracy: 0.9696\n",
      "Epoch 179/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0456 - accuracy: 0.9840 - val_loss: 0.1198 - val_accuracy: 0.9691\n",
      "Epoch 180/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0449 - accuracy: 0.9842 - val_loss: 0.1175 - val_accuracy: 0.9692\n",
      "Epoch 181/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0450 - accuracy: 0.9839 - val_loss: 0.1171 - val_accuracy: 0.9695\n",
      "Epoch 182/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0447 - accuracy: 0.9842 - val_loss: 0.1174 - val_accuracy: 0.9702\n",
      "Epoch 183/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0449 - accuracy: 0.9841 - val_loss: 0.1182 - val_accuracy: 0.9700\n",
      "Epoch 184/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0458 - accuracy: 0.9842 - val_loss: 0.1145 - val_accuracy: 0.9692\n",
      "Epoch 185/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0448 - accuracy: 0.9843 - val_loss: 0.1197 - val_accuracy: 0.9696\n",
      "Epoch 186/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0444 - accuracy: 0.9844 - val_loss: 0.1171 - val_accuracy: 0.9697\n",
      "Epoch 187/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0443 - accuracy: 0.9848 - val_loss: 0.1169 - val_accuracy: 0.9703\n",
      "Epoch 188/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0427 - accuracy: 0.9849 - val_loss: 0.1179 - val_accuracy: 0.9695\n",
      "Epoch 189/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0438 - accuracy: 0.9843 - val_loss: 0.1157 - val_accuracy: 0.9696\n",
      "Epoch 190/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0432 - accuracy: 0.9846 - val_loss: 0.1168 - val_accuracy: 0.9701\n",
      "Epoch 191/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0423 - accuracy: 0.9848 - val_loss: 0.1194 - val_accuracy: 0.9701\n",
      "Epoch 192/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0427 - accuracy: 0.9853 - val_loss: 0.1189 - val_accuracy: 0.9698\n",
      "Epoch 193/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0428 - accuracy: 0.9851 - val_loss: 0.1189 - val_accuracy: 0.9697\n",
      "Epoch 194/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0429 - accuracy: 0.9851 - val_loss: 0.1174 - val_accuracy: 0.9701\n",
      "Epoch 195/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0431 - accuracy: 0.9847 - val_loss: 0.1188 - val_accuracy: 0.9702\n",
      "Epoch 196/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0425 - accuracy: 0.9850 - val_loss: 0.1189 - val_accuracy: 0.9703\n",
      "Epoch 197/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0422 - accuracy: 0.9854 - val_loss: 0.1208 - val_accuracy: 0.9700\n",
      "Epoch 198/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0414 - accuracy: 0.9857 - val_loss: 0.1174 - val_accuracy: 0.9697\n",
      "Epoch 199/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0420 - accuracy: 0.9856 - val_loss: 0.1191 - val_accuracy: 0.9693\n",
      "Epoch 200/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0421 - accuracy: 0.9852 - val_loss: 0.1205 - val_accuracy: 0.9698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linjw/anaconda3/envs/pytorch/lib/python3.9/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 4ms/step\n",
      "\n",
      "Fold 2:\n",
      "X_train shape: (86691, 1, 1280)\n",
      "X_val shape: (9633, 1, 1280)\n",
      "Epoch 1/200\n",
      "43/43 [==============================] - 4s 23ms/step - loss: 1.3067 - accuracy: 0.5605 - val_loss: 0.9847 - val_accuracy: 0.6686\n",
      "Epoch 2/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.8228 - accuracy: 0.7327 - val_loss: 0.6276 - val_accuracy: 0.8171\n",
      "Epoch 3/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.5840 - accuracy: 0.8238 - val_loss: 0.4589 - val_accuracy: 0.8693\n",
      "Epoch 4/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.4810 - accuracy: 0.8579 - val_loss: 0.3767 - val_accuracy: 0.9002\n",
      "Epoch 5/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.4345 - accuracy: 0.8700 - val_loss: 0.3450 - val_accuracy: 0.9010\n",
      "Epoch 6/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.4054 - accuracy: 0.8786 - val_loss: 0.3114 - val_accuracy: 0.9151\n",
      "Epoch 7/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3864 - accuracy: 0.8842 - val_loss: 0.2878 - val_accuracy: 0.9222\n",
      "Epoch 8/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3690 - accuracy: 0.8868 - val_loss: 0.2744 - val_accuracy: 0.9254\n",
      "Epoch 9/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3564 - accuracy: 0.8910 - val_loss: 0.2630 - val_accuracy: 0.9299\n",
      "Epoch 10/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3463 - accuracy: 0.8941 - val_loss: 0.2520 - val_accuracy: 0.9336\n",
      "Epoch 11/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3357 - accuracy: 0.8966 - val_loss: 0.2425 - val_accuracy: 0.9361\n",
      "Epoch 12/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3292 - accuracy: 0.8993 - val_loss: 0.2383 - val_accuracy: 0.9351\n",
      "Epoch 13/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3170 - accuracy: 0.9012 - val_loss: 0.2301 - val_accuracy: 0.9371\n",
      "Epoch 14/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3096 - accuracy: 0.9035 - val_loss: 0.2262 - val_accuracy: 0.9382\n",
      "Epoch 15/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3019 - accuracy: 0.9060 - val_loss: 0.2169 - val_accuracy: 0.9398\n",
      "Epoch 16/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2934 - accuracy: 0.9085 - val_loss: 0.2152 - val_accuracy: 0.9404\n",
      "Epoch 17/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2840 - accuracy: 0.9120 - val_loss: 0.2083 - val_accuracy: 0.9426\n",
      "Epoch 18/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.2783 - accuracy: 0.9142 - val_loss: 0.2019 - val_accuracy: 0.9450\n",
      "Epoch 19/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2706 - accuracy: 0.9168 - val_loss: 0.1968 - val_accuracy: 0.9449\n",
      "Epoch 20/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.2649 - accuracy: 0.9174 - val_loss: 0.1927 - val_accuracy: 0.9472\n",
      "Epoch 21/200\n",
      "43/43 [==============================] - 0s 12ms/step - loss: 0.2592 - accuracy: 0.9205 - val_loss: 0.1916 - val_accuracy: 0.9447\n",
      "Epoch 22/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2508 - accuracy: 0.9229 - val_loss: 0.1859 - val_accuracy: 0.9474\n",
      "Epoch 23/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.2429 - accuracy: 0.9243 - val_loss: 0.1796 - val_accuracy: 0.9503\n",
      "Epoch 24/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2376 - accuracy: 0.9268 - val_loss: 0.1786 - val_accuracy: 0.9494\n",
      "Epoch 25/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2319 - accuracy: 0.9287 - val_loss: 0.1752 - val_accuracy: 0.9504\n",
      "Epoch 26/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2254 - accuracy: 0.9299 - val_loss: 0.1701 - val_accuracy: 0.9528\n",
      "Epoch 27/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2200 - accuracy: 0.9320 - val_loss: 0.1683 - val_accuracy: 0.9521\n",
      "Epoch 28/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2118 - accuracy: 0.9341 - val_loss: 0.1629 - val_accuracy: 0.9534\n",
      "Epoch 29/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2068 - accuracy: 0.9374 - val_loss: 0.1628 - val_accuracy: 0.9530\n",
      "Epoch 30/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2022 - accuracy: 0.9381 - val_loss: 0.1567 - val_accuracy: 0.9544\n",
      "Epoch 31/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1987 - accuracy: 0.9389 - val_loss: 0.1541 - val_accuracy: 0.9561\n",
      "Epoch 32/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1930 - accuracy: 0.9406 - val_loss: 0.1516 - val_accuracy: 0.9566\n",
      "Epoch 33/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1899 - accuracy: 0.9414 - val_loss: 0.1495 - val_accuracy: 0.9572\n",
      "Epoch 34/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1837 - accuracy: 0.9437 - val_loss: 0.1484 - val_accuracy: 0.9562\n",
      "Epoch 35/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1801 - accuracy: 0.9441 - val_loss: 0.1472 - val_accuracy: 0.9565\n",
      "Epoch 36/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1775 - accuracy: 0.9460 - val_loss: 0.1456 - val_accuracy: 0.9577\n",
      "Epoch 37/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1711 - accuracy: 0.9477 - val_loss: 0.1425 - val_accuracy: 0.9573\n",
      "Epoch 38/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1687 - accuracy: 0.9477 - val_loss: 0.1434 - val_accuracy: 0.9571\n",
      "Epoch 39/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1655 - accuracy: 0.9493 - val_loss: 0.1386 - val_accuracy: 0.9591\n",
      "Epoch 40/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1630 - accuracy: 0.9502 - val_loss: 0.1413 - val_accuracy: 0.9581\n",
      "Epoch 41/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1599 - accuracy: 0.9506 - val_loss: 0.1364 - val_accuracy: 0.9583\n",
      "Epoch 42/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1554 - accuracy: 0.9515 - val_loss: 0.1348 - val_accuracy: 0.9588\n",
      "Epoch 43/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1525 - accuracy: 0.9520 - val_loss: 0.1347 - val_accuracy: 0.9588\n",
      "Epoch 44/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1515 - accuracy: 0.9533 - val_loss: 0.1316 - val_accuracy: 0.9598\n",
      "Epoch 45/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1467 - accuracy: 0.9545 - val_loss: 0.1334 - val_accuracy: 0.9581\n",
      "Epoch 46/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1465 - accuracy: 0.9543 - val_loss: 0.1304 - val_accuracy: 0.9598\n",
      "Epoch 47/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1443 - accuracy: 0.9541 - val_loss: 0.1281 - val_accuracy: 0.9601\n",
      "Epoch 48/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1405 - accuracy: 0.9560 - val_loss: 0.1281 - val_accuracy: 0.9604\n",
      "Epoch 49/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1396 - accuracy: 0.9567 - val_loss: 0.1272 - val_accuracy: 0.9592\n",
      "Epoch 50/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1376 - accuracy: 0.9570 - val_loss: 0.1256 - val_accuracy: 0.9602\n",
      "Epoch 51/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1353 - accuracy: 0.9575 - val_loss: 0.1274 - val_accuracy: 0.9602\n",
      "Epoch 52/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1337 - accuracy: 0.9586 - val_loss: 0.1242 - val_accuracy: 0.9619\n",
      "Epoch 53/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.1319 - accuracy: 0.9582 - val_loss: 0.1228 - val_accuracy: 0.9627\n",
      "Epoch 54/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1284 - accuracy: 0.9592 - val_loss: 0.1227 - val_accuracy: 0.9626\n",
      "Epoch 55/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.1283 - accuracy: 0.9601 - val_loss: 0.1254 - val_accuracy: 0.9617\n",
      "Epoch 56/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1279 - accuracy: 0.9594 - val_loss: 0.1224 - val_accuracy: 0.9619\n",
      "Epoch 57/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.1255 - accuracy: 0.9597 - val_loss: 0.1199 - val_accuracy: 0.9628\n",
      "Epoch 58/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1234 - accuracy: 0.9608 - val_loss: 0.1192 - val_accuracy: 0.9641\n",
      "Epoch 59/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1211 - accuracy: 0.9609 - val_loss: 0.1188 - val_accuracy: 0.9636\n",
      "Epoch 60/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1209 - accuracy: 0.9614 - val_loss: 0.1184 - val_accuracy: 0.9637\n",
      "Epoch 61/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1174 - accuracy: 0.9618 - val_loss: 0.1189 - val_accuracy: 0.9633\n",
      "Epoch 62/200\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.1168 - accuracy: 0.9630 - val_loss: 0.1177 - val_accuracy: 0.9635\n",
      "Epoch 63/200\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.1144 - accuracy: 0.9631 - val_loss: 0.1184 - val_accuracy: 0.9642\n",
      "Epoch 64/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1141 - accuracy: 0.9634 - val_loss: 0.1182 - val_accuracy: 0.9640\n",
      "Epoch 65/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1118 - accuracy: 0.9638 - val_loss: 0.1156 - val_accuracy: 0.9646\n",
      "Epoch 66/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1116 - accuracy: 0.9639 - val_loss: 0.1153 - val_accuracy: 0.9647\n",
      "Epoch 67/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1104 - accuracy: 0.9650 - val_loss: 0.1140 - val_accuracy: 0.9660\n",
      "Epoch 68/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1083 - accuracy: 0.9653 - val_loss: 0.1154 - val_accuracy: 0.9646\n",
      "Epoch 69/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1074 - accuracy: 0.9654 - val_loss: 0.1158 - val_accuracy: 0.9653\n",
      "Epoch 70/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1059 - accuracy: 0.9652 - val_loss: 0.1176 - val_accuracy: 0.9641\n",
      "Epoch 71/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1059 - accuracy: 0.9658 - val_loss: 0.1143 - val_accuracy: 0.9650\n",
      "Epoch 72/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1055 - accuracy: 0.9660 - val_loss: 0.1147 - val_accuracy: 0.9647\n",
      "Epoch 73/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1032 - accuracy: 0.9666 - val_loss: 0.1150 - val_accuracy: 0.9651\n",
      "Epoch 74/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1020 - accuracy: 0.9666 - val_loss: 0.1133 - val_accuracy: 0.9647\n",
      "Epoch 75/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0997 - accuracy: 0.9678 - val_loss: 0.1125 - val_accuracy: 0.9655\n",
      "Epoch 76/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0988 - accuracy: 0.9676 - val_loss: 0.1132 - val_accuracy: 0.9658\n",
      "Epoch 77/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0995 - accuracy: 0.9681 - val_loss: 0.1121 - val_accuracy: 0.9652\n",
      "Epoch 78/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0971 - accuracy: 0.9679 - val_loss: 0.1133 - val_accuracy: 0.9644\n",
      "Epoch 79/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0985 - accuracy: 0.9676 - val_loss: 0.1113 - val_accuracy: 0.9667\n",
      "Epoch 80/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0972 - accuracy: 0.9682 - val_loss: 0.1111 - val_accuracy: 0.9661\n",
      "Epoch 81/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0947 - accuracy: 0.9692 - val_loss: 0.1110 - val_accuracy: 0.9664\n",
      "Epoch 82/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0946 - accuracy: 0.9688 - val_loss: 0.1099 - val_accuracy: 0.9666\n",
      "Epoch 83/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0929 - accuracy: 0.9694 - val_loss: 0.1133 - val_accuracy: 0.9661\n",
      "Epoch 84/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0932 - accuracy: 0.9694 - val_loss: 0.1092 - val_accuracy: 0.9676\n",
      "Epoch 85/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0921 - accuracy: 0.9700 - val_loss: 0.1116 - val_accuracy: 0.9662\n",
      "Epoch 86/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0925 - accuracy: 0.9695 - val_loss: 0.1090 - val_accuracy: 0.9672\n",
      "Epoch 87/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0898 - accuracy: 0.9708 - val_loss: 0.1119 - val_accuracy: 0.9657\n",
      "Epoch 88/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0890 - accuracy: 0.9707 - val_loss: 0.1109 - val_accuracy: 0.9665\n",
      "Epoch 89/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0869 - accuracy: 0.9713 - val_loss: 0.1081 - val_accuracy: 0.9676\n",
      "Epoch 90/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0870 - accuracy: 0.9714 - val_loss: 0.1113 - val_accuracy: 0.9673\n",
      "Epoch 91/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0869 - accuracy: 0.9712 - val_loss: 0.1084 - val_accuracy: 0.9668\n",
      "Epoch 92/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0848 - accuracy: 0.9720 - val_loss: 0.1107 - val_accuracy: 0.9668\n",
      "Epoch 93/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0855 - accuracy: 0.9714 - val_loss: 0.1068 - val_accuracy: 0.9674\n",
      "Epoch 94/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0839 - accuracy: 0.9725 - val_loss: 0.1071 - val_accuracy: 0.9672\n",
      "Epoch 95/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0845 - accuracy: 0.9716 - val_loss: 0.1088 - val_accuracy: 0.9669\n",
      "Epoch 96/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0822 - accuracy: 0.9727 - val_loss: 0.1075 - val_accuracy: 0.9679\n",
      "Epoch 97/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0816 - accuracy: 0.9725 - val_loss: 0.1091 - val_accuracy: 0.9679\n",
      "Epoch 98/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0819 - accuracy: 0.9726 - val_loss: 0.1122 - val_accuracy: 0.9664\n",
      "Epoch 99/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0798 - accuracy: 0.9744 - val_loss: 0.1087 - val_accuracy: 0.9676\n",
      "Epoch 100/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0793 - accuracy: 0.9731 - val_loss: 0.1076 - val_accuracy: 0.9675\n",
      "Epoch 101/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0796 - accuracy: 0.9735 - val_loss: 0.1105 - val_accuracy: 0.9680\n",
      "Epoch 102/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0777 - accuracy: 0.9739 - val_loss: 0.1066 - val_accuracy: 0.9678\n",
      "Epoch 103/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0776 - accuracy: 0.9735 - val_loss: 0.1092 - val_accuracy: 0.9685\n",
      "Epoch 104/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0776 - accuracy: 0.9742 - val_loss: 0.1080 - val_accuracy: 0.9684\n",
      "Epoch 105/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0749 - accuracy: 0.9750 - val_loss: 0.1084 - val_accuracy: 0.9681\n",
      "Epoch 106/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0768 - accuracy: 0.9748 - val_loss: 0.1107 - val_accuracy: 0.9678\n",
      "Epoch 107/200\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.0748 - accuracy: 0.9745 - val_loss: 0.1073 - val_accuracy: 0.9681\n",
      "Epoch 108/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0752 - accuracy: 0.9752 - val_loss: 0.1071 - val_accuracy: 0.9679\n",
      "Epoch 109/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0739 - accuracy: 0.9743 - val_loss: 0.1091 - val_accuracy: 0.9675\n",
      "Epoch 110/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0731 - accuracy: 0.9753 - val_loss: 0.1061 - val_accuracy: 0.9684\n",
      "Epoch 111/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0724 - accuracy: 0.9754 - val_loss: 0.1052 - val_accuracy: 0.9696\n",
      "Epoch 112/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0720 - accuracy: 0.9758 - val_loss: 0.1096 - val_accuracy: 0.9682\n",
      "Epoch 113/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0693 - accuracy: 0.9762 - val_loss: 0.1074 - val_accuracy: 0.9685\n",
      "Epoch 114/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0705 - accuracy: 0.9759 - val_loss: 0.1089 - val_accuracy: 0.9683\n",
      "Epoch 115/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0689 - accuracy: 0.9766 - val_loss: 0.1096 - val_accuracy: 0.9679\n",
      "Epoch 116/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0704 - accuracy: 0.9763 - val_loss: 0.1087 - val_accuracy: 0.9688\n",
      "Epoch 117/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0693 - accuracy: 0.9765 - val_loss: 0.1091 - val_accuracy: 0.9689\n",
      "Epoch 118/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0684 - accuracy: 0.9761 - val_loss: 0.1112 - val_accuracy: 0.9685\n",
      "Epoch 119/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0691 - accuracy: 0.9768 - val_loss: 0.1067 - val_accuracy: 0.9691\n",
      "Epoch 120/200\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0676 - accuracy: 0.9766 - val_loss: 0.1082 - val_accuracy: 0.9683\n",
      "Epoch 121/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0685 - accuracy: 0.9763 - val_loss: 0.1080 - val_accuracy: 0.9690\n",
      "Epoch 122/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0664 - accuracy: 0.9775 - val_loss: 0.1091 - val_accuracy: 0.9692\n",
      "Epoch 123/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0650 - accuracy: 0.9783 - val_loss: 0.1084 - val_accuracy: 0.9684\n",
      "Epoch 124/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0659 - accuracy: 0.9775 - val_loss: 0.1078 - val_accuracy: 0.9686\n",
      "Epoch 125/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0657 - accuracy: 0.9775 - val_loss: 0.1076 - val_accuracy: 0.9694\n",
      "Epoch 126/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0648 - accuracy: 0.9783 - val_loss: 0.1087 - val_accuracy: 0.9694\n",
      "Epoch 127/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0653 - accuracy: 0.9773 - val_loss: 0.1079 - val_accuracy: 0.9699\n",
      "Epoch 128/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0635 - accuracy: 0.9782 - val_loss: 0.1073 - val_accuracy: 0.9692\n",
      "Epoch 129/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0627 - accuracy: 0.9784 - val_loss: 0.1068 - val_accuracy: 0.9680\n",
      "Epoch 130/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0623 - accuracy: 0.9791 - val_loss: 0.1076 - val_accuracy: 0.9695\n",
      "Epoch 131/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0625 - accuracy: 0.9783 - val_loss: 0.1085 - val_accuracy: 0.9692\n",
      "Epoch 132/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0616 - accuracy: 0.9791 - val_loss: 0.1078 - val_accuracy: 0.9685\n",
      "Epoch 133/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0618 - accuracy: 0.9789 - val_loss: 0.1080 - val_accuracy: 0.9699\n",
      "Epoch 134/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0606 - accuracy: 0.9790 - val_loss: 0.1089 - val_accuracy: 0.9690\n",
      "Epoch 135/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0603 - accuracy: 0.9797 - val_loss: 0.1079 - val_accuracy: 0.9679\n",
      "Epoch 136/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0586 - accuracy: 0.9801 - val_loss: 0.1096 - val_accuracy: 0.9695\n",
      "Epoch 137/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0600 - accuracy: 0.9792 - val_loss: 0.1061 - val_accuracy: 0.9705\n",
      "Epoch 138/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0585 - accuracy: 0.9798 - val_loss: 0.1103 - val_accuracy: 0.9686\n",
      "Epoch 139/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0582 - accuracy: 0.9798 - val_loss: 0.1101 - val_accuracy: 0.9686\n",
      "Epoch 140/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0588 - accuracy: 0.9793 - val_loss: 0.1096 - val_accuracy: 0.9683\n",
      "Epoch 141/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0584 - accuracy: 0.9799 - val_loss: 0.1081 - val_accuracy: 0.9697\n",
      "Epoch 142/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0566 - accuracy: 0.9806 - val_loss: 0.1090 - val_accuracy: 0.9691\n",
      "Epoch 143/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0565 - accuracy: 0.9801 - val_loss: 0.1107 - val_accuracy: 0.9688\n",
      "Epoch 144/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0563 - accuracy: 0.9803 - val_loss: 0.1094 - val_accuracy: 0.9690\n",
      "Epoch 145/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0560 - accuracy: 0.9808 - val_loss: 0.1093 - val_accuracy: 0.9693\n",
      "Epoch 146/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0564 - accuracy: 0.9803 - val_loss: 0.1087 - val_accuracy: 0.9690\n",
      "Epoch 147/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0560 - accuracy: 0.9807 - val_loss: 0.1106 - val_accuracy: 0.9693\n",
      "Epoch 148/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0550 - accuracy: 0.9813 - val_loss: 0.1095 - val_accuracy: 0.9683\n",
      "Epoch 149/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0544 - accuracy: 0.9813 - val_loss: 0.1099 - val_accuracy: 0.9692\n",
      "Epoch 150/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0558 - accuracy: 0.9805 - val_loss: 0.1092 - val_accuracy: 0.9690\n",
      "Epoch 151/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0539 - accuracy: 0.9815 - val_loss: 0.1102 - val_accuracy: 0.9696\n",
      "Epoch 152/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0556 - accuracy: 0.9810 - val_loss: 0.1108 - val_accuracy: 0.9689\n",
      "Epoch 153/200\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.0542 - accuracy: 0.9810 - val_loss: 0.1111 - val_accuracy: 0.9688\n",
      "Epoch 154/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0539 - accuracy: 0.9813 - val_loss: 0.1117 - val_accuracy: 0.9695\n",
      "Epoch 155/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0523 - accuracy: 0.9821 - val_loss: 0.1104 - val_accuracy: 0.9694\n",
      "Epoch 156/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0515 - accuracy: 0.9822 - val_loss: 0.1104 - val_accuracy: 0.9698\n",
      "Epoch 157/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0517 - accuracy: 0.9819 - val_loss: 0.1120 - val_accuracy: 0.9689\n",
      "Epoch 158/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0508 - accuracy: 0.9818 - val_loss: 0.1103 - val_accuracy: 0.9698\n",
      "Epoch 159/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0507 - accuracy: 0.9822 - val_loss: 0.1110 - val_accuracy: 0.9696\n",
      "Epoch 160/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0499 - accuracy: 0.9825 - val_loss: 0.1123 - val_accuracy: 0.9690\n",
      "Epoch 161/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0504 - accuracy: 0.9826 - val_loss: 0.1134 - val_accuracy: 0.9693\n",
      "Epoch 162/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0508 - accuracy: 0.9819 - val_loss: 0.1122 - val_accuracy: 0.9696\n",
      "Epoch 163/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0490 - accuracy: 0.9831 - val_loss: 0.1118 - val_accuracy: 0.9698\n",
      "Epoch 164/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0504 - accuracy: 0.9827 - val_loss: 0.1113 - val_accuracy: 0.9699\n",
      "Epoch 165/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0493 - accuracy: 0.9825 - val_loss: 0.1102 - val_accuracy: 0.9699\n",
      "Epoch 166/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0489 - accuracy: 0.9829 - val_loss: 0.1117 - val_accuracy: 0.9698\n",
      "Epoch 167/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0485 - accuracy: 0.9831 - val_loss: 0.1136 - val_accuracy: 0.9692\n",
      "Epoch 168/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0483 - accuracy: 0.9829 - val_loss: 0.1112 - val_accuracy: 0.9696\n",
      "Epoch 169/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0475 - accuracy: 0.9836 - val_loss: 0.1109 - val_accuracy: 0.9700\n",
      "Epoch 170/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0482 - accuracy: 0.9833 - val_loss: 0.1113 - val_accuracy: 0.9699\n",
      "Epoch 171/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0465 - accuracy: 0.9840 - val_loss: 0.1150 - val_accuracy: 0.9701\n",
      "Epoch 172/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0483 - accuracy: 0.9834 - val_loss: 0.1131 - val_accuracy: 0.9701\n",
      "Epoch 173/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0469 - accuracy: 0.9837 - val_loss: 0.1142 - val_accuracy: 0.9693\n",
      "Epoch 174/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0472 - accuracy: 0.9837 - val_loss: 0.1131 - val_accuracy: 0.9698\n",
      "Epoch 175/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0469 - accuracy: 0.9834 - val_loss: 0.1121 - val_accuracy: 0.9692\n",
      "Epoch 176/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0459 - accuracy: 0.9841 - val_loss: 0.1122 - val_accuracy: 0.9694\n",
      "Epoch 177/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0458 - accuracy: 0.9838 - val_loss: 0.1138 - val_accuracy: 0.9702\n",
      "Epoch 178/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0449 - accuracy: 0.9844 - val_loss: 0.1157 - val_accuracy: 0.9700\n",
      "Epoch 179/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0455 - accuracy: 0.9839 - val_loss: 0.1118 - val_accuracy: 0.9702\n",
      "Epoch 180/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0455 - accuracy: 0.9841 - val_loss: 0.1138 - val_accuracy: 0.9698\n",
      "Epoch 181/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0448 - accuracy: 0.9839 - val_loss: 0.1138 - val_accuracy: 0.9701\n",
      "Epoch 182/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0455 - accuracy: 0.9838 - val_loss: 0.1145 - val_accuracy: 0.9704\n",
      "Epoch 183/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0446 - accuracy: 0.9838 - val_loss: 0.1155 - val_accuracy: 0.9704\n",
      "Epoch 184/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0439 - accuracy: 0.9842 - val_loss: 0.1124 - val_accuracy: 0.9706\n",
      "Epoch 185/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0437 - accuracy: 0.9850 - val_loss: 0.1168 - val_accuracy: 0.9697\n",
      "Epoch 186/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0443 - accuracy: 0.9845 - val_loss: 0.1150 - val_accuracy: 0.9701\n",
      "Epoch 187/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0426 - accuracy: 0.9849 - val_loss: 0.1136 - val_accuracy: 0.9700\n",
      "Epoch 188/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0437 - accuracy: 0.9848 - val_loss: 0.1160 - val_accuracy: 0.9703\n",
      "Epoch 189/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0421 - accuracy: 0.9855 - val_loss: 0.1140 - val_accuracy: 0.9697\n",
      "Epoch 190/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0435 - accuracy: 0.9850 - val_loss: 0.1156 - val_accuracy: 0.9704\n",
      "Epoch 191/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0431 - accuracy: 0.9851 - val_loss: 0.1169 - val_accuracy: 0.9698\n",
      "Epoch 192/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0432 - accuracy: 0.9848 - val_loss: 0.1148 - val_accuracy: 0.9704\n",
      "Epoch 193/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0428 - accuracy: 0.9849 - val_loss: 0.1153 - val_accuracy: 0.9703\n",
      "Epoch 194/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0418 - accuracy: 0.9859 - val_loss: 0.1158 - val_accuracy: 0.9695\n",
      "Epoch 195/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0421 - accuracy: 0.9853 - val_loss: 0.1162 - val_accuracy: 0.9706\n",
      "Epoch 196/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0406 - accuracy: 0.9857 - val_loss: 0.1166 - val_accuracy: 0.9700\n",
      "Epoch 197/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0411 - accuracy: 0.9857 - val_loss: 0.1147 - val_accuracy: 0.9691\n",
      "Epoch 198/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0421 - accuracy: 0.9851 - val_loss: 0.1158 - val_accuracy: 0.9704\n",
      "Epoch 199/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0398 - accuracy: 0.9859 - val_loss: 0.1147 - val_accuracy: 0.9707\n",
      "Epoch 200/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0416 - accuracy: 0.9849 - val_loss: 0.1159 - val_accuracy: 0.9699\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "\n",
      "Fold 3:\n",
      "X_train shape: (86691, 1, 1280)\n",
      "X_val shape: (9633, 1, 1280)\n",
      "Epoch 1/200\n",
      "43/43 [==============================] - 4s 22ms/step - loss: 1.3112 - accuracy: 0.5566 - val_loss: 0.9951 - val_accuracy: 0.6549\n",
      "Epoch 2/200\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.8367 - accuracy: 0.7263 - val_loss: 0.6267 - val_accuracy: 0.8178\n",
      "Epoch 3/200\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.5894 - accuracy: 0.8238 - val_loss: 0.4526 - val_accuracy: 0.8693\n",
      "Epoch 4/200\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.4857 - accuracy: 0.8574 - val_loss: 0.3733 - val_accuracy: 0.8942\n",
      "Epoch 5/200\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.4367 - accuracy: 0.8717 - val_loss: 0.3307 - val_accuracy: 0.9054\n",
      "Epoch 6/200\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.4080 - accuracy: 0.8781 - val_loss: 0.3041 - val_accuracy: 0.9136\n",
      "Epoch 7/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3872 - accuracy: 0.8841 - val_loss: 0.2864 - val_accuracy: 0.9177\n",
      "Epoch 8/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3702 - accuracy: 0.8884 - val_loss: 0.2696 - val_accuracy: 0.9247\n",
      "Epoch 9/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3591 - accuracy: 0.8905 - val_loss: 0.2635 - val_accuracy: 0.9253\n",
      "Epoch 10/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3464 - accuracy: 0.8935 - val_loss: 0.2472 - val_accuracy: 0.9328\n",
      "Epoch 11/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3386 - accuracy: 0.8956 - val_loss: 0.2404 - val_accuracy: 0.9336\n",
      "Epoch 12/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3283 - accuracy: 0.8989 - val_loss: 0.2375 - val_accuracy: 0.9336\n",
      "Epoch 13/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3231 - accuracy: 0.8994 - val_loss: 0.2283 - val_accuracy: 0.9359\n",
      "Epoch 14/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3105 - accuracy: 0.9033 - val_loss: 0.2213 - val_accuracy: 0.9380\n",
      "Epoch 15/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3035 - accuracy: 0.9061 - val_loss: 0.2137 - val_accuracy: 0.9396\n",
      "Epoch 16/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2938 - accuracy: 0.9097 - val_loss: 0.2100 - val_accuracy: 0.9411\n",
      "Epoch 17/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2879 - accuracy: 0.9102 - val_loss: 0.2024 - val_accuracy: 0.9418\n",
      "Epoch 18/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2776 - accuracy: 0.9134 - val_loss: 0.1986 - val_accuracy: 0.9431\n",
      "Epoch 19/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2707 - accuracy: 0.9160 - val_loss: 0.1960 - val_accuracy: 0.9424\n",
      "Epoch 20/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2636 - accuracy: 0.9191 - val_loss: 0.1884 - val_accuracy: 0.9456\n",
      "Epoch 21/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2576 - accuracy: 0.9195 - val_loss: 0.1851 - val_accuracy: 0.9463\n",
      "Epoch 22/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2492 - accuracy: 0.9235 - val_loss: 0.1808 - val_accuracy: 0.9466\n",
      "Epoch 23/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2412 - accuracy: 0.9247 - val_loss: 0.1776 - val_accuracy: 0.9479\n",
      "Epoch 24/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2367 - accuracy: 0.9267 - val_loss: 0.1730 - val_accuracy: 0.9487\n",
      "Epoch 25/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2290 - accuracy: 0.9309 - val_loss: 0.1701 - val_accuracy: 0.9495\n",
      "Epoch 26/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2237 - accuracy: 0.9312 - val_loss: 0.1723 - val_accuracy: 0.9488\n",
      "Epoch 27/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2185 - accuracy: 0.9319 - val_loss: 0.1652 - val_accuracy: 0.9508\n",
      "Epoch 28/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2137 - accuracy: 0.9346 - val_loss: 0.1628 - val_accuracy: 0.9516\n",
      "Epoch 29/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2076 - accuracy: 0.9365 - val_loss: 0.1644 - val_accuracy: 0.9518\n",
      "Epoch 30/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2031 - accuracy: 0.9387 - val_loss: 0.1581 - val_accuracy: 0.9525\n",
      "Epoch 31/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1970 - accuracy: 0.9396 - val_loss: 0.1529 - val_accuracy: 0.9540\n",
      "Epoch 32/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1923 - accuracy: 0.9411 - val_loss: 0.1526 - val_accuracy: 0.9540\n",
      "Epoch 33/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1885 - accuracy: 0.9414 - val_loss: 0.1528 - val_accuracy: 0.9542\n",
      "Epoch 34/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1828 - accuracy: 0.9437 - val_loss: 0.1481 - val_accuracy: 0.9554\n",
      "Epoch 35/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1779 - accuracy: 0.9457 - val_loss: 0.1477 - val_accuracy: 0.9563\n",
      "Epoch 36/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1761 - accuracy: 0.9463 - val_loss: 0.1443 - val_accuracy: 0.9566\n",
      "Epoch 37/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1728 - accuracy: 0.9468 - val_loss: 0.1425 - val_accuracy: 0.9567\n",
      "Epoch 38/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1671 - accuracy: 0.9487 - val_loss: 0.1403 - val_accuracy: 0.9572\n",
      "Epoch 39/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1658 - accuracy: 0.9491 - val_loss: 0.1404 - val_accuracy: 0.9574\n",
      "Epoch 40/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1621 - accuracy: 0.9497 - val_loss: 0.1388 - val_accuracy: 0.9576\n",
      "Epoch 41/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1595 - accuracy: 0.9509 - val_loss: 0.1375 - val_accuracy: 0.9581\n",
      "Epoch 42/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1557 - accuracy: 0.9528 - val_loss: 0.1360 - val_accuracy: 0.9594\n",
      "Epoch 43/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.1536 - accuracy: 0.9516 - val_loss: 0.1366 - val_accuracy: 0.9593\n",
      "Epoch 44/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1510 - accuracy: 0.9523 - val_loss: 0.1344 - val_accuracy: 0.9600\n",
      "Epoch 45/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1494 - accuracy: 0.9541 - val_loss: 0.1328 - val_accuracy: 0.9596\n",
      "Epoch 46/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1460 - accuracy: 0.9543 - val_loss: 0.1340 - val_accuracy: 0.9602\n",
      "Epoch 47/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1431 - accuracy: 0.9551 - val_loss: 0.1303 - val_accuracy: 0.9602\n",
      "Epoch 48/200\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.1399 - accuracy: 0.9567 - val_loss: 0.1304 - val_accuracy: 0.9612\n",
      "Epoch 49/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1407 - accuracy: 0.9560 - val_loss: 0.1306 - val_accuracy: 0.9614\n",
      "Epoch 50/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1366 - accuracy: 0.9572 - val_loss: 0.1277 - val_accuracy: 0.9609\n",
      "Epoch 51/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.1346 - accuracy: 0.9580 - val_loss: 0.1277 - val_accuracy: 0.9617\n",
      "Epoch 52/200\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.1333 - accuracy: 0.9582 - val_loss: 0.1264 - val_accuracy: 0.9610\n",
      "Epoch 53/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1318 - accuracy: 0.9585 - val_loss: 0.1258 - val_accuracy: 0.9611\n",
      "Epoch 54/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1299 - accuracy: 0.9594 - val_loss: 0.1240 - val_accuracy: 0.9610\n",
      "Epoch 55/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1284 - accuracy: 0.9594 - val_loss: 0.1234 - val_accuracy: 0.9624\n",
      "Epoch 56/200\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.1262 - accuracy: 0.9596 - val_loss: 0.1230 - val_accuracy: 0.9613\n",
      "Epoch 57/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1236 - accuracy: 0.9606 - val_loss: 0.1246 - val_accuracy: 0.9613\n",
      "Epoch 58/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1233 - accuracy: 0.9601 - val_loss: 0.1234 - val_accuracy: 0.9616\n",
      "Epoch 59/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1210 - accuracy: 0.9616 - val_loss: 0.1228 - val_accuracy: 0.9618\n",
      "Epoch 60/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1183 - accuracy: 0.9627 - val_loss: 0.1217 - val_accuracy: 0.9617\n",
      "Epoch 61/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1181 - accuracy: 0.9629 - val_loss: 0.1205 - val_accuracy: 0.9629\n",
      "Epoch 62/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1168 - accuracy: 0.9630 - val_loss: 0.1198 - val_accuracy: 0.9638\n",
      "Epoch 63/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1160 - accuracy: 0.9627 - val_loss: 0.1183 - val_accuracy: 0.9636\n",
      "Epoch 64/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1137 - accuracy: 0.9639 - val_loss: 0.1200 - val_accuracy: 0.9637\n",
      "Epoch 65/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1132 - accuracy: 0.9637 - val_loss: 0.1186 - val_accuracy: 0.9642\n",
      "Epoch 66/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1120 - accuracy: 0.9642 - val_loss: 0.1166 - val_accuracy: 0.9638\n",
      "Epoch 67/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1103 - accuracy: 0.9645 - val_loss: 0.1180 - val_accuracy: 0.9641\n",
      "Epoch 68/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1090 - accuracy: 0.9648 - val_loss: 0.1166 - val_accuracy: 0.9642\n",
      "Epoch 69/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1087 - accuracy: 0.9649 - val_loss: 0.1161 - val_accuracy: 0.9645\n",
      "Epoch 70/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1068 - accuracy: 0.9653 - val_loss: 0.1153 - val_accuracy: 0.9650\n",
      "Epoch 71/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1048 - accuracy: 0.9662 - val_loss: 0.1141 - val_accuracy: 0.9646\n",
      "Epoch 72/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1043 - accuracy: 0.9666 - val_loss: 0.1152 - val_accuracy: 0.9641\n",
      "Epoch 73/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1029 - accuracy: 0.9662 - val_loss: 0.1144 - val_accuracy: 0.9651\n",
      "Epoch 74/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1022 - accuracy: 0.9667 - val_loss: 0.1151 - val_accuracy: 0.9652\n",
      "Epoch 75/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1022 - accuracy: 0.9665 - val_loss: 0.1155 - val_accuracy: 0.9656\n",
      "Epoch 76/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1010 - accuracy: 0.9676 - val_loss: 0.1143 - val_accuracy: 0.9646\n",
      "Epoch 77/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1006 - accuracy: 0.9671 - val_loss: 0.1137 - val_accuracy: 0.9652\n",
      "Epoch 78/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1002 - accuracy: 0.9672 - val_loss: 0.1135 - val_accuracy: 0.9655\n",
      "Epoch 79/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0973 - accuracy: 0.9681 - val_loss: 0.1149 - val_accuracy: 0.9650\n",
      "Epoch 80/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0956 - accuracy: 0.9686 - val_loss: 0.1138 - val_accuracy: 0.9654\n",
      "Epoch 81/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0959 - accuracy: 0.9684 - val_loss: 0.1126 - val_accuracy: 0.9648\n",
      "Epoch 82/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0946 - accuracy: 0.9690 - val_loss: 0.1129 - val_accuracy: 0.9657\n",
      "Epoch 83/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0924 - accuracy: 0.9701 - val_loss: 0.1136 - val_accuracy: 0.9656\n",
      "Epoch 84/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0926 - accuracy: 0.9689 - val_loss: 0.1116 - val_accuracy: 0.9656\n",
      "Epoch 85/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0915 - accuracy: 0.9696 - val_loss: 0.1130 - val_accuracy: 0.9660\n",
      "Epoch 86/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0918 - accuracy: 0.9695 - val_loss: 0.1109 - val_accuracy: 0.9661\n",
      "Epoch 87/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0911 - accuracy: 0.9699 - val_loss: 0.1104 - val_accuracy: 0.9660\n",
      "Epoch 88/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0879 - accuracy: 0.9707 - val_loss: 0.1100 - val_accuracy: 0.9656\n",
      "Epoch 89/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0879 - accuracy: 0.9707 - val_loss: 0.1116 - val_accuracy: 0.9652\n",
      "Epoch 90/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0872 - accuracy: 0.9721 - val_loss: 0.1119 - val_accuracy: 0.9660\n",
      "Epoch 91/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0861 - accuracy: 0.9714 - val_loss: 0.1097 - val_accuracy: 0.9655\n",
      "Epoch 92/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0861 - accuracy: 0.9714 - val_loss: 0.1098 - val_accuracy: 0.9657\n",
      "Epoch 93/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0854 - accuracy: 0.9723 - val_loss: 0.1104 - val_accuracy: 0.9660\n",
      "Epoch 94/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0838 - accuracy: 0.9720 - val_loss: 0.1095 - val_accuracy: 0.9663\n",
      "Epoch 95/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0833 - accuracy: 0.9723 - val_loss: 0.1102 - val_accuracy: 0.9657\n",
      "Epoch 96/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0824 - accuracy: 0.9724 - val_loss: 0.1121 - val_accuracy: 0.9669\n",
      "Epoch 97/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0823 - accuracy: 0.9722 - val_loss: 0.1097 - val_accuracy: 0.9665\n",
      "Epoch 98/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0821 - accuracy: 0.9722 - val_loss: 0.1097 - val_accuracy: 0.9672\n",
      "Epoch 99/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0799 - accuracy: 0.9731 - val_loss: 0.1095 - val_accuracy: 0.9670\n",
      "Epoch 100/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0800 - accuracy: 0.9735 - val_loss: 0.1088 - val_accuracy: 0.9673\n",
      "Epoch 101/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0793 - accuracy: 0.9734 - val_loss: 0.1091 - val_accuracy: 0.9674\n",
      "Epoch 102/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0780 - accuracy: 0.9740 - val_loss: 0.1083 - val_accuracy: 0.9664\n",
      "Epoch 103/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0756 - accuracy: 0.9751 - val_loss: 0.1096 - val_accuracy: 0.9678\n",
      "Epoch 104/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0769 - accuracy: 0.9743 - val_loss: 0.1099 - val_accuracy: 0.9677\n",
      "Epoch 105/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0749 - accuracy: 0.9747 - val_loss: 0.1080 - val_accuracy: 0.9674\n",
      "Epoch 106/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0749 - accuracy: 0.9743 - val_loss: 0.1100 - val_accuracy: 0.9671\n",
      "Epoch 107/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0756 - accuracy: 0.9738 - val_loss: 0.1085 - val_accuracy: 0.9665\n",
      "Epoch 108/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0744 - accuracy: 0.9747 - val_loss: 0.1090 - val_accuracy: 0.9672\n",
      "Epoch 109/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0739 - accuracy: 0.9750 - val_loss: 0.1086 - val_accuracy: 0.9670\n",
      "Epoch 110/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0743 - accuracy: 0.9748 - val_loss: 0.1087 - val_accuracy: 0.9673\n",
      "Epoch 111/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0727 - accuracy: 0.9750 - val_loss: 0.1086 - val_accuracy: 0.9668\n",
      "Epoch 112/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0722 - accuracy: 0.9754 - val_loss: 0.1096 - val_accuracy: 0.9675\n",
      "Epoch 113/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0716 - accuracy: 0.9749 - val_loss: 0.1089 - val_accuracy: 0.9683\n",
      "Epoch 114/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0702 - accuracy: 0.9762 - val_loss: 0.1084 - val_accuracy: 0.9681\n",
      "Epoch 115/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0697 - accuracy: 0.9760 - val_loss: 0.1084 - val_accuracy: 0.9675\n",
      "Epoch 116/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0707 - accuracy: 0.9758 - val_loss: 0.1092 - val_accuracy: 0.9682\n",
      "Epoch 117/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0683 - accuracy: 0.9770 - val_loss: 0.1119 - val_accuracy: 0.9679\n",
      "Epoch 118/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0686 - accuracy: 0.9763 - val_loss: 0.1099 - val_accuracy: 0.9681\n",
      "Epoch 119/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0675 - accuracy: 0.9772 - val_loss: 0.1083 - val_accuracy: 0.9686\n",
      "Epoch 120/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0668 - accuracy: 0.9773 - val_loss: 0.1097 - val_accuracy: 0.9676\n",
      "Epoch 121/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0671 - accuracy: 0.9770 - val_loss: 0.1099 - val_accuracy: 0.9680\n",
      "Epoch 122/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0653 - accuracy: 0.9780 - val_loss: 0.1099 - val_accuracy: 0.9683\n",
      "Epoch 123/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0661 - accuracy: 0.9769 - val_loss: 0.1104 - val_accuracy: 0.9682\n",
      "Epoch 124/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0654 - accuracy: 0.9775 - val_loss: 0.1124 - val_accuracy: 0.9676\n",
      "Epoch 125/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0648 - accuracy: 0.9776 - val_loss: 0.1102 - val_accuracy: 0.9673\n",
      "Epoch 126/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0649 - accuracy: 0.9774 - val_loss: 0.1086 - val_accuracy: 0.9678\n",
      "Epoch 127/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0633 - accuracy: 0.9784 - val_loss: 0.1092 - val_accuracy: 0.9686\n",
      "Epoch 128/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0629 - accuracy: 0.9780 - val_loss: 0.1106 - val_accuracy: 0.9677\n",
      "Epoch 129/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0623 - accuracy: 0.9785 - val_loss: 0.1093 - val_accuracy: 0.9685\n",
      "Epoch 130/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0630 - accuracy: 0.9783 - val_loss: 0.1094 - val_accuracy: 0.9676\n",
      "Epoch 131/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0629 - accuracy: 0.9782 - val_loss: 0.1100 - val_accuracy: 0.9686\n",
      "Epoch 132/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0608 - accuracy: 0.9788 - val_loss: 0.1109 - val_accuracy: 0.9685\n",
      "Epoch 133/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0616 - accuracy: 0.9790 - val_loss: 0.1088 - val_accuracy: 0.9689\n",
      "Epoch 134/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0607 - accuracy: 0.9794 - val_loss: 0.1109 - val_accuracy: 0.9693\n",
      "Epoch 135/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0599 - accuracy: 0.9791 - val_loss: 0.1101 - val_accuracy: 0.9685\n",
      "Epoch 136/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0597 - accuracy: 0.9799 - val_loss: 0.1099 - val_accuracy: 0.9686\n",
      "Epoch 137/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0590 - accuracy: 0.9797 - val_loss: 0.1111 - val_accuracy: 0.9698\n",
      "Epoch 138/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0582 - accuracy: 0.9801 - val_loss: 0.1127 - val_accuracy: 0.9686\n",
      "Epoch 139/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0595 - accuracy: 0.9795 - val_loss: 0.1095 - val_accuracy: 0.9695\n",
      "Epoch 140/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0581 - accuracy: 0.9795 - val_loss: 0.1108 - val_accuracy: 0.9690\n",
      "Epoch 141/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0580 - accuracy: 0.9802 - val_loss: 0.1116 - val_accuracy: 0.9688\n",
      "Epoch 142/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0569 - accuracy: 0.9804 - val_loss: 0.1116 - val_accuracy: 0.9692\n",
      "Epoch 143/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0577 - accuracy: 0.9800 - val_loss: 0.1111 - val_accuracy: 0.9689\n",
      "Epoch 144/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0575 - accuracy: 0.9798 - val_loss: 0.1119 - val_accuracy: 0.9690\n",
      "Epoch 145/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0562 - accuracy: 0.9808 - val_loss: 0.1112 - val_accuracy: 0.9697\n",
      "Epoch 146/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0554 - accuracy: 0.9804 - val_loss: 0.1100 - val_accuracy: 0.9684\n",
      "Epoch 147/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0547 - accuracy: 0.9809 - val_loss: 0.1137 - val_accuracy: 0.9685\n",
      "Epoch 148/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0553 - accuracy: 0.9806 - val_loss: 0.1118 - val_accuracy: 0.9694\n",
      "Epoch 149/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0552 - accuracy: 0.9805 - val_loss: 0.1102 - val_accuracy: 0.9694\n",
      "Epoch 150/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0540 - accuracy: 0.9811 - val_loss: 0.1114 - val_accuracy: 0.9692\n",
      "Epoch 151/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0530 - accuracy: 0.9817 - val_loss: 0.1117 - val_accuracy: 0.9691\n",
      "Epoch 152/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0543 - accuracy: 0.9813 - val_loss: 0.1121 - val_accuracy: 0.9690\n",
      "Epoch 153/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0532 - accuracy: 0.9818 - val_loss: 0.1122 - val_accuracy: 0.9686\n",
      "Epoch 154/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0527 - accuracy: 0.9817 - val_loss: 0.1142 - val_accuracy: 0.9691\n",
      "Epoch 155/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0523 - accuracy: 0.9818 - val_loss: 0.1114 - val_accuracy: 0.9683\n",
      "Epoch 156/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0529 - accuracy: 0.9817 - val_loss: 0.1137 - val_accuracy: 0.9696\n",
      "Epoch 157/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0521 - accuracy: 0.9818 - val_loss: 0.1132 - val_accuracy: 0.9696\n",
      "Epoch 158/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0527 - accuracy: 0.9814 - val_loss: 0.1126 - val_accuracy: 0.9694\n",
      "Epoch 159/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0510 - accuracy: 0.9815 - val_loss: 0.1156 - val_accuracy: 0.9699\n",
      "Epoch 160/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0512 - accuracy: 0.9824 - val_loss: 0.1122 - val_accuracy: 0.9685\n",
      "Epoch 161/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0504 - accuracy: 0.9825 - val_loss: 0.1140 - val_accuracy: 0.9691\n",
      "Epoch 162/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0515 - accuracy: 0.9817 - val_loss: 0.1124 - val_accuracy: 0.9693\n",
      "Epoch 163/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0500 - accuracy: 0.9826 - val_loss: 0.1122 - val_accuracy: 0.9690\n",
      "Epoch 164/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0498 - accuracy: 0.9822 - val_loss: 0.1157 - val_accuracy: 0.9691\n",
      "Epoch 165/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0495 - accuracy: 0.9827 - val_loss: 0.1146 - val_accuracy: 0.9696\n",
      "Epoch 166/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0492 - accuracy: 0.9831 - val_loss: 0.1143 - val_accuracy: 0.9696\n",
      "Epoch 167/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0484 - accuracy: 0.9832 - val_loss: 0.1181 - val_accuracy: 0.9697\n",
      "Epoch 168/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0491 - accuracy: 0.9831 - val_loss: 0.1139 - val_accuracy: 0.9699\n",
      "Epoch 169/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0483 - accuracy: 0.9830 - val_loss: 0.1144 - val_accuracy: 0.9694\n",
      "Epoch 170/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0467 - accuracy: 0.9838 - val_loss: 0.1155 - val_accuracy: 0.9690\n",
      "Epoch 171/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0483 - accuracy: 0.9830 - val_loss: 0.1148 - val_accuracy: 0.9701\n",
      "Epoch 172/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0470 - accuracy: 0.9837 - val_loss: 0.1192 - val_accuracy: 0.9696\n",
      "Epoch 173/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0484 - accuracy: 0.9827 - val_loss: 0.1134 - val_accuracy: 0.9697\n",
      "Epoch 174/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0463 - accuracy: 0.9838 - val_loss: 0.1149 - val_accuracy: 0.9688\n",
      "Epoch 175/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0461 - accuracy: 0.9838 - val_loss: 0.1149 - val_accuracy: 0.9700\n",
      "Epoch 176/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0457 - accuracy: 0.9842 - val_loss: 0.1150 - val_accuracy: 0.9690\n",
      "Epoch 177/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0461 - accuracy: 0.9838 - val_loss: 0.1161 - val_accuracy: 0.9703\n",
      "Epoch 178/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0441 - accuracy: 0.9843 - val_loss: 0.1153 - val_accuracy: 0.9703\n",
      "Epoch 179/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0452 - accuracy: 0.9839 - val_loss: 0.1175 - val_accuracy: 0.9701\n",
      "Epoch 180/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0466 - accuracy: 0.9838 - val_loss: 0.1147 - val_accuracy: 0.9696\n",
      "Epoch 181/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0444 - accuracy: 0.9846 - val_loss: 0.1151 - val_accuracy: 0.9694\n",
      "Epoch 182/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0441 - accuracy: 0.9843 - val_loss: 0.1172 - val_accuracy: 0.9695\n",
      "Epoch 183/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0439 - accuracy: 0.9845 - val_loss: 0.1190 - val_accuracy: 0.9699\n",
      "Epoch 184/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0438 - accuracy: 0.9850 - val_loss: 0.1159 - val_accuracy: 0.9689\n",
      "Epoch 185/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0438 - accuracy: 0.9845 - val_loss: 0.1185 - val_accuracy: 0.9697\n",
      "Epoch 186/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0424 - accuracy: 0.9852 - val_loss: 0.1191 - val_accuracy: 0.9691\n",
      "Epoch 187/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0442 - accuracy: 0.9844 - val_loss: 0.1180 - val_accuracy: 0.9702\n",
      "Epoch 188/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0417 - accuracy: 0.9852 - val_loss: 0.1183 - val_accuracy: 0.9700\n",
      "Epoch 189/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0419 - accuracy: 0.9857 - val_loss: 0.1187 - val_accuracy: 0.9695\n",
      "Epoch 190/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0429 - accuracy: 0.9851 - val_loss: 0.1177 - val_accuracy: 0.9705\n",
      "Epoch 191/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0426 - accuracy: 0.9849 - val_loss: 0.1167 - val_accuracy: 0.9703\n",
      "Epoch 192/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0415 - accuracy: 0.9855 - val_loss: 0.1187 - val_accuracy: 0.9703\n",
      "Epoch 193/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0417 - accuracy: 0.9854 - val_loss: 0.1166 - val_accuracy: 0.9698\n",
      "Epoch 194/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0411 - accuracy: 0.9856 - val_loss: 0.1181 - val_accuracy: 0.9696\n",
      "Epoch 195/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0425 - accuracy: 0.9852 - val_loss: 0.1180 - val_accuracy: 0.9700\n",
      "Epoch 196/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0420 - accuracy: 0.9851 - val_loss: 0.1178 - val_accuracy: 0.9701\n",
      "Epoch 197/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0413 - accuracy: 0.9859 - val_loss: 0.1197 - val_accuracy: 0.9703\n",
      "Epoch 198/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0408 - accuracy: 0.9853 - val_loss: 0.1154 - val_accuracy: 0.9695\n",
      "Epoch 199/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0420 - accuracy: 0.9855 - val_loss: 0.1167 - val_accuracy: 0.9707\n",
      "Epoch 200/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0399 - accuracy: 0.9862 - val_loss: 0.1170 - val_accuracy: 0.9696\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "\n",
      "Fold 4:\n",
      "X_train shape: (86691, 1, 1280)\n",
      "X_val shape: (9633, 1, 1280)\n",
      "Epoch 1/200\n",
      "43/43 [==============================] - 4s 22ms/step - loss: 1.2919 - accuracy: 0.5680 - val_loss: 0.9623 - val_accuracy: 0.6591\n",
      "Epoch 2/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.8103 - accuracy: 0.7377 - val_loss: 0.6090 - val_accuracy: 0.8165\n",
      "Epoch 3/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.5755 - accuracy: 0.8261 - val_loss: 0.4444 - val_accuracy: 0.8779\n",
      "Epoch 4/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.4762 - accuracy: 0.8608 - val_loss: 0.3733 - val_accuracy: 0.8946\n",
      "Epoch 5/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.4324 - accuracy: 0.8716 - val_loss: 0.3335 - val_accuracy: 0.9063\n",
      "Epoch 6/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.4033 - accuracy: 0.8805 - val_loss: 0.3098 - val_accuracy: 0.9139\n",
      "Epoch 7/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3841 - accuracy: 0.8846 - val_loss: 0.2866 - val_accuracy: 0.9212\n",
      "Epoch 8/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3690 - accuracy: 0.8878 - val_loss: 0.2713 - val_accuracy: 0.9252\n",
      "Epoch 9/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3582 - accuracy: 0.8906 - val_loss: 0.2615 - val_accuracy: 0.9266\n",
      "Epoch 10/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3459 - accuracy: 0.8931 - val_loss: 0.2518 - val_accuracy: 0.9289\n",
      "Epoch 11/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3360 - accuracy: 0.8963 - val_loss: 0.2438 - val_accuracy: 0.9313\n",
      "Epoch 12/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3270 - accuracy: 0.8991 - val_loss: 0.2345 - val_accuracy: 0.9333\n",
      "Epoch 13/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3150 - accuracy: 0.9026 - val_loss: 0.2257 - val_accuracy: 0.9362\n",
      "Epoch 14/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3058 - accuracy: 0.9053 - val_loss: 0.2199 - val_accuracy: 0.9351\n",
      "Epoch 15/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2990 - accuracy: 0.9075 - val_loss: 0.2144 - val_accuracy: 0.9366\n",
      "Epoch 16/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2882 - accuracy: 0.9112 - val_loss: 0.2085 - val_accuracy: 0.9393\n",
      "Epoch 17/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2830 - accuracy: 0.9122 - val_loss: 0.2035 - val_accuracy: 0.9389\n",
      "Epoch 18/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2747 - accuracy: 0.9151 - val_loss: 0.1983 - val_accuracy: 0.9403\n",
      "Epoch 19/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2661 - accuracy: 0.9186 - val_loss: 0.1925 - val_accuracy: 0.9435\n",
      "Epoch 20/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2577 - accuracy: 0.9208 - val_loss: 0.1889 - val_accuracy: 0.9422\n",
      "Epoch 21/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2494 - accuracy: 0.9224 - val_loss: 0.1847 - val_accuracy: 0.9435\n",
      "Epoch 22/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2439 - accuracy: 0.9247 - val_loss: 0.1799 - val_accuracy: 0.9475\n",
      "Epoch 23/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2370 - accuracy: 0.9277 - val_loss: 0.1827 - val_accuracy: 0.9426\n",
      "Epoch 24/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2311 - accuracy: 0.9295 - val_loss: 0.1752 - val_accuracy: 0.9451\n",
      "Epoch 25/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2239 - accuracy: 0.9316 - val_loss: 0.1700 - val_accuracy: 0.9483\n",
      "Epoch 26/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2174 - accuracy: 0.9332 - val_loss: 0.1682 - val_accuracy: 0.9479\n",
      "Epoch 27/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2119 - accuracy: 0.9345 - val_loss: 0.1655 - val_accuracy: 0.9487\n",
      "Epoch 28/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2071 - accuracy: 0.9363 - val_loss: 0.1606 - val_accuracy: 0.9506\n",
      "Epoch 29/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2013 - accuracy: 0.9381 - val_loss: 0.1613 - val_accuracy: 0.9493\n",
      "Epoch 30/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1979 - accuracy: 0.9388 - val_loss: 0.1562 - val_accuracy: 0.9516\n",
      "Epoch 31/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1920 - accuracy: 0.9416 - val_loss: 0.1562 - val_accuracy: 0.9510\n",
      "Epoch 32/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1894 - accuracy: 0.9425 - val_loss: 0.1527 - val_accuracy: 0.9526\n",
      "Epoch 33/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1834 - accuracy: 0.9439 - val_loss: 0.1489 - val_accuracy: 0.9543\n",
      "Epoch 34/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1800 - accuracy: 0.9446 - val_loss: 0.1489 - val_accuracy: 0.9537\n",
      "Epoch 35/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1769 - accuracy: 0.9461 - val_loss: 0.1474 - val_accuracy: 0.9540\n",
      "Epoch 36/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1733 - accuracy: 0.9471 - val_loss: 0.1429 - val_accuracy: 0.9559\n",
      "Epoch 37/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1700 - accuracy: 0.9479 - val_loss: 0.1438 - val_accuracy: 0.9559\n",
      "Epoch 38/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1651 - accuracy: 0.9495 - val_loss: 0.1424 - val_accuracy: 0.9543\n",
      "Epoch 39/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1640 - accuracy: 0.9495 - val_loss: 0.1372 - val_accuracy: 0.9573\n",
      "Epoch 40/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1601 - accuracy: 0.9510 - val_loss: 0.1371 - val_accuracy: 0.9567\n",
      "Epoch 41/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1581 - accuracy: 0.9511 - val_loss: 0.1362 - val_accuracy: 0.9573\n",
      "Epoch 42/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1538 - accuracy: 0.9525 - val_loss: 0.1345 - val_accuracy: 0.9575\n",
      "Epoch 43/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1516 - accuracy: 0.9530 - val_loss: 0.1353 - val_accuracy: 0.9573\n",
      "Epoch 44/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1489 - accuracy: 0.9538 - val_loss: 0.1337 - val_accuracy: 0.9582\n",
      "Epoch 45/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1474 - accuracy: 0.9541 - val_loss: 0.1308 - val_accuracy: 0.9575\n",
      "Epoch 46/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1450 - accuracy: 0.9544 - val_loss: 0.1296 - val_accuracy: 0.9591\n",
      "Epoch 47/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1416 - accuracy: 0.9562 - val_loss: 0.1278 - val_accuracy: 0.9591\n",
      "Epoch 48/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1398 - accuracy: 0.9565 - val_loss: 0.1294 - val_accuracy: 0.9583\n",
      "Epoch 49/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1398 - accuracy: 0.9561 - val_loss: 0.1276 - val_accuracy: 0.9591\n",
      "Epoch 50/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1377 - accuracy: 0.9574 - val_loss: 0.1236 - val_accuracy: 0.9611\n",
      "Epoch 51/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1359 - accuracy: 0.9573 - val_loss: 0.1270 - val_accuracy: 0.9585\n",
      "Epoch 52/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1332 - accuracy: 0.9581 - val_loss: 0.1234 - val_accuracy: 0.9609\n",
      "Epoch 53/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1304 - accuracy: 0.9592 - val_loss: 0.1226 - val_accuracy: 0.9602\n",
      "Epoch 54/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1280 - accuracy: 0.9596 - val_loss: 0.1230 - val_accuracy: 0.9613\n",
      "Epoch 55/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1274 - accuracy: 0.9597 - val_loss: 0.1280 - val_accuracy: 0.9585\n",
      "Epoch 56/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1276 - accuracy: 0.9595 - val_loss: 0.1206 - val_accuracy: 0.9612\n",
      "Epoch 57/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1234 - accuracy: 0.9610 - val_loss: 0.1186 - val_accuracy: 0.9622\n",
      "Epoch 58/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1231 - accuracy: 0.9611 - val_loss: 0.1205 - val_accuracy: 0.9614\n",
      "Epoch 59/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1217 - accuracy: 0.9614 - val_loss: 0.1190 - val_accuracy: 0.9626\n",
      "Epoch 60/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1197 - accuracy: 0.9619 - val_loss: 0.1174 - val_accuracy: 0.9628\n",
      "Epoch 61/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1180 - accuracy: 0.9627 - val_loss: 0.1185 - val_accuracy: 0.9622\n",
      "Epoch 62/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1160 - accuracy: 0.9627 - val_loss: 0.1190 - val_accuracy: 0.9617\n",
      "Epoch 63/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1146 - accuracy: 0.9636 - val_loss: 0.1145 - val_accuracy: 0.9647\n",
      "Epoch 64/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1147 - accuracy: 0.9634 - val_loss: 0.1187 - val_accuracy: 0.9625\n",
      "Epoch 65/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1141 - accuracy: 0.9635 - val_loss: 0.1145 - val_accuracy: 0.9638\n",
      "Epoch 66/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1127 - accuracy: 0.9639 - val_loss: 0.1140 - val_accuracy: 0.9639\n",
      "Epoch 67/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1115 - accuracy: 0.9639 - val_loss: 0.1157 - val_accuracy: 0.9630\n",
      "Epoch 68/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1096 - accuracy: 0.9644 - val_loss: 0.1143 - val_accuracy: 0.9635\n",
      "Epoch 69/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1082 - accuracy: 0.9650 - val_loss: 0.1123 - val_accuracy: 0.9647\n",
      "Epoch 70/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1064 - accuracy: 0.9655 - val_loss: 0.1146 - val_accuracy: 0.9641\n",
      "Epoch 71/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1056 - accuracy: 0.9662 - val_loss: 0.1135 - val_accuracy: 0.9641\n",
      "Epoch 72/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1047 - accuracy: 0.9662 - val_loss: 0.1116 - val_accuracy: 0.9653\n",
      "Epoch 73/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1032 - accuracy: 0.9667 - val_loss: 0.1115 - val_accuracy: 0.9654\n",
      "Epoch 74/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1020 - accuracy: 0.9665 - val_loss: 0.1111 - val_accuracy: 0.9652\n",
      "Epoch 75/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1017 - accuracy: 0.9670 - val_loss: 0.1127 - val_accuracy: 0.9643\n",
      "Epoch 76/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1017 - accuracy: 0.9668 - val_loss: 0.1096 - val_accuracy: 0.9657\n",
      "Epoch 77/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0980 - accuracy: 0.9680 - val_loss: 0.1104 - val_accuracy: 0.9655\n",
      "Epoch 78/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0971 - accuracy: 0.9679 - val_loss: 0.1104 - val_accuracy: 0.9650\n",
      "Epoch 79/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0969 - accuracy: 0.9685 - val_loss: 0.1096 - val_accuracy: 0.9651\n",
      "Epoch 80/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0965 - accuracy: 0.9685 - val_loss: 0.1102 - val_accuracy: 0.9645\n",
      "Epoch 81/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0955 - accuracy: 0.9687 - val_loss: 0.1089 - val_accuracy: 0.9660\n",
      "Epoch 82/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0929 - accuracy: 0.9697 - val_loss: 0.1100 - val_accuracy: 0.9651\n",
      "Epoch 83/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0935 - accuracy: 0.9686 - val_loss: 0.1095 - val_accuracy: 0.9653\n",
      "Epoch 84/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0939 - accuracy: 0.9688 - val_loss: 0.1078 - val_accuracy: 0.9656\n",
      "Epoch 85/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0938 - accuracy: 0.9687 - val_loss: 0.1105 - val_accuracy: 0.9656\n",
      "Epoch 86/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0907 - accuracy: 0.9705 - val_loss: 0.1079 - val_accuracy: 0.9654\n",
      "Epoch 87/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0906 - accuracy: 0.9699 - val_loss: 0.1069 - val_accuracy: 0.9657\n",
      "Epoch 88/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0897 - accuracy: 0.9703 - val_loss: 0.1082 - val_accuracy: 0.9656\n",
      "Epoch 89/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0870 - accuracy: 0.9713 - val_loss: 0.1068 - val_accuracy: 0.9666\n",
      "Epoch 90/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0884 - accuracy: 0.9705 - val_loss: 0.1078 - val_accuracy: 0.9658\n",
      "Epoch 91/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0852 - accuracy: 0.9718 - val_loss: 0.1101 - val_accuracy: 0.9661\n",
      "Epoch 92/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0852 - accuracy: 0.9727 - val_loss: 0.1075 - val_accuracy: 0.9662\n",
      "Epoch 93/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0851 - accuracy: 0.9714 - val_loss: 0.1052 - val_accuracy: 0.9672\n",
      "Epoch 94/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0847 - accuracy: 0.9719 - val_loss: 0.1056 - val_accuracy: 0.9660\n",
      "Epoch 95/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0831 - accuracy: 0.9722 - val_loss: 0.1080 - val_accuracy: 0.9660\n",
      "Epoch 96/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0828 - accuracy: 0.9718 - val_loss: 0.1087 - val_accuracy: 0.9671\n",
      "Epoch 97/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0816 - accuracy: 0.9730 - val_loss: 0.1057 - val_accuracy: 0.9660\n",
      "Epoch 98/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0816 - accuracy: 0.9727 - val_loss: 0.1058 - val_accuracy: 0.9663\n",
      "Epoch 99/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0799 - accuracy: 0.9732 - val_loss: 0.1066 - val_accuracy: 0.9665\n",
      "Epoch 100/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0802 - accuracy: 0.9729 - val_loss: 0.1070 - val_accuracy: 0.9665\n",
      "Epoch 101/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0802 - accuracy: 0.9728 - val_loss: 0.1048 - val_accuracy: 0.9677\n",
      "Epoch 102/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0795 - accuracy: 0.9732 - val_loss: 0.1064 - val_accuracy: 0.9672\n",
      "Epoch 103/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0803 - accuracy: 0.9730 - val_loss: 0.1053 - val_accuracy: 0.9678\n",
      "Epoch 104/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0778 - accuracy: 0.9745 - val_loss: 0.1049 - val_accuracy: 0.9679\n",
      "Epoch 105/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0767 - accuracy: 0.9740 - val_loss: 0.1062 - val_accuracy: 0.9678\n",
      "Epoch 106/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0770 - accuracy: 0.9740 - val_loss: 0.1059 - val_accuracy: 0.9679\n",
      "Epoch 107/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0755 - accuracy: 0.9740 - val_loss: 0.1050 - val_accuracy: 0.9679\n",
      "Epoch 108/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0737 - accuracy: 0.9751 - val_loss: 0.1082 - val_accuracy: 0.9682\n",
      "Epoch 109/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0747 - accuracy: 0.9744 - val_loss: 0.1054 - val_accuracy: 0.9679\n",
      "Epoch 110/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0735 - accuracy: 0.9749 - val_loss: 0.1089 - val_accuracy: 0.9672\n",
      "Epoch 111/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0733 - accuracy: 0.9750 - val_loss: 0.1048 - val_accuracy: 0.9691\n",
      "Epoch 112/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0714 - accuracy: 0.9757 - val_loss: 0.1030 - val_accuracy: 0.9686\n",
      "Epoch 113/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0722 - accuracy: 0.9756 - val_loss: 0.1104 - val_accuracy: 0.9671\n",
      "Epoch 114/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0715 - accuracy: 0.9759 - val_loss: 0.1033 - val_accuracy: 0.9684\n",
      "Epoch 115/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0714 - accuracy: 0.9759 - val_loss: 0.1062 - val_accuracy: 0.9674\n",
      "Epoch 116/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0697 - accuracy: 0.9762 - val_loss: 0.1067 - val_accuracy: 0.9676\n",
      "Epoch 117/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0691 - accuracy: 0.9765 - val_loss: 0.1059 - val_accuracy: 0.9685\n",
      "Epoch 118/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0699 - accuracy: 0.9764 - val_loss: 0.1044 - val_accuracy: 0.9689\n",
      "Epoch 119/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0683 - accuracy: 0.9760 - val_loss: 0.1061 - val_accuracy: 0.9683\n",
      "Epoch 120/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0682 - accuracy: 0.9764 - val_loss: 0.1046 - val_accuracy: 0.9679\n",
      "Epoch 121/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0666 - accuracy: 0.9773 - val_loss: 0.1046 - val_accuracy: 0.9682\n",
      "Epoch 122/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0665 - accuracy: 0.9775 - val_loss: 0.1044 - val_accuracy: 0.9678\n",
      "Epoch 123/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0654 - accuracy: 0.9779 - val_loss: 0.1036 - val_accuracy: 0.9681\n",
      "Epoch 124/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0655 - accuracy: 0.9773 - val_loss: 0.1035 - val_accuracy: 0.9689\n",
      "Epoch 125/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0647 - accuracy: 0.9776 - val_loss: 0.1051 - val_accuracy: 0.9673\n",
      "Epoch 126/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0650 - accuracy: 0.9774 - val_loss: 0.1032 - val_accuracy: 0.9688\n",
      "Epoch 127/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0652 - accuracy: 0.9775 - val_loss: 0.1033 - val_accuracy: 0.9690\n",
      "Epoch 128/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0645 - accuracy: 0.9776 - val_loss: 0.1038 - val_accuracy: 0.9694\n",
      "Epoch 129/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0622 - accuracy: 0.9785 - val_loss: 0.1062 - val_accuracy: 0.9694\n",
      "Epoch 130/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0632 - accuracy: 0.9778 - val_loss: 0.1035 - val_accuracy: 0.9688\n",
      "Epoch 131/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0623 - accuracy: 0.9786 - val_loss: 0.1059 - val_accuracy: 0.9682\n",
      "Epoch 132/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0611 - accuracy: 0.9792 - val_loss: 0.1065 - val_accuracy: 0.9686\n",
      "Epoch 133/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0622 - accuracy: 0.9783 - val_loss: 0.1087 - val_accuracy: 0.9674\n",
      "Epoch 134/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0617 - accuracy: 0.9783 - val_loss: 0.1034 - val_accuracy: 0.9698\n",
      "Epoch 135/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0613 - accuracy: 0.9784 - val_loss: 0.1096 - val_accuracy: 0.9679\n",
      "Epoch 136/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0616 - accuracy: 0.9784 - val_loss: 0.1086 - val_accuracy: 0.9685\n",
      "Epoch 137/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0606 - accuracy: 0.9792 - val_loss: 0.1051 - val_accuracy: 0.9692\n",
      "Epoch 138/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0596 - accuracy: 0.9793 - val_loss: 0.1055 - val_accuracy: 0.9685\n",
      "Epoch 139/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0580 - accuracy: 0.9799 - val_loss: 0.1054 - val_accuracy: 0.9685\n",
      "Epoch 140/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0585 - accuracy: 0.9796 - val_loss: 0.1035 - val_accuracy: 0.9691\n",
      "Epoch 141/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0580 - accuracy: 0.9797 - val_loss: 0.1047 - val_accuracy: 0.9688\n",
      "Epoch 142/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0581 - accuracy: 0.9798 - val_loss: 0.1066 - val_accuracy: 0.9686\n",
      "Epoch 143/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0578 - accuracy: 0.9800 - val_loss: 0.1069 - val_accuracy: 0.9682\n",
      "Epoch 144/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0572 - accuracy: 0.9799 - val_loss: 0.1059 - val_accuracy: 0.9693\n",
      "Epoch 145/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0558 - accuracy: 0.9807 - val_loss: 0.1060 - val_accuracy: 0.9692\n",
      "Epoch 146/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0567 - accuracy: 0.9798 - val_loss: 0.1078 - val_accuracy: 0.9695\n",
      "Epoch 147/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0569 - accuracy: 0.9799 - val_loss: 0.1082 - val_accuracy: 0.9684\n",
      "Epoch 148/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0550 - accuracy: 0.9806 - val_loss: 0.1048 - val_accuracy: 0.9689\n",
      "Epoch 149/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0546 - accuracy: 0.9816 - val_loss: 0.1103 - val_accuracy: 0.9686\n",
      "Epoch 150/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0547 - accuracy: 0.9807 - val_loss: 0.1069 - val_accuracy: 0.9681\n",
      "Epoch 151/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0543 - accuracy: 0.9814 - val_loss: 0.1081 - val_accuracy: 0.9689\n",
      "Epoch 152/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0534 - accuracy: 0.9812 - val_loss: 0.1067 - val_accuracy: 0.9693\n",
      "Epoch 153/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0536 - accuracy: 0.9816 - val_loss: 0.1047 - val_accuracy: 0.9691\n",
      "Epoch 154/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0528 - accuracy: 0.9817 - val_loss: 0.1088 - val_accuracy: 0.9690\n",
      "Epoch 155/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0531 - accuracy: 0.9813 - val_loss: 0.1080 - val_accuracy: 0.9694\n",
      "Epoch 156/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0517 - accuracy: 0.9819 - val_loss: 0.1066 - val_accuracy: 0.9698\n",
      "Epoch 157/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0527 - accuracy: 0.9813 - val_loss: 0.1111 - val_accuracy: 0.9693\n",
      "Epoch 158/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0518 - accuracy: 0.9814 - val_loss: 0.1065 - val_accuracy: 0.9695\n",
      "Epoch 159/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0519 - accuracy: 0.9817 - val_loss: 0.1085 - val_accuracy: 0.9698\n",
      "Epoch 160/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0509 - accuracy: 0.9823 - val_loss: 0.1076 - val_accuracy: 0.9693\n",
      "Epoch 161/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0512 - accuracy: 0.9820 - val_loss: 0.1075 - val_accuracy: 0.9697\n",
      "Epoch 162/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0497 - accuracy: 0.9825 - val_loss: 0.1068 - val_accuracy: 0.9704\n",
      "Epoch 163/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0496 - accuracy: 0.9829 - val_loss: 0.1089 - val_accuracy: 0.9712\n",
      "Epoch 164/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0505 - accuracy: 0.9820 - val_loss: 0.1068 - val_accuracy: 0.9704\n",
      "Epoch 165/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0490 - accuracy: 0.9830 - val_loss: 0.1104 - val_accuracy: 0.9701\n",
      "Epoch 166/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0488 - accuracy: 0.9826 - val_loss: 0.1117 - val_accuracy: 0.9697\n",
      "Epoch 167/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0495 - accuracy: 0.9827 - val_loss: 0.1079 - val_accuracy: 0.9703\n",
      "Epoch 168/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0489 - accuracy: 0.9832 - val_loss: 0.1115 - val_accuracy: 0.9696\n",
      "Epoch 169/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0485 - accuracy: 0.9831 - val_loss: 0.1092 - val_accuracy: 0.9699\n",
      "Epoch 170/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0479 - accuracy: 0.9828 - val_loss: 0.1080 - val_accuracy: 0.9707\n",
      "Epoch 171/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0474 - accuracy: 0.9833 - val_loss: 0.1104 - val_accuracy: 0.9699\n",
      "Epoch 172/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0468 - accuracy: 0.9835 - val_loss: 0.1085 - val_accuracy: 0.9703\n",
      "Epoch 173/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0494 - accuracy: 0.9825 - val_loss: 0.1094 - val_accuracy: 0.9700\n",
      "Epoch 174/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0466 - accuracy: 0.9837 - val_loss: 0.1086 - val_accuracy: 0.9699\n",
      "Epoch 175/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0460 - accuracy: 0.9838 - val_loss: 0.1100 - val_accuracy: 0.9695\n",
      "Epoch 176/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0465 - accuracy: 0.9835 - val_loss: 0.1126 - val_accuracy: 0.9698\n",
      "Epoch 177/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0456 - accuracy: 0.9837 - val_loss: 0.1103 - val_accuracy: 0.9704\n",
      "Epoch 178/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0474 - accuracy: 0.9834 - val_loss: 0.1096 - val_accuracy: 0.9703\n",
      "Epoch 179/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0451 - accuracy: 0.9843 - val_loss: 0.1084 - val_accuracy: 0.9706\n",
      "Epoch 180/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0459 - accuracy: 0.9842 - val_loss: 0.1093 - val_accuracy: 0.9711\n",
      "Epoch 181/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0452 - accuracy: 0.9841 - val_loss: 0.1119 - val_accuracy: 0.9698\n",
      "Epoch 182/200\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.0449 - accuracy: 0.9842 - val_loss: 0.1113 - val_accuracy: 0.9692\n",
      "Epoch 183/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0441 - accuracy: 0.9843 - val_loss: 0.1122 - val_accuracy: 0.9708\n",
      "Epoch 184/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0455 - accuracy: 0.9838 - val_loss: 0.1110 - val_accuracy: 0.9705\n",
      "Epoch 185/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0450 - accuracy: 0.9841 - val_loss: 0.1144 - val_accuracy: 0.9698\n",
      "Epoch 186/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0446 - accuracy: 0.9844 - val_loss: 0.1136 - val_accuracy: 0.9690\n",
      "Epoch 187/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0433 - accuracy: 0.9847 - val_loss: 0.1099 - val_accuracy: 0.9706\n",
      "Epoch 188/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0426 - accuracy: 0.9849 - val_loss: 0.1112 - val_accuracy: 0.9701\n",
      "Epoch 189/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0428 - accuracy: 0.9854 - val_loss: 0.1138 - val_accuracy: 0.9695\n",
      "Epoch 190/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0439 - accuracy: 0.9847 - val_loss: 0.1109 - val_accuracy: 0.9696\n",
      "Epoch 191/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0432 - accuracy: 0.9849 - val_loss: 0.1135 - val_accuracy: 0.9693\n",
      "Epoch 192/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0429 - accuracy: 0.9850 - val_loss: 0.1103 - val_accuracy: 0.9700\n",
      "Epoch 193/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0422 - accuracy: 0.9850 - val_loss: 0.1098 - val_accuracy: 0.9704\n",
      "Epoch 194/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0416 - accuracy: 0.9852 - val_loss: 0.1129 - val_accuracy: 0.9702\n",
      "Epoch 195/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0425 - accuracy: 0.9847 - val_loss: 0.1144 - val_accuracy: 0.9695\n",
      "Epoch 196/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0419 - accuracy: 0.9851 - val_loss: 0.1094 - val_accuracy: 0.9703\n",
      "Epoch 197/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0422 - accuracy: 0.9854 - val_loss: 0.1122 - val_accuracy: 0.9689\n",
      "Epoch 198/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0412 - accuracy: 0.9855 - val_loss: 0.1128 - val_accuracy: 0.9695\n",
      "Epoch 199/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0418 - accuracy: 0.9854 - val_loss: 0.1121 - val_accuracy: 0.9700\n",
      "Epoch 200/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0400 - accuracy: 0.9862 - val_loss: 0.1159 - val_accuracy: 0.9700\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "\n",
      "Fold 5:\n",
      "X_train shape: (86692, 1, 1280)\n",
      "X_val shape: (9632, 1, 1280)\n",
      "Epoch 1/200\n",
      "43/43 [==============================] - 4s 23ms/step - loss: 1.2850 - accuracy: 0.5720 - val_loss: 0.9662 - val_accuracy: 0.6661\n",
      "Epoch 2/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.8108 - accuracy: 0.7363 - val_loss: 0.6133 - val_accuracy: 0.8142\n",
      "Epoch 3/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.5718 - accuracy: 0.8302 - val_loss: 0.4477 - val_accuracy: 0.8722\n",
      "Epoch 4/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.4738 - accuracy: 0.8601 - val_loss: 0.3780 - val_accuracy: 0.8923\n",
      "Epoch 5/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.4285 - accuracy: 0.8727 - val_loss: 0.3319 - val_accuracy: 0.9073\n",
      "Epoch 6/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.4052 - accuracy: 0.8788 - val_loss: 0.3062 - val_accuracy: 0.9123\n",
      "Epoch 7/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3846 - accuracy: 0.8833 - val_loss: 0.2894 - val_accuracy: 0.9164\n",
      "Epoch 8/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3674 - accuracy: 0.8888 - val_loss: 0.2796 - val_accuracy: 0.9183\n",
      "Epoch 9/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3586 - accuracy: 0.8897 - val_loss: 0.2604 - val_accuracy: 0.9263\n",
      "Epoch 10/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3444 - accuracy: 0.8951 - val_loss: 0.2552 - val_accuracy: 0.9254\n",
      "Epoch 11/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3377 - accuracy: 0.8956 - val_loss: 0.2418 - val_accuracy: 0.9318\n",
      "Epoch 12/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3279 - accuracy: 0.8987 - val_loss: 0.2346 - val_accuracy: 0.9321\n",
      "Epoch 13/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3177 - accuracy: 0.9012 - val_loss: 0.2251 - val_accuracy: 0.9363\n",
      "Epoch 14/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3059 - accuracy: 0.9050 - val_loss: 0.2198 - val_accuracy: 0.9364\n",
      "Epoch 15/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2996 - accuracy: 0.9069 - val_loss: 0.2126 - val_accuracy: 0.9397\n",
      "Epoch 16/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2919 - accuracy: 0.9097 - val_loss: 0.2069 - val_accuracy: 0.9430\n",
      "Epoch 17/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2818 - accuracy: 0.9135 - val_loss: 0.2015 - val_accuracy: 0.9425\n",
      "Epoch 18/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2723 - accuracy: 0.9165 - val_loss: 0.1967 - val_accuracy: 0.9429\n",
      "Epoch 19/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2695 - accuracy: 0.9163 - val_loss: 0.1936 - val_accuracy: 0.9420\n",
      "Epoch 20/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2595 - accuracy: 0.9201 - val_loss: 0.1870 - val_accuracy: 0.9462\n",
      "Epoch 21/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2515 - accuracy: 0.9216 - val_loss: 0.1857 - val_accuracy: 0.9435\n",
      "Epoch 22/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2464 - accuracy: 0.9242 - val_loss: 0.1798 - val_accuracy: 0.9457\n",
      "Epoch 23/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2351 - accuracy: 0.9280 - val_loss: 0.1752 - val_accuracy: 0.9485\n",
      "Epoch 24/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2335 - accuracy: 0.9277 - val_loss: 0.1731 - val_accuracy: 0.9471\n",
      "Epoch 25/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2254 - accuracy: 0.9313 - val_loss: 0.1692 - val_accuracy: 0.9485\n",
      "Epoch 26/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2178 - accuracy: 0.9331 - val_loss: 0.1641 - val_accuracy: 0.9518\n",
      "Epoch 27/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2136 - accuracy: 0.9344 - val_loss: 0.1636 - val_accuracy: 0.9511\n",
      "Epoch 28/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2080 - accuracy: 0.9363 - val_loss: 0.1628 - val_accuracy: 0.9506\n",
      "Epoch 29/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2020 - accuracy: 0.9387 - val_loss: 0.1572 - val_accuracy: 0.9528\n",
      "Epoch 30/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1971 - accuracy: 0.9392 - val_loss: 0.1556 - val_accuracy: 0.9512\n",
      "Epoch 31/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1925 - accuracy: 0.9409 - val_loss: 0.1549 - val_accuracy: 0.9527\n",
      "Epoch 32/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1886 - accuracy: 0.9418 - val_loss: 0.1496 - val_accuracy: 0.9544\n",
      "Epoch 33/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1838 - accuracy: 0.9439 - val_loss: 0.1492 - val_accuracy: 0.9538\n",
      "Epoch 34/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1815 - accuracy: 0.9448 - val_loss: 0.1467 - val_accuracy: 0.9548\n",
      "Epoch 35/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1764 - accuracy: 0.9452 - val_loss: 0.1462 - val_accuracy: 0.9530\n",
      "Epoch 36/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1734 - accuracy: 0.9471 - val_loss: 0.1426 - val_accuracy: 0.9549\n",
      "Epoch 37/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1695 - accuracy: 0.9485 - val_loss: 0.1410 - val_accuracy: 0.9559\n",
      "Epoch 38/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1652 - accuracy: 0.9492 - val_loss: 0.1416 - val_accuracy: 0.9560\n",
      "Epoch 39/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1627 - accuracy: 0.9500 - val_loss: 0.1376 - val_accuracy: 0.9571\n",
      "Epoch 40/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1608 - accuracy: 0.9503 - val_loss: 0.1383 - val_accuracy: 0.9559\n",
      "Epoch 41/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1571 - accuracy: 0.9514 - val_loss: 0.1355 - val_accuracy: 0.9587\n",
      "Epoch 42/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1547 - accuracy: 0.9519 - val_loss: 0.1352 - val_accuracy: 0.9578\n",
      "Epoch 43/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1506 - accuracy: 0.9540 - val_loss: 0.1353 - val_accuracy: 0.9563\n",
      "Epoch 44/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1488 - accuracy: 0.9540 - val_loss: 0.1345 - val_accuracy: 0.9575\n",
      "Epoch 45/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1479 - accuracy: 0.9539 - val_loss: 0.1305 - val_accuracy: 0.9580\n",
      "Epoch 46/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1436 - accuracy: 0.9556 - val_loss: 0.1286 - val_accuracy: 0.9589\n",
      "Epoch 47/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1435 - accuracy: 0.9553 - val_loss: 0.1296 - val_accuracy: 0.9585\n",
      "Epoch 48/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1413 - accuracy: 0.9566 - val_loss: 0.1269 - val_accuracy: 0.9597\n",
      "Epoch 49/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1387 - accuracy: 0.9559 - val_loss: 0.1287 - val_accuracy: 0.9583\n",
      "Epoch 50/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1348 - accuracy: 0.9571 - val_loss: 0.1245 - val_accuracy: 0.9602\n",
      "Epoch 51/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1352 - accuracy: 0.9575 - val_loss: 0.1237 - val_accuracy: 0.9602\n",
      "Epoch 52/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1304 - accuracy: 0.9589 - val_loss: 0.1227 - val_accuracy: 0.9612\n",
      "Epoch 53/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1315 - accuracy: 0.9583 - val_loss: 0.1228 - val_accuracy: 0.9613\n",
      "Epoch 54/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1294 - accuracy: 0.9588 - val_loss: 0.1236 - val_accuracy: 0.9607\n",
      "Epoch 55/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1271 - accuracy: 0.9598 - val_loss: 0.1207 - val_accuracy: 0.9614\n",
      "Epoch 56/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1265 - accuracy: 0.9601 - val_loss: 0.1204 - val_accuracy: 0.9617\n",
      "Epoch 57/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1231 - accuracy: 0.9610 - val_loss: 0.1189 - val_accuracy: 0.9630\n",
      "Epoch 58/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1214 - accuracy: 0.9615 - val_loss: 0.1195 - val_accuracy: 0.9617\n",
      "Epoch 59/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1210 - accuracy: 0.9610 - val_loss: 0.1170 - val_accuracy: 0.9635\n",
      "Epoch 60/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1204 - accuracy: 0.9614 - val_loss: 0.1182 - val_accuracy: 0.9624\n",
      "Epoch 61/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1174 - accuracy: 0.9630 - val_loss: 0.1160 - val_accuracy: 0.9629\n",
      "Epoch 62/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1169 - accuracy: 0.9626 - val_loss: 0.1153 - val_accuracy: 0.9643\n",
      "Epoch 63/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1156 - accuracy: 0.9630 - val_loss: 0.1155 - val_accuracy: 0.9641\n",
      "Epoch 64/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1141 - accuracy: 0.9625 - val_loss: 0.1154 - val_accuracy: 0.9636\n",
      "Epoch 65/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1129 - accuracy: 0.9638 - val_loss: 0.1146 - val_accuracy: 0.9637\n",
      "Epoch 66/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1092 - accuracy: 0.9646 - val_loss: 0.1135 - val_accuracy: 0.9636\n",
      "Epoch 67/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1097 - accuracy: 0.9649 - val_loss: 0.1166 - val_accuracy: 0.9641\n",
      "Epoch 68/200\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.1073 - accuracy: 0.9655 - val_loss: 0.1124 - val_accuracy: 0.9644\n",
      "Epoch 69/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1065 - accuracy: 0.9656 - val_loss: 0.1118 - val_accuracy: 0.9645\n",
      "Epoch 70/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1070 - accuracy: 0.9645 - val_loss: 0.1154 - val_accuracy: 0.9640\n",
      "Epoch 71/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1054 - accuracy: 0.9665 - val_loss: 0.1119 - val_accuracy: 0.9642\n",
      "Epoch 72/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1039 - accuracy: 0.9662 - val_loss: 0.1120 - val_accuracy: 0.9651\n",
      "Epoch 73/200\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.1040 - accuracy: 0.9662 - val_loss: 0.1149 - val_accuracy: 0.9643\n",
      "Epoch 74/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1025 - accuracy: 0.9675 - val_loss: 0.1117 - val_accuracy: 0.9657\n",
      "Epoch 75/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0995 - accuracy: 0.9677 - val_loss: 0.1109 - val_accuracy: 0.9654\n",
      "Epoch 76/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0993 - accuracy: 0.9680 - val_loss: 0.1108 - val_accuracy: 0.9650\n",
      "Epoch 77/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0988 - accuracy: 0.9676 - val_loss: 0.1117 - val_accuracy: 0.9652\n",
      "Epoch 78/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0972 - accuracy: 0.9687 - val_loss: 0.1097 - val_accuracy: 0.9647\n",
      "Epoch 79/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0966 - accuracy: 0.9687 - val_loss: 0.1087 - val_accuracy: 0.9665\n",
      "Epoch 80/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0951 - accuracy: 0.9687 - val_loss: 0.1083 - val_accuracy: 0.9661\n",
      "Epoch 81/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0945 - accuracy: 0.9682 - val_loss: 0.1093 - val_accuracy: 0.9662\n",
      "Epoch 82/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0920 - accuracy: 0.9696 - val_loss: 0.1093 - val_accuracy: 0.9658\n",
      "Epoch 83/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0927 - accuracy: 0.9694 - val_loss: 0.1103 - val_accuracy: 0.9656\n",
      "Epoch 84/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0918 - accuracy: 0.9696 - val_loss: 0.1084 - val_accuracy: 0.9664\n",
      "Epoch 85/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0916 - accuracy: 0.9690 - val_loss: 0.1073 - val_accuracy: 0.9663\n",
      "Epoch 86/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0895 - accuracy: 0.9705 - val_loss: 0.1084 - val_accuracy: 0.9662\n",
      "Epoch 87/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0890 - accuracy: 0.9707 - val_loss: 0.1105 - val_accuracy: 0.9659\n",
      "Epoch 88/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0874 - accuracy: 0.9711 - val_loss: 0.1085 - val_accuracy: 0.9666\n",
      "Epoch 89/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0883 - accuracy: 0.9710 - val_loss: 0.1065 - val_accuracy: 0.9673\n",
      "Epoch 90/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0861 - accuracy: 0.9712 - val_loss: 0.1086 - val_accuracy: 0.9670\n",
      "Epoch 91/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0860 - accuracy: 0.9714 - val_loss: 0.1078 - val_accuracy: 0.9669\n",
      "Epoch 92/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0847 - accuracy: 0.9715 - val_loss: 0.1096 - val_accuracy: 0.9661\n",
      "Epoch 93/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0840 - accuracy: 0.9717 - val_loss: 0.1067 - val_accuracy: 0.9665\n",
      "Epoch 94/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0841 - accuracy: 0.9721 - val_loss: 0.1078 - val_accuracy: 0.9671\n",
      "Epoch 95/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0821 - accuracy: 0.9727 - val_loss: 0.1065 - val_accuracy: 0.9675\n",
      "Epoch 96/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0828 - accuracy: 0.9721 - val_loss: 0.1082 - val_accuracy: 0.9667\n",
      "Epoch 97/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0815 - accuracy: 0.9726 - val_loss: 0.1063 - val_accuracy: 0.9673\n",
      "Epoch 98/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0802 - accuracy: 0.9727 - val_loss: 0.1049 - val_accuracy: 0.9689\n",
      "Epoch 99/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0801 - accuracy: 0.9734 - val_loss: 0.1076 - val_accuracy: 0.9676\n",
      "Epoch 100/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0797 - accuracy: 0.9734 - val_loss: 0.1071 - val_accuracy: 0.9675\n",
      "Epoch 101/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0778 - accuracy: 0.9738 - val_loss: 0.1046 - val_accuracy: 0.9691\n",
      "Epoch 102/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0790 - accuracy: 0.9735 - val_loss: 0.1045 - val_accuracy: 0.9682\n",
      "Epoch 103/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0769 - accuracy: 0.9740 - val_loss: 0.1047 - val_accuracy: 0.9683\n",
      "Epoch 104/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0764 - accuracy: 0.9744 - val_loss: 0.1054 - val_accuracy: 0.9684\n",
      "Epoch 105/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0772 - accuracy: 0.9744 - val_loss: 0.1050 - val_accuracy: 0.9690\n",
      "Epoch 106/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0760 - accuracy: 0.9742 - val_loss: 0.1054 - val_accuracy: 0.9686\n",
      "Epoch 107/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0736 - accuracy: 0.9751 - val_loss: 0.1059 - val_accuracy: 0.9690\n",
      "Epoch 108/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0755 - accuracy: 0.9743 - val_loss: 0.1062 - val_accuracy: 0.9681\n",
      "Epoch 109/200\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.0731 - accuracy: 0.9753 - val_loss: 0.1053 - val_accuracy: 0.9685\n",
      "Epoch 110/200\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.0732 - accuracy: 0.9753 - val_loss: 0.1052 - val_accuracy: 0.9689\n",
      "Epoch 111/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0700 - accuracy: 0.9768 - val_loss: 0.1070 - val_accuracy: 0.9679\n",
      "Epoch 112/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0721 - accuracy: 0.9753 - val_loss: 0.1032 - val_accuracy: 0.9692\n",
      "Epoch 113/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0721 - accuracy: 0.9754 - val_loss: 0.1038 - val_accuracy: 0.9697\n",
      "Epoch 114/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0708 - accuracy: 0.9756 - val_loss: 0.1048 - val_accuracy: 0.9696\n",
      "Epoch 115/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0705 - accuracy: 0.9759 - val_loss: 0.1065 - val_accuracy: 0.9689\n",
      "Epoch 116/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0690 - accuracy: 0.9765 - val_loss: 0.1046 - val_accuracy: 0.9696\n",
      "Epoch 117/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0684 - accuracy: 0.9770 - val_loss: 0.1040 - val_accuracy: 0.9695\n",
      "Epoch 118/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0694 - accuracy: 0.9765 - val_loss: 0.1041 - val_accuracy: 0.9700\n",
      "Epoch 119/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0687 - accuracy: 0.9765 - val_loss: 0.1057 - val_accuracy: 0.9697\n",
      "Epoch 120/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0671 - accuracy: 0.9770 - val_loss: 0.1065 - val_accuracy: 0.9689\n",
      "Epoch 121/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0664 - accuracy: 0.9770 - val_loss: 0.1071 - val_accuracy: 0.9688\n",
      "Epoch 122/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0675 - accuracy: 0.9769 - val_loss: 0.1022 - val_accuracy: 0.9700\n",
      "Epoch 123/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0661 - accuracy: 0.9776 - val_loss: 0.1064 - val_accuracy: 0.9692\n",
      "Epoch 124/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0660 - accuracy: 0.9779 - val_loss: 0.1045 - val_accuracy: 0.9702\n",
      "Epoch 125/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0641 - accuracy: 0.9780 - val_loss: 0.1056 - val_accuracy: 0.9707\n",
      "Epoch 126/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0631 - accuracy: 0.9782 - val_loss: 0.1041 - val_accuracy: 0.9700\n",
      "Epoch 127/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0635 - accuracy: 0.9781 - val_loss: 0.1061 - val_accuracy: 0.9697\n",
      "Epoch 128/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0638 - accuracy: 0.9781 - val_loss: 0.1054 - val_accuracy: 0.9701\n",
      "Epoch 129/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0632 - accuracy: 0.9778 - val_loss: 0.1057 - val_accuracy: 0.9709\n",
      "Epoch 130/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0615 - accuracy: 0.9791 - val_loss: 0.1046 - val_accuracy: 0.9714\n",
      "Epoch 131/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0630 - accuracy: 0.9780 - val_loss: 0.1046 - val_accuracy: 0.9707\n",
      "Epoch 132/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0621 - accuracy: 0.9786 - val_loss: 0.1050 - val_accuracy: 0.9714\n",
      "Epoch 133/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0616 - accuracy: 0.9789 - val_loss: 0.1047 - val_accuracy: 0.9704\n",
      "Epoch 134/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0600 - accuracy: 0.9792 - val_loss: 0.1030 - val_accuracy: 0.9710\n",
      "Epoch 135/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0602 - accuracy: 0.9791 - val_loss: 0.1059 - val_accuracy: 0.9705\n",
      "Epoch 136/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0604 - accuracy: 0.9790 - val_loss: 0.1055 - val_accuracy: 0.9712\n",
      "Epoch 137/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0591 - accuracy: 0.9801 - val_loss: 0.1059 - val_accuracy: 0.9714\n",
      "Epoch 138/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0592 - accuracy: 0.9794 - val_loss: 0.1054 - val_accuracy: 0.9705\n",
      "Epoch 139/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0586 - accuracy: 0.9798 - val_loss: 0.1054 - val_accuracy: 0.9695\n",
      "Epoch 140/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0576 - accuracy: 0.9802 - val_loss: 0.1060 - val_accuracy: 0.9706\n",
      "Epoch 141/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0560 - accuracy: 0.9805 - val_loss: 0.1061 - val_accuracy: 0.9701\n",
      "Epoch 142/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0562 - accuracy: 0.9805 - val_loss: 0.1057 - val_accuracy: 0.9714\n",
      "Epoch 143/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0567 - accuracy: 0.9805 - val_loss: 0.1052 - val_accuracy: 0.9707\n",
      "Epoch 144/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0565 - accuracy: 0.9804 - val_loss: 0.1078 - val_accuracy: 0.9701\n",
      "Epoch 145/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0564 - accuracy: 0.9806 - val_loss: 0.1082 - val_accuracy: 0.9699\n",
      "Epoch 146/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0554 - accuracy: 0.9807 - val_loss: 0.1071 - val_accuracy: 0.9699\n",
      "Epoch 147/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0547 - accuracy: 0.9809 - val_loss: 0.1037 - val_accuracy: 0.9709\n",
      "Epoch 148/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0545 - accuracy: 0.9812 - val_loss: 0.1045 - val_accuracy: 0.9712\n",
      "Epoch 149/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0534 - accuracy: 0.9815 - val_loss: 0.1056 - val_accuracy: 0.9713\n",
      "Epoch 150/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0533 - accuracy: 0.9822 - val_loss: 0.1062 - val_accuracy: 0.9709\n",
      "Epoch 151/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0535 - accuracy: 0.9813 - val_loss: 0.1092 - val_accuracy: 0.9707\n",
      "Epoch 152/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0521 - accuracy: 0.9815 - val_loss: 0.1070 - val_accuracy: 0.9713\n",
      "Epoch 153/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0519 - accuracy: 0.9821 - val_loss: 0.1058 - val_accuracy: 0.9712\n",
      "Epoch 154/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0530 - accuracy: 0.9814 - val_loss: 0.1084 - val_accuracy: 0.9710\n",
      "Epoch 155/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0534 - accuracy: 0.9811 - val_loss: 0.1101 - val_accuracy: 0.9712\n",
      "Epoch 156/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0516 - accuracy: 0.9818 - val_loss: 0.1076 - val_accuracy: 0.9710\n",
      "Epoch 157/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0522 - accuracy: 0.9819 - val_loss: 0.1073 - val_accuracy: 0.9719\n",
      "Epoch 158/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0516 - accuracy: 0.9817 - val_loss: 0.1099 - val_accuracy: 0.9703\n",
      "Epoch 159/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0508 - accuracy: 0.9825 - val_loss: 0.1083 - val_accuracy: 0.9711\n",
      "Epoch 160/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0512 - accuracy: 0.9823 - val_loss: 0.1094 - val_accuracy: 0.9710\n",
      "Epoch 161/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0497 - accuracy: 0.9827 - val_loss: 0.1104 - val_accuracy: 0.9705\n",
      "Epoch 162/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0498 - accuracy: 0.9825 - val_loss: 0.1094 - val_accuracy: 0.9712\n",
      "Epoch 163/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0489 - accuracy: 0.9828 - val_loss: 0.1060 - val_accuracy: 0.9712\n",
      "Epoch 164/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0495 - accuracy: 0.9827 - val_loss: 0.1079 - val_accuracy: 0.9718\n",
      "Epoch 165/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0488 - accuracy: 0.9823 - val_loss: 0.1073 - val_accuracy: 0.9711\n",
      "Epoch 166/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0487 - accuracy: 0.9832 - val_loss: 0.1075 - val_accuracy: 0.9712\n",
      "Epoch 167/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0485 - accuracy: 0.9832 - val_loss: 0.1077 - val_accuracy: 0.9718\n",
      "Epoch 168/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0484 - accuracy: 0.9832 - val_loss: 0.1073 - val_accuracy: 0.9722\n",
      "Epoch 169/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0471 - accuracy: 0.9835 - val_loss: 0.1078 - val_accuracy: 0.9707\n",
      "Epoch 170/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0485 - accuracy: 0.9830 - val_loss: 0.1096 - val_accuracy: 0.9711\n",
      "Epoch 171/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0481 - accuracy: 0.9833 - val_loss: 0.1085 - val_accuracy: 0.9716\n",
      "Epoch 172/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0460 - accuracy: 0.9842 - val_loss: 0.1100 - val_accuracy: 0.9719\n",
      "Epoch 173/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0457 - accuracy: 0.9845 - val_loss: 0.1121 - val_accuracy: 0.9710\n",
      "Epoch 174/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0460 - accuracy: 0.9841 - val_loss: 0.1095 - val_accuracy: 0.9720\n",
      "Epoch 175/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0464 - accuracy: 0.9835 - val_loss: 0.1092 - val_accuracy: 0.9718\n",
      "Epoch 176/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0459 - accuracy: 0.9840 - val_loss: 0.1129 - val_accuracy: 0.9708\n",
      "Epoch 177/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0455 - accuracy: 0.9838 - val_loss: 0.1104 - val_accuracy: 0.9714\n",
      "Epoch 178/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0452 - accuracy: 0.9842 - val_loss: 0.1089 - val_accuracy: 0.9721\n",
      "Epoch 179/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0466 - accuracy: 0.9838 - val_loss: 0.1086 - val_accuracy: 0.9716\n",
      "Epoch 180/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0452 - accuracy: 0.9843 - val_loss: 0.1087 - val_accuracy: 0.9721\n",
      "Epoch 181/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0448 - accuracy: 0.9847 - val_loss: 0.1088 - val_accuracy: 0.9719\n",
      "Epoch 182/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0446 - accuracy: 0.9846 - val_loss: 0.1114 - val_accuracy: 0.9719\n",
      "Epoch 183/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0447 - accuracy: 0.9847 - val_loss: 0.1093 - val_accuracy: 0.9719\n",
      "Epoch 184/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0427 - accuracy: 0.9853 - val_loss: 0.1112 - val_accuracy: 0.9717\n",
      "Epoch 185/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0433 - accuracy: 0.9847 - val_loss: 0.1115 - val_accuracy: 0.9713\n",
      "Epoch 186/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0445 - accuracy: 0.9840 - val_loss: 0.1094 - val_accuracy: 0.9712\n",
      "Epoch 187/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0436 - accuracy: 0.9849 - val_loss: 0.1130 - val_accuracy: 0.9705\n",
      "Epoch 188/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0415 - accuracy: 0.9854 - val_loss: 0.1123 - val_accuracy: 0.9703\n",
      "Epoch 189/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0434 - accuracy: 0.9848 - val_loss: 0.1109 - val_accuracy: 0.9718\n",
      "Epoch 190/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0426 - accuracy: 0.9850 - val_loss: 0.1118 - val_accuracy: 0.9719\n",
      "Epoch 191/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0425 - accuracy: 0.9851 - val_loss: 0.1122 - val_accuracy: 0.9717\n",
      "Epoch 192/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0436 - accuracy: 0.9844 - val_loss: 0.1125 - val_accuracy: 0.9712\n",
      "Epoch 193/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0423 - accuracy: 0.9850 - val_loss: 0.1121 - val_accuracy: 0.9710\n",
      "Epoch 194/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0422 - accuracy: 0.9850 - val_loss: 0.1110 - val_accuracy: 0.9711\n",
      "Epoch 195/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0407 - accuracy: 0.9857 - val_loss: 0.1127 - val_accuracy: 0.9713\n",
      "Epoch 196/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0414 - accuracy: 0.9853 - val_loss: 0.1117 - val_accuracy: 0.9710\n",
      "Epoch 197/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0408 - accuracy: 0.9858 - val_loss: 0.1118 - val_accuracy: 0.9722\n",
      "Epoch 198/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0407 - accuracy: 0.9858 - val_loss: 0.1141 - val_accuracy: 0.9718\n",
      "Epoch 199/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0401 - accuracy: 0.9863 - val_loss: 0.1138 - val_accuracy: 0.9716\n",
      "Epoch 200/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0405 - accuracy: 0.9859 - val_loss: 0.1116 - val_accuracy: 0.9717\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "\n",
      "Fold 6:\n",
      "X_train shape: (86692, 1, 1280)\n",
      "X_val shape: (9632, 1, 1280)\n",
      "Epoch 1/200\n",
      "43/43 [==============================] - 4s 22ms/step - loss: 1.3063 - accuracy: 0.5612 - val_loss: 0.9887 - val_accuracy: 0.6483\n",
      "Epoch 2/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.8278 - accuracy: 0.7300 - val_loss: 0.6204 - val_accuracy: 0.8090\n",
      "Epoch 3/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.5836 - accuracy: 0.8248 - val_loss: 0.4469 - val_accuracy: 0.8720\n",
      "Epoch 4/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.4798 - accuracy: 0.8594 - val_loss: 0.3758 - val_accuracy: 0.8924\n",
      "Epoch 5/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.4309 - accuracy: 0.8714 - val_loss: 0.3334 - val_accuracy: 0.9033\n",
      "Epoch 6/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.4058 - accuracy: 0.8787 - val_loss: 0.3010 - val_accuracy: 0.9149\n",
      "Epoch 7/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3869 - accuracy: 0.8839 - val_loss: 0.2816 - val_accuracy: 0.9200\n",
      "Epoch 8/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3723 - accuracy: 0.8882 - val_loss: 0.2726 - val_accuracy: 0.9197\n",
      "Epoch 9/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3575 - accuracy: 0.8903 - val_loss: 0.2554 - val_accuracy: 0.9256\n",
      "Epoch 10/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3479 - accuracy: 0.8935 - val_loss: 0.2462 - val_accuracy: 0.9288\n",
      "Epoch 11/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3368 - accuracy: 0.8972 - val_loss: 0.2366 - val_accuracy: 0.9315\n",
      "Epoch 12/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3267 - accuracy: 0.8990 - val_loss: 0.2287 - val_accuracy: 0.9345\n",
      "Epoch 13/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3184 - accuracy: 0.9006 - val_loss: 0.2235 - val_accuracy: 0.9337\n",
      "Epoch 14/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3099 - accuracy: 0.9045 - val_loss: 0.2177 - val_accuracy: 0.9352\n",
      "Epoch 15/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3021 - accuracy: 0.9060 - val_loss: 0.2113 - val_accuracy: 0.9371\n",
      "Epoch 16/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2949 - accuracy: 0.9090 - val_loss: 0.2144 - val_accuracy: 0.9350\n",
      "Epoch 17/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2842 - accuracy: 0.9126 - val_loss: 0.1985 - val_accuracy: 0.9401\n",
      "Epoch 18/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2759 - accuracy: 0.9138 - val_loss: 0.1964 - val_accuracy: 0.9394\n",
      "Epoch 19/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2702 - accuracy: 0.9167 - val_loss: 0.1883 - val_accuracy: 0.9422\n",
      "Epoch 20/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2615 - accuracy: 0.9200 - val_loss: 0.1883 - val_accuracy: 0.9418\n",
      "Epoch 21/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2557 - accuracy: 0.9214 - val_loss: 0.1817 - val_accuracy: 0.9436\n",
      "Epoch 22/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2485 - accuracy: 0.9241 - val_loss: 0.1785 - val_accuracy: 0.9450\n",
      "Epoch 23/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2400 - accuracy: 0.9262 - val_loss: 0.1770 - val_accuracy: 0.9433\n",
      "Epoch 24/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2351 - accuracy: 0.9286 - val_loss: 0.1722 - val_accuracy: 0.9453\n",
      "Epoch 25/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2290 - accuracy: 0.9301 - val_loss: 0.1683 - val_accuracy: 0.9485\n",
      "Epoch 26/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2235 - accuracy: 0.9309 - val_loss: 0.1625 - val_accuracy: 0.9502\n",
      "Epoch 27/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2164 - accuracy: 0.9338 - val_loss: 0.1608 - val_accuracy: 0.9488\n",
      "Epoch 28/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2121 - accuracy: 0.9351 - val_loss: 0.1608 - val_accuracy: 0.9501\n",
      "Epoch 29/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2072 - accuracy: 0.9366 - val_loss: 0.1564 - val_accuracy: 0.9505\n",
      "Epoch 30/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2020 - accuracy: 0.9380 - val_loss: 0.1556 - val_accuracy: 0.9498\n",
      "Epoch 31/200\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.1951 - accuracy: 0.9416 - val_loss: 0.1527 - val_accuracy: 0.9515\n",
      "Epoch 32/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1928 - accuracy: 0.9405 - val_loss: 0.1487 - val_accuracy: 0.9514\n",
      "Epoch 33/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1881 - accuracy: 0.9432 - val_loss: 0.1474 - val_accuracy: 0.9525\n",
      "Epoch 34/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1855 - accuracy: 0.9432 - val_loss: 0.1441 - val_accuracy: 0.9538\n",
      "Epoch 35/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1797 - accuracy: 0.9455 - val_loss: 0.1443 - val_accuracy: 0.9530\n",
      "Epoch 36/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1756 - accuracy: 0.9470 - val_loss: 0.1373 - val_accuracy: 0.9545\n",
      "Epoch 37/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1740 - accuracy: 0.9466 - val_loss: 0.1371 - val_accuracy: 0.9545\n",
      "Epoch 38/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1689 - accuracy: 0.9482 - val_loss: 0.1348 - val_accuracy: 0.9555\n",
      "Epoch 39/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1668 - accuracy: 0.9488 - val_loss: 0.1311 - val_accuracy: 0.9573\n",
      "Epoch 40/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1646 - accuracy: 0.9493 - val_loss: 0.1310 - val_accuracy: 0.9563\n",
      "Epoch 41/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1605 - accuracy: 0.9507 - val_loss: 0.1286 - val_accuracy: 0.9568\n",
      "Epoch 42/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1576 - accuracy: 0.9516 - val_loss: 0.1276 - val_accuracy: 0.9572\n",
      "Epoch 43/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1551 - accuracy: 0.9519 - val_loss: 0.1274 - val_accuracy: 0.9573\n",
      "Epoch 44/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1520 - accuracy: 0.9532 - val_loss: 0.1253 - val_accuracy: 0.9586\n",
      "Epoch 45/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1501 - accuracy: 0.9540 - val_loss: 0.1243 - val_accuracy: 0.9586\n",
      "Epoch 46/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1469 - accuracy: 0.9540 - val_loss: 0.1229 - val_accuracy: 0.9580\n",
      "Epoch 47/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1456 - accuracy: 0.9548 - val_loss: 0.1221 - val_accuracy: 0.9592\n",
      "Epoch 48/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1442 - accuracy: 0.9546 - val_loss: 0.1247 - val_accuracy: 0.9575\n",
      "Epoch 49/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1428 - accuracy: 0.9558 - val_loss: 0.1189 - val_accuracy: 0.9600\n",
      "Epoch 50/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1382 - accuracy: 0.9568 - val_loss: 0.1210 - val_accuracy: 0.9590\n",
      "Epoch 51/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1376 - accuracy: 0.9570 - val_loss: 0.1169 - val_accuracy: 0.9602\n",
      "Epoch 52/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1350 - accuracy: 0.9576 - val_loss: 0.1162 - val_accuracy: 0.9615\n",
      "Epoch 53/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1326 - accuracy: 0.9580 - val_loss: 0.1152 - val_accuracy: 0.9615\n",
      "Epoch 54/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1323 - accuracy: 0.9585 - val_loss: 0.1164 - val_accuracy: 0.9610\n",
      "Epoch 55/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1305 - accuracy: 0.9590 - val_loss: 0.1149 - val_accuracy: 0.9607\n",
      "Epoch 56/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1287 - accuracy: 0.9589 - val_loss: 0.1137 - val_accuracy: 0.9617\n",
      "Epoch 57/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1255 - accuracy: 0.9607 - val_loss: 0.1121 - val_accuracy: 0.9612\n",
      "Epoch 58/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1243 - accuracy: 0.9608 - val_loss: 0.1125 - val_accuracy: 0.9617\n",
      "Epoch 59/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1250 - accuracy: 0.9604 - val_loss: 0.1108 - val_accuracy: 0.9625\n",
      "Epoch 60/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1217 - accuracy: 0.9613 - val_loss: 0.1128 - val_accuracy: 0.9621\n",
      "Epoch 61/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1200 - accuracy: 0.9626 - val_loss: 0.1107 - val_accuracy: 0.9616\n",
      "Epoch 62/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1194 - accuracy: 0.9617 - val_loss: 0.1104 - val_accuracy: 0.9625\n",
      "Epoch 63/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1172 - accuracy: 0.9624 - val_loss: 0.1127 - val_accuracy: 0.9623\n",
      "Epoch 64/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1164 - accuracy: 0.9629 - val_loss: 0.1101 - val_accuracy: 0.9626\n",
      "Epoch 65/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1162 - accuracy: 0.9630 - val_loss: 0.1065 - val_accuracy: 0.9632\n",
      "Epoch 66/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1154 - accuracy: 0.9631 - val_loss: 0.1064 - val_accuracy: 0.9628\n",
      "Epoch 67/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1102 - accuracy: 0.9646 - val_loss: 0.1088 - val_accuracy: 0.9627\n",
      "Epoch 68/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1110 - accuracy: 0.9634 - val_loss: 0.1061 - val_accuracy: 0.9636\n",
      "Epoch 69/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1091 - accuracy: 0.9646 - val_loss: 0.1090 - val_accuracy: 0.9623\n",
      "Epoch 70/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1091 - accuracy: 0.9646 - val_loss: 0.1072 - val_accuracy: 0.9635\n",
      "Epoch 71/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1075 - accuracy: 0.9651 - val_loss: 0.1053 - val_accuracy: 0.9642\n",
      "Epoch 72/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1056 - accuracy: 0.9661 - val_loss: 0.1063 - val_accuracy: 0.9625\n",
      "Epoch 73/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1065 - accuracy: 0.9659 - val_loss: 0.1033 - val_accuracy: 0.9652\n",
      "Epoch 74/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1032 - accuracy: 0.9663 - val_loss: 0.1040 - val_accuracy: 0.9655\n",
      "Epoch 75/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1036 - accuracy: 0.9664 - val_loss: 0.1016 - val_accuracy: 0.9649\n",
      "Epoch 76/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1016 - accuracy: 0.9672 - val_loss: 0.1031 - val_accuracy: 0.9654\n",
      "Epoch 77/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1005 - accuracy: 0.9673 - val_loss: 0.1037 - val_accuracy: 0.9643\n",
      "Epoch 78/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1006 - accuracy: 0.9672 - val_loss: 0.1022 - val_accuracy: 0.9653\n",
      "Epoch 79/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0974 - accuracy: 0.9680 - val_loss: 0.1019 - val_accuracy: 0.9653\n",
      "Epoch 80/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0988 - accuracy: 0.9680 - val_loss: 0.1022 - val_accuracy: 0.9655\n",
      "Epoch 81/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0971 - accuracy: 0.9685 - val_loss: 0.1006 - val_accuracy: 0.9656\n",
      "Epoch 82/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0963 - accuracy: 0.9685 - val_loss: 0.1049 - val_accuracy: 0.9642\n",
      "Epoch 83/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0949 - accuracy: 0.9682 - val_loss: 0.0999 - val_accuracy: 0.9674\n",
      "Epoch 84/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0942 - accuracy: 0.9688 - val_loss: 0.1002 - val_accuracy: 0.9655\n",
      "Epoch 85/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0922 - accuracy: 0.9692 - val_loss: 0.1009 - val_accuracy: 0.9662\n",
      "Epoch 86/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0918 - accuracy: 0.9699 - val_loss: 0.1004 - val_accuracy: 0.9650\n",
      "Epoch 87/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0909 - accuracy: 0.9701 - val_loss: 0.1001 - val_accuracy: 0.9666\n",
      "Epoch 88/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0901 - accuracy: 0.9702 - val_loss: 0.0970 - val_accuracy: 0.9674\n",
      "Epoch 89/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0888 - accuracy: 0.9702 - val_loss: 0.0986 - val_accuracy: 0.9659\n",
      "Epoch 90/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0892 - accuracy: 0.9699 - val_loss: 0.1012 - val_accuracy: 0.9664\n",
      "Epoch 91/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0886 - accuracy: 0.9707 - val_loss: 0.0968 - val_accuracy: 0.9669\n",
      "Epoch 92/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0868 - accuracy: 0.9707 - val_loss: 0.1004 - val_accuracy: 0.9675\n",
      "Epoch 93/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0864 - accuracy: 0.9720 - val_loss: 0.0985 - val_accuracy: 0.9673\n",
      "Epoch 94/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0851 - accuracy: 0.9722 - val_loss: 0.0973 - val_accuracy: 0.9675\n",
      "Epoch 95/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0854 - accuracy: 0.9712 - val_loss: 0.0996 - val_accuracy: 0.9666\n",
      "Epoch 96/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0843 - accuracy: 0.9711 - val_loss: 0.1001 - val_accuracy: 0.9668\n",
      "Epoch 97/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0835 - accuracy: 0.9723 - val_loss: 0.0992 - val_accuracy: 0.9670\n",
      "Epoch 98/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0824 - accuracy: 0.9721 - val_loss: 0.0962 - val_accuracy: 0.9684\n",
      "Epoch 99/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0816 - accuracy: 0.9722 - val_loss: 0.0995 - val_accuracy: 0.9669\n",
      "Epoch 100/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0818 - accuracy: 0.9725 - val_loss: 0.0971 - val_accuracy: 0.9684\n",
      "Epoch 101/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0803 - accuracy: 0.9732 - val_loss: 0.0977 - val_accuracy: 0.9674\n",
      "Epoch 102/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0799 - accuracy: 0.9731 - val_loss: 0.0964 - val_accuracy: 0.9677\n",
      "Epoch 103/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0791 - accuracy: 0.9734 - val_loss: 0.1003 - val_accuracy: 0.9677\n",
      "Epoch 104/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0785 - accuracy: 0.9731 - val_loss: 0.0959 - val_accuracy: 0.9689\n",
      "Epoch 105/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0765 - accuracy: 0.9738 - val_loss: 0.0980 - val_accuracy: 0.9682\n",
      "Epoch 106/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0764 - accuracy: 0.9740 - val_loss: 0.0948 - val_accuracy: 0.9694\n",
      "Epoch 107/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0761 - accuracy: 0.9749 - val_loss: 0.0976 - val_accuracy: 0.9689\n",
      "Epoch 108/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0741 - accuracy: 0.9743 - val_loss: 0.0946 - val_accuracy: 0.9691\n",
      "Epoch 109/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0736 - accuracy: 0.9746 - val_loss: 0.0970 - val_accuracy: 0.9682\n",
      "Epoch 110/200\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.0729 - accuracy: 0.9753 - val_loss: 0.0959 - val_accuracy: 0.9690\n",
      "Epoch 111/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0727 - accuracy: 0.9757 - val_loss: 0.0949 - val_accuracy: 0.9694\n",
      "Epoch 112/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0727 - accuracy: 0.9758 - val_loss: 0.0962 - val_accuracy: 0.9684\n",
      "Epoch 113/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0728 - accuracy: 0.9753 - val_loss: 0.0973 - val_accuracy: 0.9695\n",
      "Epoch 114/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0710 - accuracy: 0.9759 - val_loss: 0.0959 - val_accuracy: 0.9688\n",
      "Epoch 115/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0713 - accuracy: 0.9757 - val_loss: 0.1005 - val_accuracy: 0.9675\n",
      "Epoch 116/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0709 - accuracy: 0.9756 - val_loss: 0.0987 - val_accuracy: 0.9685\n",
      "Epoch 117/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0701 - accuracy: 0.9759 - val_loss: 0.0945 - val_accuracy: 0.9699\n",
      "Epoch 118/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0706 - accuracy: 0.9761 - val_loss: 0.0963 - val_accuracy: 0.9700\n",
      "Epoch 119/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0700 - accuracy: 0.9764 - val_loss: 0.0973 - val_accuracy: 0.9684\n",
      "Epoch 120/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0697 - accuracy: 0.9757 - val_loss: 0.0947 - val_accuracy: 0.9706\n",
      "Epoch 121/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0666 - accuracy: 0.9772 - val_loss: 0.0981 - val_accuracy: 0.9685\n",
      "Epoch 122/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0668 - accuracy: 0.9771 - val_loss: 0.0950 - val_accuracy: 0.9703\n",
      "Epoch 123/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0661 - accuracy: 0.9773 - val_loss: 0.0941 - val_accuracy: 0.9698\n",
      "Epoch 124/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0663 - accuracy: 0.9773 - val_loss: 0.0966 - val_accuracy: 0.9697\n",
      "Epoch 125/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0657 - accuracy: 0.9776 - val_loss: 0.0948 - val_accuracy: 0.9699\n",
      "Epoch 126/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0648 - accuracy: 0.9775 - val_loss: 0.0945 - val_accuracy: 0.9694\n",
      "Epoch 127/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0646 - accuracy: 0.9779 - val_loss: 0.0999 - val_accuracy: 0.9690\n",
      "Epoch 128/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0631 - accuracy: 0.9781 - val_loss: 0.0964 - val_accuracy: 0.9696\n",
      "Epoch 129/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0642 - accuracy: 0.9785 - val_loss: 0.0984 - val_accuracy: 0.9696\n",
      "Epoch 130/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0623 - accuracy: 0.9787 - val_loss: 0.0996 - val_accuracy: 0.9692\n",
      "Epoch 131/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0639 - accuracy: 0.9780 - val_loss: 0.0969 - val_accuracy: 0.9695\n",
      "Epoch 132/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0629 - accuracy: 0.9787 - val_loss: 0.0972 - val_accuracy: 0.9701\n",
      "Epoch 133/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0614 - accuracy: 0.9790 - val_loss: 0.1010 - val_accuracy: 0.9691\n",
      "Epoch 134/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0620 - accuracy: 0.9790 - val_loss: 0.0983 - val_accuracy: 0.9702\n",
      "Epoch 135/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0604 - accuracy: 0.9787 - val_loss: 0.0949 - val_accuracy: 0.9708\n",
      "Epoch 136/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0601 - accuracy: 0.9790 - val_loss: 0.0963 - val_accuracy: 0.9692\n",
      "Epoch 137/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0597 - accuracy: 0.9794 - val_loss: 0.0984 - val_accuracy: 0.9691\n",
      "Epoch 138/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0596 - accuracy: 0.9791 - val_loss: 0.1009 - val_accuracy: 0.9690\n",
      "Epoch 139/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0576 - accuracy: 0.9798 - val_loss: 0.0982 - val_accuracy: 0.9699\n",
      "Epoch 140/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0597 - accuracy: 0.9793 - val_loss: 0.0968 - val_accuracy: 0.9704\n",
      "Epoch 141/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0593 - accuracy: 0.9792 - val_loss: 0.0975 - val_accuracy: 0.9709\n",
      "Epoch 142/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0586 - accuracy: 0.9797 - val_loss: 0.1003 - val_accuracy: 0.9704\n",
      "Epoch 143/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0586 - accuracy: 0.9800 - val_loss: 0.0974 - val_accuracy: 0.9705\n",
      "Epoch 144/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0577 - accuracy: 0.9805 - val_loss: 0.0978 - val_accuracy: 0.9709\n",
      "Epoch 145/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0573 - accuracy: 0.9792 - val_loss: 0.0986 - val_accuracy: 0.9699\n",
      "Epoch 146/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0569 - accuracy: 0.9804 - val_loss: 0.0957 - val_accuracy: 0.9701\n",
      "Epoch 147/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0566 - accuracy: 0.9803 - val_loss: 0.0961 - val_accuracy: 0.9703\n",
      "Epoch 148/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0563 - accuracy: 0.9807 - val_loss: 0.0963 - val_accuracy: 0.9709\n",
      "Epoch 149/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0551 - accuracy: 0.9812 - val_loss: 0.0983 - val_accuracy: 0.9700\n",
      "Epoch 150/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0544 - accuracy: 0.9813 - val_loss: 0.0973 - val_accuracy: 0.9710\n",
      "Epoch 151/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0548 - accuracy: 0.9812 - val_loss: 0.0989 - val_accuracy: 0.9706\n",
      "Epoch 152/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0544 - accuracy: 0.9807 - val_loss: 0.0969 - val_accuracy: 0.9707\n",
      "Epoch 153/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0540 - accuracy: 0.9809 - val_loss: 0.0979 - val_accuracy: 0.9701\n",
      "Epoch 154/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0528 - accuracy: 0.9815 - val_loss: 0.0987 - val_accuracy: 0.9707\n",
      "Epoch 155/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0525 - accuracy: 0.9817 - val_loss: 0.0992 - val_accuracy: 0.9704\n",
      "Epoch 156/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0506 - accuracy: 0.9822 - val_loss: 0.0996 - val_accuracy: 0.9702\n",
      "Epoch 157/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0518 - accuracy: 0.9820 - val_loss: 0.1000 - val_accuracy: 0.9702\n",
      "Epoch 158/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0527 - accuracy: 0.9817 - val_loss: 0.0997 - val_accuracy: 0.9699\n",
      "Epoch 159/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0511 - accuracy: 0.9822 - val_loss: 0.0963 - val_accuracy: 0.9705\n",
      "Epoch 160/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0513 - accuracy: 0.9820 - val_loss: 0.1031 - val_accuracy: 0.9696\n",
      "Epoch 161/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0523 - accuracy: 0.9821 - val_loss: 0.1006 - val_accuracy: 0.9700\n",
      "Epoch 162/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0524 - accuracy: 0.9819 - val_loss: 0.1025 - val_accuracy: 0.9698\n",
      "Epoch 163/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0515 - accuracy: 0.9821 - val_loss: 0.0983 - val_accuracy: 0.9711\n",
      "Epoch 164/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0500 - accuracy: 0.9825 - val_loss: 0.1002 - val_accuracy: 0.9709\n",
      "Epoch 165/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0488 - accuracy: 0.9828 - val_loss: 0.0998 - val_accuracy: 0.9712\n",
      "Epoch 166/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0500 - accuracy: 0.9824 - val_loss: 0.0978 - val_accuracy: 0.9702\n",
      "Epoch 167/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0507 - accuracy: 0.9823 - val_loss: 0.0990 - val_accuracy: 0.9702\n",
      "Epoch 168/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0482 - accuracy: 0.9836 - val_loss: 0.1045 - val_accuracy: 0.9697\n",
      "Epoch 169/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0488 - accuracy: 0.9831 - val_loss: 0.1017 - val_accuracy: 0.9697\n",
      "Epoch 170/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0479 - accuracy: 0.9830 - val_loss: 0.0988 - val_accuracy: 0.9705\n",
      "Epoch 171/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0473 - accuracy: 0.9836 - val_loss: 0.1004 - val_accuracy: 0.9706\n",
      "Epoch 172/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0481 - accuracy: 0.9832 - val_loss: 0.1032 - val_accuracy: 0.9704\n",
      "Epoch 173/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0469 - accuracy: 0.9836 - val_loss: 0.1007 - val_accuracy: 0.9709\n",
      "Epoch 174/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0460 - accuracy: 0.9843 - val_loss: 0.1026 - val_accuracy: 0.9704\n",
      "Epoch 175/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0473 - accuracy: 0.9832 - val_loss: 0.1054 - val_accuracy: 0.9700\n",
      "Epoch 176/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0477 - accuracy: 0.9833 - val_loss: 0.0981 - val_accuracy: 0.9704\n",
      "Epoch 177/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0468 - accuracy: 0.9835 - val_loss: 0.1001 - val_accuracy: 0.9709\n",
      "Epoch 178/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0458 - accuracy: 0.9840 - val_loss: 0.1019 - val_accuracy: 0.9709\n",
      "Epoch 179/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0455 - accuracy: 0.9843 - val_loss: 0.1017 - val_accuracy: 0.9705\n",
      "Epoch 180/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0461 - accuracy: 0.9837 - val_loss: 0.1027 - val_accuracy: 0.9710\n",
      "Epoch 181/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0446 - accuracy: 0.9842 - val_loss: 0.1015 - val_accuracy: 0.9707\n",
      "Epoch 182/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0449 - accuracy: 0.9846 - val_loss: 0.1017 - val_accuracy: 0.9707\n",
      "Epoch 183/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0461 - accuracy: 0.9842 - val_loss: 0.1026 - val_accuracy: 0.9707\n",
      "Epoch 184/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0450 - accuracy: 0.9844 - val_loss: 0.1022 - val_accuracy: 0.9709\n",
      "Epoch 185/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0445 - accuracy: 0.9847 - val_loss: 0.1013 - val_accuracy: 0.9711\n",
      "Epoch 186/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0442 - accuracy: 0.9844 - val_loss: 0.1023 - val_accuracy: 0.9704\n",
      "Epoch 187/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0441 - accuracy: 0.9845 - val_loss: 0.1029 - val_accuracy: 0.9707\n",
      "Epoch 188/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0441 - accuracy: 0.9848 - val_loss: 0.1005 - val_accuracy: 0.9709\n",
      "Epoch 189/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0442 - accuracy: 0.9842 - val_loss: 0.1024 - val_accuracy: 0.9718\n",
      "Epoch 190/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0427 - accuracy: 0.9857 - val_loss: 0.1012 - val_accuracy: 0.9707\n",
      "Epoch 191/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0431 - accuracy: 0.9846 - val_loss: 0.1040 - val_accuracy: 0.9703\n",
      "Epoch 192/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0427 - accuracy: 0.9847 - val_loss: 0.1109 - val_accuracy: 0.9697\n",
      "Epoch 193/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0425 - accuracy: 0.9850 - val_loss: 0.1057 - val_accuracy: 0.9701\n",
      "Epoch 194/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0444 - accuracy: 0.9844 - val_loss: 0.1093 - val_accuracy: 0.9693\n",
      "Epoch 195/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0421 - accuracy: 0.9858 - val_loss: 0.1037 - val_accuracy: 0.9714\n",
      "Epoch 196/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0429 - accuracy: 0.9849 - val_loss: 0.1056 - val_accuracy: 0.9711\n",
      "Epoch 197/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0414 - accuracy: 0.9854 - val_loss: 0.1024 - val_accuracy: 0.9717\n",
      "Epoch 198/200\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.0428 - accuracy: 0.9848 - val_loss: 0.1049 - val_accuracy: 0.9712\n",
      "Epoch 199/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0410 - accuracy: 0.9856 - val_loss: 0.1042 - val_accuracy: 0.9707\n",
      "Epoch 200/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0404 - accuracy: 0.9860 - val_loss: 0.1031 - val_accuracy: 0.9709\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "\n",
      "Fold 7:\n",
      "X_train shape: (86692, 1, 1280)\n",
      "X_val shape: (9632, 1, 1280)\n",
      "Epoch 1/200\n",
      "43/43 [==============================] - 4s 22ms/step - loss: 1.2729 - accuracy: 0.5779 - val_loss: 0.9531 - val_accuracy: 0.6678\n",
      "Epoch 2/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.7919 - accuracy: 0.7474 - val_loss: 0.6009 - val_accuracy: 0.8317\n",
      "Epoch 3/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.5620 - accuracy: 0.8318 - val_loss: 0.4425 - val_accuracy: 0.8776\n",
      "Epoch 4/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.4692 - accuracy: 0.8609 - val_loss: 0.3744 - val_accuracy: 0.8947\n",
      "Epoch 5/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.4261 - accuracy: 0.8727 - val_loss: 0.3424 - val_accuracy: 0.9025\n",
      "Epoch 6/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.4014 - accuracy: 0.8790 - val_loss: 0.3144 - val_accuracy: 0.9119\n",
      "Epoch 7/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3836 - accuracy: 0.8843 - val_loss: 0.2947 - val_accuracy: 0.9148\n",
      "Epoch 8/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3663 - accuracy: 0.8879 - val_loss: 0.2752 - val_accuracy: 0.9211\n",
      "Epoch 9/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3541 - accuracy: 0.8914 - val_loss: 0.2659 - val_accuracy: 0.9240\n",
      "Epoch 10/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3415 - accuracy: 0.8953 - val_loss: 0.2525 - val_accuracy: 0.9286\n",
      "Epoch 11/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3353 - accuracy: 0.8950 - val_loss: 0.2488 - val_accuracy: 0.9276\n",
      "Epoch 12/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3225 - accuracy: 0.9009 - val_loss: 0.2380 - val_accuracy: 0.9323\n",
      "Epoch 13/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3118 - accuracy: 0.9038 - val_loss: 0.2302 - val_accuracy: 0.9328\n",
      "Epoch 14/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3063 - accuracy: 0.9063 - val_loss: 0.2245 - val_accuracy: 0.9339\n",
      "Epoch 15/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2981 - accuracy: 0.9089 - val_loss: 0.2161 - val_accuracy: 0.9368\n",
      "Epoch 16/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2873 - accuracy: 0.9109 - val_loss: 0.2117 - val_accuracy: 0.9353\n",
      "Epoch 17/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2817 - accuracy: 0.9124 - val_loss: 0.2040 - val_accuracy: 0.9391\n",
      "Epoch 18/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2729 - accuracy: 0.9158 - val_loss: 0.2020 - val_accuracy: 0.9385\n",
      "Epoch 19/200\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.2647 - accuracy: 0.9176 - val_loss: 0.1976 - val_accuracy: 0.9388\n",
      "Epoch 20/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2575 - accuracy: 0.9205 - val_loss: 0.1931 - val_accuracy: 0.9421\n",
      "Epoch 21/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2492 - accuracy: 0.9226 - val_loss: 0.1883 - val_accuracy: 0.9432\n",
      "Epoch 22/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2431 - accuracy: 0.9244 - val_loss: 0.1843 - val_accuracy: 0.9447\n",
      "Epoch 23/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2363 - accuracy: 0.9268 - val_loss: 0.1808 - val_accuracy: 0.9453\n",
      "Epoch 24/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2282 - accuracy: 0.9304 - val_loss: 0.1752 - val_accuracy: 0.9482\n",
      "Epoch 25/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2232 - accuracy: 0.9312 - val_loss: 0.1724 - val_accuracy: 0.9474\n",
      "Epoch 26/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2180 - accuracy: 0.9331 - val_loss: 0.1716 - val_accuracy: 0.9471\n",
      "Epoch 27/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2111 - accuracy: 0.9355 - val_loss: 0.1661 - val_accuracy: 0.9481\n",
      "Epoch 28/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2057 - accuracy: 0.9375 - val_loss: 0.1648 - val_accuracy: 0.9483\n",
      "Epoch 29/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2018 - accuracy: 0.9383 - val_loss: 0.1622 - val_accuracy: 0.9484\n",
      "Epoch 30/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1949 - accuracy: 0.9401 - val_loss: 0.1580 - val_accuracy: 0.9508\n",
      "Epoch 31/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1923 - accuracy: 0.9412 - val_loss: 0.1547 - val_accuracy: 0.9522\n",
      "Epoch 32/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1865 - accuracy: 0.9432 - val_loss: 0.1513 - val_accuracy: 0.9519\n",
      "Epoch 33/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1824 - accuracy: 0.9442 - val_loss: 0.1517 - val_accuracy: 0.9520\n",
      "Epoch 34/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1790 - accuracy: 0.9450 - val_loss: 0.1489 - val_accuracy: 0.9530\n",
      "Epoch 35/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1749 - accuracy: 0.9461 - val_loss: 0.1462 - val_accuracy: 0.9535\n",
      "Epoch 36/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1728 - accuracy: 0.9470 - val_loss: 0.1445 - val_accuracy: 0.9546\n",
      "Epoch 37/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1679 - accuracy: 0.9484 - val_loss: 0.1423 - val_accuracy: 0.9549\n",
      "Epoch 38/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1648 - accuracy: 0.9497 - val_loss: 0.1436 - val_accuracy: 0.9547\n",
      "Epoch 39/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1623 - accuracy: 0.9498 - val_loss: 0.1402 - val_accuracy: 0.9550\n",
      "Epoch 40/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1586 - accuracy: 0.9509 - val_loss: 0.1390 - val_accuracy: 0.9556\n",
      "Epoch 41/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1555 - accuracy: 0.9520 - val_loss: 0.1391 - val_accuracy: 0.9557\n",
      "Epoch 42/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1528 - accuracy: 0.9527 - val_loss: 0.1371 - val_accuracy: 0.9562\n",
      "Epoch 43/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1505 - accuracy: 0.9534 - val_loss: 0.1354 - val_accuracy: 0.9563\n",
      "Epoch 44/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1481 - accuracy: 0.9542 - val_loss: 0.1354 - val_accuracy: 0.9562\n",
      "Epoch 45/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1459 - accuracy: 0.9543 - val_loss: 0.1311 - val_accuracy: 0.9577\n",
      "Epoch 46/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1422 - accuracy: 0.9557 - val_loss: 0.1326 - val_accuracy: 0.9576\n",
      "Epoch 47/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1413 - accuracy: 0.9558 - val_loss: 0.1300 - val_accuracy: 0.9586\n",
      "Epoch 48/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1389 - accuracy: 0.9563 - val_loss: 0.1284 - val_accuracy: 0.9590\n",
      "Epoch 49/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1359 - accuracy: 0.9577 - val_loss: 0.1283 - val_accuracy: 0.9595\n",
      "Epoch 50/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1348 - accuracy: 0.9580 - val_loss: 0.1272 - val_accuracy: 0.9586\n",
      "Epoch 51/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1331 - accuracy: 0.9587 - val_loss: 0.1272 - val_accuracy: 0.9603\n",
      "Epoch 52/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1317 - accuracy: 0.9583 - val_loss: 0.1268 - val_accuracy: 0.9601\n",
      "Epoch 53/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1292 - accuracy: 0.9591 - val_loss: 0.1255 - val_accuracy: 0.9594\n",
      "Epoch 54/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1279 - accuracy: 0.9595 - val_loss: 0.1243 - val_accuracy: 0.9604\n",
      "Epoch 55/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1252 - accuracy: 0.9604 - val_loss: 0.1229 - val_accuracy: 0.9599\n",
      "Epoch 56/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1241 - accuracy: 0.9607 - val_loss: 0.1212 - val_accuracy: 0.9607\n",
      "Epoch 57/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1224 - accuracy: 0.9611 - val_loss: 0.1222 - val_accuracy: 0.9611\n",
      "Epoch 58/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1228 - accuracy: 0.9610 - val_loss: 0.1221 - val_accuracy: 0.9600\n",
      "Epoch 59/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1204 - accuracy: 0.9619 - val_loss: 0.1211 - val_accuracy: 0.9616\n",
      "Epoch 60/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1182 - accuracy: 0.9626 - val_loss: 0.1220 - val_accuracy: 0.9615\n",
      "Epoch 61/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1173 - accuracy: 0.9626 - val_loss: 0.1209 - val_accuracy: 0.9620\n",
      "Epoch 62/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1160 - accuracy: 0.9637 - val_loss: 0.1187 - val_accuracy: 0.9617\n",
      "Epoch 63/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1134 - accuracy: 0.9640 - val_loss: 0.1186 - val_accuracy: 0.9623\n",
      "Epoch 64/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1127 - accuracy: 0.9637 - val_loss: 0.1197 - val_accuracy: 0.9617\n",
      "Epoch 65/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1112 - accuracy: 0.9644 - val_loss: 0.1169 - val_accuracy: 0.9627\n",
      "Epoch 66/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1106 - accuracy: 0.9639 - val_loss: 0.1169 - val_accuracy: 0.9625\n",
      "Epoch 67/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1097 - accuracy: 0.9646 - val_loss: 0.1169 - val_accuracy: 0.9631\n",
      "Epoch 68/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1080 - accuracy: 0.9655 - val_loss: 0.1165 - val_accuracy: 0.9627\n",
      "Epoch 69/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1054 - accuracy: 0.9655 - val_loss: 0.1163 - val_accuracy: 0.9632\n",
      "Epoch 70/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1048 - accuracy: 0.9667 - val_loss: 0.1149 - val_accuracy: 0.9637\n",
      "Epoch 71/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1031 - accuracy: 0.9668 - val_loss: 0.1145 - val_accuracy: 0.9637\n",
      "Epoch 72/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1016 - accuracy: 0.9662 - val_loss: 0.1151 - val_accuracy: 0.9642\n",
      "Epoch 73/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1024 - accuracy: 0.9666 - val_loss: 0.1160 - val_accuracy: 0.9630\n",
      "Epoch 74/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0998 - accuracy: 0.9679 - val_loss: 0.1143 - val_accuracy: 0.9634\n",
      "Epoch 75/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0990 - accuracy: 0.9682 - val_loss: 0.1131 - val_accuracy: 0.9639\n",
      "Epoch 76/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0989 - accuracy: 0.9678 - val_loss: 0.1138 - val_accuracy: 0.9638\n",
      "Epoch 77/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0978 - accuracy: 0.9682 - val_loss: 0.1129 - val_accuracy: 0.9641\n",
      "Epoch 78/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0970 - accuracy: 0.9681 - val_loss: 0.1132 - val_accuracy: 0.9635\n",
      "Epoch 79/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0954 - accuracy: 0.9687 - val_loss: 0.1131 - val_accuracy: 0.9638\n",
      "Epoch 80/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0941 - accuracy: 0.9689 - val_loss: 0.1134 - val_accuracy: 0.9649\n",
      "Epoch 81/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0942 - accuracy: 0.9695 - val_loss: 0.1133 - val_accuracy: 0.9646\n",
      "Epoch 82/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0934 - accuracy: 0.9695 - val_loss: 0.1133 - val_accuracy: 0.9652\n",
      "Epoch 83/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0922 - accuracy: 0.9701 - val_loss: 0.1132 - val_accuracy: 0.9648\n",
      "Epoch 84/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0889 - accuracy: 0.9715 - val_loss: 0.1115 - val_accuracy: 0.9656\n",
      "Epoch 85/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0901 - accuracy: 0.9697 - val_loss: 0.1120 - val_accuracy: 0.9651\n",
      "Epoch 86/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0885 - accuracy: 0.9711 - val_loss: 0.1118 - val_accuracy: 0.9652\n",
      "Epoch 87/200\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.0885 - accuracy: 0.9701 - val_loss: 0.1098 - val_accuracy: 0.9663\n",
      "Epoch 88/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0883 - accuracy: 0.9707 - val_loss: 0.1107 - val_accuracy: 0.9662\n",
      "Epoch 89/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0854 - accuracy: 0.9715 - val_loss: 0.1111 - val_accuracy: 0.9659\n",
      "Epoch 90/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0851 - accuracy: 0.9717 - val_loss: 0.1111 - val_accuracy: 0.9659\n",
      "Epoch 91/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0862 - accuracy: 0.9714 - val_loss: 0.1109 - val_accuracy: 0.9661\n",
      "Epoch 92/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0828 - accuracy: 0.9722 - val_loss: 0.1097 - val_accuracy: 0.9661\n",
      "Epoch 93/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0834 - accuracy: 0.9722 - val_loss: 0.1118 - val_accuracy: 0.9668\n",
      "Epoch 94/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0829 - accuracy: 0.9723 - val_loss: 0.1113 - val_accuracy: 0.9673\n",
      "Epoch 95/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0810 - accuracy: 0.9730 - val_loss: 0.1098 - val_accuracy: 0.9670\n",
      "Epoch 96/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0806 - accuracy: 0.9733 - val_loss: 0.1090 - val_accuracy: 0.9670\n",
      "Epoch 97/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0811 - accuracy: 0.9729 - val_loss: 0.1085 - val_accuracy: 0.9669\n",
      "Epoch 98/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0804 - accuracy: 0.9728 - val_loss: 0.1095 - val_accuracy: 0.9672\n",
      "Epoch 99/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0807 - accuracy: 0.9726 - val_loss: 0.1102 - val_accuracy: 0.9676\n",
      "Epoch 100/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0792 - accuracy: 0.9735 - val_loss: 0.1088 - val_accuracy: 0.9677\n",
      "Epoch 101/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0775 - accuracy: 0.9742 - val_loss: 0.1097 - val_accuracy: 0.9672\n",
      "Epoch 102/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0768 - accuracy: 0.9744 - val_loss: 0.1101 - val_accuracy: 0.9667\n",
      "Epoch 103/200\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0758 - accuracy: 0.9746 - val_loss: 0.1094 - val_accuracy: 0.9674\n",
      "Epoch 104/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0761 - accuracy: 0.9746 - val_loss: 0.1100 - val_accuracy: 0.9674\n",
      "Epoch 105/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0751 - accuracy: 0.9745 - val_loss: 0.1099 - val_accuracy: 0.9669\n",
      "Epoch 106/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0739 - accuracy: 0.9749 - val_loss: 0.1093 - val_accuracy: 0.9673\n",
      "Epoch 107/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0731 - accuracy: 0.9760 - val_loss: 0.1096 - val_accuracy: 0.9670\n",
      "Epoch 108/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0725 - accuracy: 0.9755 - val_loss: 0.1117 - val_accuracy: 0.9676\n",
      "Epoch 109/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0724 - accuracy: 0.9754 - val_loss: 0.1101 - val_accuracy: 0.9675\n",
      "Epoch 110/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0722 - accuracy: 0.9757 - val_loss: 0.1084 - val_accuracy: 0.9677\n",
      "Epoch 111/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0709 - accuracy: 0.9764 - val_loss: 0.1103 - val_accuracy: 0.9674\n",
      "Epoch 112/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0705 - accuracy: 0.9757 - val_loss: 0.1097 - val_accuracy: 0.9680\n",
      "Epoch 113/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0713 - accuracy: 0.9754 - val_loss: 0.1097 - val_accuracy: 0.9683\n",
      "Epoch 114/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0699 - accuracy: 0.9763 - val_loss: 0.1095 - val_accuracy: 0.9688\n",
      "Epoch 115/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0691 - accuracy: 0.9764 - val_loss: 0.1083 - val_accuracy: 0.9688\n",
      "Epoch 116/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0677 - accuracy: 0.9768 - val_loss: 0.1084 - val_accuracy: 0.9674\n",
      "Epoch 117/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0684 - accuracy: 0.9768 - val_loss: 0.1087 - val_accuracy: 0.9700\n",
      "Epoch 118/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0679 - accuracy: 0.9765 - val_loss: 0.1102 - val_accuracy: 0.9691\n",
      "Epoch 119/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0679 - accuracy: 0.9768 - val_loss: 0.1077 - val_accuracy: 0.9679\n",
      "Epoch 120/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0663 - accuracy: 0.9773 - val_loss: 0.1080 - val_accuracy: 0.9679\n",
      "Epoch 121/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0646 - accuracy: 0.9779 - val_loss: 0.1102 - val_accuracy: 0.9692\n",
      "Epoch 122/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0659 - accuracy: 0.9779 - val_loss: 0.1089 - val_accuracy: 0.9680\n",
      "Epoch 123/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0656 - accuracy: 0.9775 - val_loss: 0.1098 - val_accuracy: 0.9689\n",
      "Epoch 124/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0633 - accuracy: 0.9785 - val_loss: 0.1103 - val_accuracy: 0.9692\n",
      "Epoch 125/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0633 - accuracy: 0.9782 - val_loss: 0.1099 - val_accuracy: 0.9673\n",
      "Epoch 126/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0638 - accuracy: 0.9781 - val_loss: 0.1112 - val_accuracy: 0.9692\n",
      "Epoch 127/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0619 - accuracy: 0.9791 - val_loss: 0.1116 - val_accuracy: 0.9692\n",
      "Epoch 128/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0618 - accuracy: 0.9787 - val_loss: 0.1103 - val_accuracy: 0.9691\n",
      "Epoch 129/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0622 - accuracy: 0.9784 - val_loss: 0.1127 - val_accuracy: 0.9680\n",
      "Epoch 130/200\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0605 - accuracy: 0.9793 - val_loss: 0.1111 - val_accuracy: 0.9689\n",
      "Epoch 131/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0605 - accuracy: 0.9790 - val_loss: 0.1106 - val_accuracy: 0.9686\n",
      "Epoch 132/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0618 - accuracy: 0.9788 - val_loss: 0.1125 - val_accuracy: 0.9684\n",
      "Epoch 133/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0600 - accuracy: 0.9791 - val_loss: 0.1103 - val_accuracy: 0.9690\n",
      "Epoch 134/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0597 - accuracy: 0.9797 - val_loss: 0.1118 - val_accuracy: 0.9678\n",
      "Epoch 135/200\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.0582 - accuracy: 0.9799 - val_loss: 0.1122 - val_accuracy: 0.9689\n",
      "Epoch 136/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0593 - accuracy: 0.9798 - val_loss: 0.1103 - val_accuracy: 0.9677\n",
      "Epoch 137/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0600 - accuracy: 0.9795 - val_loss: 0.1146 - val_accuracy: 0.9688\n",
      "Epoch 138/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0586 - accuracy: 0.9796 - val_loss: 0.1134 - val_accuracy: 0.9688\n",
      "Epoch 139/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0567 - accuracy: 0.9802 - val_loss: 0.1106 - val_accuracy: 0.9689\n",
      "Epoch 140/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0563 - accuracy: 0.9805 - val_loss: 0.1127 - val_accuracy: 0.9693\n",
      "Epoch 141/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0569 - accuracy: 0.9801 - val_loss: 0.1128 - val_accuracy: 0.9691\n",
      "Epoch 142/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0561 - accuracy: 0.9806 - val_loss: 0.1127 - val_accuracy: 0.9696\n",
      "Epoch 143/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0548 - accuracy: 0.9812 - val_loss: 0.1110 - val_accuracy: 0.9685\n",
      "Epoch 144/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0565 - accuracy: 0.9804 - val_loss: 0.1106 - val_accuracy: 0.9696\n",
      "Epoch 145/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0545 - accuracy: 0.9804 - val_loss: 0.1120 - val_accuracy: 0.9694\n",
      "Epoch 146/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0560 - accuracy: 0.9806 - val_loss: 0.1111 - val_accuracy: 0.9700\n",
      "Epoch 147/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0540 - accuracy: 0.9816 - val_loss: 0.1119 - val_accuracy: 0.9701\n",
      "Epoch 148/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0535 - accuracy: 0.9812 - val_loss: 0.1133 - val_accuracy: 0.9694\n",
      "Epoch 149/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0549 - accuracy: 0.9806 - val_loss: 0.1121 - val_accuracy: 0.9699\n",
      "Epoch 150/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0536 - accuracy: 0.9809 - val_loss: 0.1191 - val_accuracy: 0.9690\n",
      "Epoch 151/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0526 - accuracy: 0.9816 - val_loss: 0.1143 - val_accuracy: 0.9696\n",
      "Epoch 152/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0525 - accuracy: 0.9816 - val_loss: 0.1137 - val_accuracy: 0.9695\n",
      "Epoch 153/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0529 - accuracy: 0.9817 - val_loss: 0.1123 - val_accuracy: 0.9688\n",
      "Epoch 154/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0518 - accuracy: 0.9819 - val_loss: 0.1131 - val_accuracy: 0.9698\n",
      "Epoch 155/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0516 - accuracy: 0.9819 - val_loss: 0.1128 - val_accuracy: 0.9685\n",
      "Epoch 156/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0522 - accuracy: 0.9817 - val_loss: 0.1150 - val_accuracy: 0.9690\n",
      "Epoch 157/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0513 - accuracy: 0.9815 - val_loss: 0.1141 - val_accuracy: 0.9692\n",
      "Epoch 158/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0512 - accuracy: 0.9823 - val_loss: 0.1138 - val_accuracy: 0.9695\n",
      "Epoch 159/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0512 - accuracy: 0.9826 - val_loss: 0.1140 - val_accuracy: 0.9685\n",
      "Epoch 160/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0491 - accuracy: 0.9827 - val_loss: 0.1143 - val_accuracy: 0.9678\n",
      "Epoch 161/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0493 - accuracy: 0.9829 - val_loss: 0.1131 - val_accuracy: 0.9691\n",
      "Epoch 162/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0487 - accuracy: 0.9829 - val_loss: 0.1166 - val_accuracy: 0.9699\n",
      "Epoch 163/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0493 - accuracy: 0.9828 - val_loss: 0.1140 - val_accuracy: 0.9691\n",
      "Epoch 164/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0496 - accuracy: 0.9825 - val_loss: 0.1140 - val_accuracy: 0.9691\n",
      "Epoch 165/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0478 - accuracy: 0.9834 - val_loss: 0.1139 - val_accuracy: 0.9696\n",
      "Epoch 166/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0479 - accuracy: 0.9834 - val_loss: 0.1165 - val_accuracy: 0.9704\n",
      "Epoch 167/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0476 - accuracy: 0.9831 - val_loss: 0.1166 - val_accuracy: 0.9699\n",
      "Epoch 168/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0480 - accuracy: 0.9838 - val_loss: 0.1171 - val_accuracy: 0.9693\n",
      "Epoch 169/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0464 - accuracy: 0.9835 - val_loss: 0.1147 - val_accuracy: 0.9695\n",
      "Epoch 170/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0460 - accuracy: 0.9840 - val_loss: 0.1149 - val_accuracy: 0.9690\n",
      "Epoch 171/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0475 - accuracy: 0.9835 - val_loss: 0.1161 - val_accuracy: 0.9681\n",
      "Epoch 172/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0467 - accuracy: 0.9834 - val_loss: 0.1168 - val_accuracy: 0.9685\n",
      "Epoch 173/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0471 - accuracy: 0.9836 - val_loss: 0.1178 - val_accuracy: 0.9693\n",
      "Epoch 174/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0456 - accuracy: 0.9843 - val_loss: 0.1197 - val_accuracy: 0.9689\n",
      "Epoch 175/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0461 - accuracy: 0.9837 - val_loss: 0.1205 - val_accuracy: 0.9695\n",
      "Epoch 176/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0460 - accuracy: 0.9840 - val_loss: 0.1185 - val_accuracy: 0.9679\n",
      "Epoch 177/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0451 - accuracy: 0.9845 - val_loss: 0.1179 - val_accuracy: 0.9702\n",
      "Epoch 178/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0456 - accuracy: 0.9843 - val_loss: 0.1164 - val_accuracy: 0.9692\n",
      "Epoch 179/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0456 - accuracy: 0.9844 - val_loss: 0.1168 - val_accuracy: 0.9695\n",
      "Epoch 180/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0453 - accuracy: 0.9841 - val_loss: 0.1169 - val_accuracy: 0.9686\n",
      "Epoch 181/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0432 - accuracy: 0.9848 - val_loss: 0.1202 - val_accuracy: 0.9703\n",
      "Epoch 182/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0433 - accuracy: 0.9849 - val_loss: 0.1220 - val_accuracy: 0.9692\n",
      "Epoch 183/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0442 - accuracy: 0.9844 - val_loss: 0.1184 - val_accuracy: 0.9696\n",
      "Epoch 184/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0436 - accuracy: 0.9847 - val_loss: 0.1192 - val_accuracy: 0.9682\n",
      "Epoch 185/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0431 - accuracy: 0.9850 - val_loss: 0.1188 - val_accuracy: 0.9695\n",
      "Epoch 186/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0440 - accuracy: 0.9845 - val_loss: 0.1215 - val_accuracy: 0.9691\n",
      "Epoch 187/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0431 - accuracy: 0.9849 - val_loss: 0.1196 - val_accuracy: 0.9692\n",
      "Epoch 188/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0427 - accuracy: 0.9850 - val_loss: 0.1194 - val_accuracy: 0.9688\n",
      "Epoch 189/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0422 - accuracy: 0.9854 - val_loss: 0.1195 - val_accuracy: 0.9699\n",
      "Epoch 190/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0414 - accuracy: 0.9853 - val_loss: 0.1211 - val_accuracy: 0.9697\n",
      "Epoch 191/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0411 - accuracy: 0.9852 - val_loss: 0.1187 - val_accuracy: 0.9702\n",
      "Epoch 192/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0426 - accuracy: 0.9849 - val_loss: 0.1191 - val_accuracy: 0.9694\n",
      "Epoch 193/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0421 - accuracy: 0.9848 - val_loss: 0.1192 - val_accuracy: 0.9700\n",
      "Epoch 194/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0422 - accuracy: 0.9850 - val_loss: 0.1194 - val_accuracy: 0.9709\n",
      "Epoch 195/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0407 - accuracy: 0.9860 - val_loss: 0.1231 - val_accuracy: 0.9694\n",
      "Epoch 196/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0403 - accuracy: 0.9862 - val_loss: 0.1203 - val_accuracy: 0.9702\n",
      "Epoch 197/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0421 - accuracy: 0.9854 - val_loss: 0.1264 - val_accuracy: 0.9696\n",
      "Epoch 198/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0395 - accuracy: 0.9863 - val_loss: 0.1207 - val_accuracy: 0.9694\n",
      "Epoch 199/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0402 - accuracy: 0.9860 - val_loss: 0.1233 - val_accuracy: 0.9695\n",
      "Epoch 200/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0407 - accuracy: 0.9860 - val_loss: 0.1214 - val_accuracy: 0.9697\n",
      "5/5 [==============================] - 1s 4ms/step\n",
      "\n",
      "Fold 8:\n",
      "X_train shape: (86692, 1, 1280)\n",
      "X_val shape: (9632, 1, 1280)\n",
      "Epoch 1/200\n",
      "43/43 [==============================] - 4s 21ms/step - loss: 1.2722 - accuracy: 0.5778 - val_loss: 0.9571 - val_accuracy: 0.6629\n",
      "Epoch 2/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.7899 - accuracy: 0.7466 - val_loss: 0.6087 - val_accuracy: 0.8183\n",
      "Epoch 3/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.5618 - accuracy: 0.8337 - val_loss: 0.4532 - val_accuracy: 0.8699\n",
      "Epoch 4/200\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.4689 - accuracy: 0.8620 - val_loss: 0.3831 - val_accuracy: 0.8904\n",
      "Epoch 5/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.4244 - accuracy: 0.8745 - val_loss: 0.3435 - val_accuracy: 0.9047\n",
      "Epoch 6/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3970 - accuracy: 0.8819 - val_loss: 0.3198 - val_accuracy: 0.9128\n",
      "Epoch 7/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3807 - accuracy: 0.8845 - val_loss: 0.3049 - val_accuracy: 0.9169\n",
      "Epoch 8/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3649 - accuracy: 0.8890 - val_loss: 0.2852 - val_accuracy: 0.9215\n",
      "Epoch 9/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3551 - accuracy: 0.8908 - val_loss: 0.2801 - val_accuracy: 0.9216\n",
      "Epoch 10/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3452 - accuracy: 0.8940 - val_loss: 0.2706 - val_accuracy: 0.9247\n",
      "Epoch 11/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3338 - accuracy: 0.8976 - val_loss: 0.2571 - val_accuracy: 0.9285\n",
      "Epoch 12/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.3253 - accuracy: 0.8996 - val_loss: 0.2484 - val_accuracy: 0.9313\n",
      "Epoch 13/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.3148 - accuracy: 0.9016 - val_loss: 0.2401 - val_accuracy: 0.9332\n",
      "Epoch 14/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3061 - accuracy: 0.9047 - val_loss: 0.2342 - val_accuracy: 0.9350\n",
      "Epoch 15/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2978 - accuracy: 0.9072 - val_loss: 0.2368 - val_accuracy: 0.9316\n",
      "Epoch 16/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2913 - accuracy: 0.9096 - val_loss: 0.2264 - val_accuracy: 0.9362\n",
      "Epoch 17/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.2822 - accuracy: 0.9129 - val_loss: 0.2185 - val_accuracy: 0.9364\n",
      "Epoch 18/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2735 - accuracy: 0.9148 - val_loss: 0.2139 - val_accuracy: 0.9376\n",
      "Epoch 19/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2670 - accuracy: 0.9160 - val_loss: 0.2109 - val_accuracy: 0.9381\n",
      "Epoch 20/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2590 - accuracy: 0.9192 - val_loss: 0.2052 - val_accuracy: 0.9395\n",
      "Epoch 21/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2526 - accuracy: 0.9228 - val_loss: 0.2010 - val_accuracy: 0.9388\n",
      "Epoch 22/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2477 - accuracy: 0.9239 - val_loss: 0.1948 - val_accuracy: 0.9435\n",
      "Epoch 23/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2379 - accuracy: 0.9274 - val_loss: 0.1905 - val_accuracy: 0.9439\n",
      "Epoch 24/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2323 - accuracy: 0.9287 - val_loss: 0.1859 - val_accuracy: 0.9468\n",
      "Epoch 25/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.2237 - accuracy: 0.9317 - val_loss: 0.1846 - val_accuracy: 0.9450\n",
      "Epoch 26/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2200 - accuracy: 0.9323 - val_loss: 0.1836 - val_accuracy: 0.9460\n",
      "Epoch 27/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2126 - accuracy: 0.9350 - val_loss: 0.1786 - val_accuracy: 0.9471\n",
      "Epoch 28/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2099 - accuracy: 0.9350 - val_loss: 0.1706 - val_accuracy: 0.9486\n",
      "Epoch 29/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2025 - accuracy: 0.9376 - val_loss: 0.1709 - val_accuracy: 0.9487\n",
      "Epoch 30/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1971 - accuracy: 0.9403 - val_loss: 0.1702 - val_accuracy: 0.9491\n",
      "Epoch 31/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1948 - accuracy: 0.9401 - val_loss: 0.1631 - val_accuracy: 0.9505\n",
      "Epoch 32/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1873 - accuracy: 0.9431 - val_loss: 0.1690 - val_accuracy: 0.9475\n",
      "Epoch 33/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1860 - accuracy: 0.9426 - val_loss: 0.1606 - val_accuracy: 0.9505\n",
      "Epoch 34/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1807 - accuracy: 0.9449 - val_loss: 0.1579 - val_accuracy: 0.9519\n",
      "Epoch 35/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.1789 - accuracy: 0.9450 - val_loss: 0.1583 - val_accuracy: 0.9511\n",
      "Epoch 36/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1725 - accuracy: 0.9463 - val_loss: 0.1535 - val_accuracy: 0.9532\n",
      "Epoch 37/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1692 - accuracy: 0.9476 - val_loss: 0.1526 - val_accuracy: 0.9526\n",
      "Epoch 38/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1668 - accuracy: 0.9491 - val_loss: 0.1506 - val_accuracy: 0.9541\n",
      "Epoch 39/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1622 - accuracy: 0.9505 - val_loss: 0.1507 - val_accuracy: 0.9533\n",
      "Epoch 40/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.1589 - accuracy: 0.9511 - val_loss: 0.1487 - val_accuracy: 0.9549\n",
      "Epoch 41/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1553 - accuracy: 0.9520 - val_loss: 0.1458 - val_accuracy: 0.9561\n",
      "Epoch 42/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1549 - accuracy: 0.9520 - val_loss: 0.1458 - val_accuracy: 0.9550\n",
      "Epoch 43/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1522 - accuracy: 0.9526 - val_loss: 0.1461 - val_accuracy: 0.9560\n",
      "Epoch 44/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1481 - accuracy: 0.9546 - val_loss: 0.1411 - val_accuracy: 0.9568\n",
      "Epoch 45/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.1467 - accuracy: 0.9543 - val_loss: 0.1415 - val_accuracy: 0.9576\n",
      "Epoch 46/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1442 - accuracy: 0.9550 - val_loss: 0.1401 - val_accuracy: 0.9574\n",
      "Epoch 47/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1418 - accuracy: 0.9559 - val_loss: 0.1387 - val_accuracy: 0.9578\n",
      "Epoch 48/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1405 - accuracy: 0.9563 - val_loss: 0.1388 - val_accuracy: 0.9574\n",
      "Epoch 49/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1383 - accuracy: 0.9572 - val_loss: 0.1390 - val_accuracy: 0.9575\n",
      "Epoch 50/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1373 - accuracy: 0.9569 - val_loss: 0.1348 - val_accuracy: 0.9597\n",
      "Epoch 51/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1366 - accuracy: 0.9572 - val_loss: 0.1357 - val_accuracy: 0.9594\n",
      "Epoch 52/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1328 - accuracy: 0.9590 - val_loss: 0.1343 - val_accuracy: 0.9595\n",
      "Epoch 53/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1288 - accuracy: 0.9590 - val_loss: 0.1336 - val_accuracy: 0.9598\n",
      "Epoch 54/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1278 - accuracy: 0.9600 - val_loss: 0.1335 - val_accuracy: 0.9595\n",
      "Epoch 55/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1255 - accuracy: 0.9603 - val_loss: 0.1333 - val_accuracy: 0.9604\n",
      "Epoch 56/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1251 - accuracy: 0.9603 - val_loss: 0.1316 - val_accuracy: 0.9607\n",
      "Epoch 57/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1238 - accuracy: 0.9606 - val_loss: 0.1297 - val_accuracy: 0.9615\n",
      "Epoch 58/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1217 - accuracy: 0.9608 - val_loss: 0.1322 - val_accuracy: 0.9605\n",
      "Epoch 59/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1201 - accuracy: 0.9618 - val_loss: 0.1287 - val_accuracy: 0.9611\n",
      "Epoch 60/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1188 - accuracy: 0.9626 - val_loss: 0.1285 - val_accuracy: 0.9621\n",
      "Epoch 61/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1180 - accuracy: 0.9627 - val_loss: 0.1280 - val_accuracy: 0.9618\n",
      "Epoch 62/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1168 - accuracy: 0.9622 - val_loss: 0.1271 - val_accuracy: 0.9611\n",
      "Epoch 63/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1139 - accuracy: 0.9636 - val_loss: 0.1277 - val_accuracy: 0.9621\n",
      "Epoch 64/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1132 - accuracy: 0.9642 - val_loss: 0.1253 - val_accuracy: 0.9623\n",
      "Epoch 65/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1127 - accuracy: 0.9641 - val_loss: 0.1257 - val_accuracy: 0.9615\n",
      "Epoch 66/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1102 - accuracy: 0.9642 - val_loss: 0.1284 - val_accuracy: 0.9607\n",
      "Epoch 67/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1089 - accuracy: 0.9650 - val_loss: 0.1266 - val_accuracy: 0.9616\n",
      "Epoch 68/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1095 - accuracy: 0.9655 - val_loss: 0.1262 - val_accuracy: 0.9617\n",
      "Epoch 69/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1060 - accuracy: 0.9660 - val_loss: 0.1246 - val_accuracy: 0.9634\n",
      "Epoch 70/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1063 - accuracy: 0.9651 - val_loss: 0.1226 - val_accuracy: 0.9626\n",
      "Epoch 71/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1043 - accuracy: 0.9658 - val_loss: 0.1228 - val_accuracy: 0.9620\n",
      "Epoch 72/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1026 - accuracy: 0.9664 - val_loss: 0.1246 - val_accuracy: 0.9626\n",
      "Epoch 73/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1020 - accuracy: 0.9671 - val_loss: 0.1219 - val_accuracy: 0.9634\n",
      "Epoch 74/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1021 - accuracy: 0.9669 - val_loss: 0.1215 - val_accuracy: 0.9636\n",
      "Epoch 75/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1002 - accuracy: 0.9673 - val_loss: 0.1223 - val_accuracy: 0.9626\n",
      "Epoch 76/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0989 - accuracy: 0.9685 - val_loss: 0.1212 - val_accuracy: 0.9632\n",
      "Epoch 77/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0990 - accuracy: 0.9675 - val_loss: 0.1233 - val_accuracy: 0.9631\n",
      "Epoch 78/200\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.0974 - accuracy: 0.9683 - val_loss: 0.1223 - val_accuracy: 0.9623\n",
      "Epoch 79/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0955 - accuracy: 0.9683 - val_loss: 0.1194 - val_accuracy: 0.9642\n",
      "Epoch 80/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0945 - accuracy: 0.9694 - val_loss: 0.1208 - val_accuracy: 0.9631\n",
      "Epoch 81/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0927 - accuracy: 0.9699 - val_loss: 0.1200 - val_accuracy: 0.9642\n",
      "Epoch 82/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0925 - accuracy: 0.9703 - val_loss: 0.1187 - val_accuracy: 0.9635\n",
      "Epoch 83/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0924 - accuracy: 0.9691 - val_loss: 0.1195 - val_accuracy: 0.9632\n",
      "Epoch 84/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0918 - accuracy: 0.9694 - val_loss: 0.1199 - val_accuracy: 0.9634\n",
      "Epoch 85/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0902 - accuracy: 0.9698 - val_loss: 0.1193 - val_accuracy: 0.9646\n",
      "Epoch 86/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0881 - accuracy: 0.9707 - val_loss: 0.1206 - val_accuracy: 0.9634\n",
      "Epoch 87/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0898 - accuracy: 0.9706 - val_loss: 0.1197 - val_accuracy: 0.9636\n",
      "Epoch 88/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0892 - accuracy: 0.9701 - val_loss: 0.1181 - val_accuracy: 0.9649\n",
      "Epoch 89/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0874 - accuracy: 0.9711 - val_loss: 0.1173 - val_accuracy: 0.9655\n",
      "Epoch 90/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0867 - accuracy: 0.9715 - val_loss: 0.1177 - val_accuracy: 0.9644\n",
      "Epoch 91/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0844 - accuracy: 0.9723 - val_loss: 0.1207 - val_accuracy: 0.9646\n",
      "Epoch 92/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0844 - accuracy: 0.9721 - val_loss: 0.1180 - val_accuracy: 0.9650\n",
      "Epoch 93/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0839 - accuracy: 0.9716 - val_loss: 0.1181 - val_accuracy: 0.9656\n",
      "Epoch 94/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0827 - accuracy: 0.9724 - val_loss: 0.1176 - val_accuracy: 0.9661\n",
      "Epoch 95/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0818 - accuracy: 0.9729 - val_loss: 0.1224 - val_accuracy: 0.9644\n",
      "Epoch 96/200\n",
      "43/43 [==============================] - 0s 12ms/step - loss: 0.0811 - accuracy: 0.9724 - val_loss: 0.1182 - val_accuracy: 0.9656\n",
      "Epoch 97/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0808 - accuracy: 0.9728 - val_loss: 0.1179 - val_accuracy: 0.9650\n",
      "Epoch 98/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0807 - accuracy: 0.9733 - val_loss: 0.1186 - val_accuracy: 0.9648\n",
      "Epoch 99/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0802 - accuracy: 0.9727 - val_loss: 0.1190 - val_accuracy: 0.9651\n",
      "Epoch 100/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0784 - accuracy: 0.9740 - val_loss: 0.1184 - val_accuracy: 0.9656\n",
      "Epoch 101/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0777 - accuracy: 0.9738 - val_loss: 0.1165 - val_accuracy: 0.9659\n",
      "Epoch 102/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0774 - accuracy: 0.9734 - val_loss: 0.1170 - val_accuracy: 0.9655\n",
      "Epoch 103/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0766 - accuracy: 0.9741 - val_loss: 0.1153 - val_accuracy: 0.9657\n",
      "Epoch 104/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0766 - accuracy: 0.9741 - val_loss: 0.1154 - val_accuracy: 0.9662\n",
      "Epoch 105/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0750 - accuracy: 0.9752 - val_loss: 0.1162 - val_accuracy: 0.9656\n",
      "Epoch 106/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0755 - accuracy: 0.9746 - val_loss: 0.1180 - val_accuracy: 0.9663\n",
      "Epoch 107/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0743 - accuracy: 0.9753 - val_loss: 0.1168 - val_accuracy: 0.9667\n",
      "Epoch 108/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0733 - accuracy: 0.9753 - val_loss: 0.1190 - val_accuracy: 0.9653\n",
      "Epoch 109/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0723 - accuracy: 0.9754 - val_loss: 0.1161 - val_accuracy: 0.9667\n",
      "Epoch 110/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0716 - accuracy: 0.9755 - val_loss: 0.1163 - val_accuracy: 0.9662\n",
      "Epoch 111/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0717 - accuracy: 0.9755 - val_loss: 0.1149 - val_accuracy: 0.9666\n",
      "Epoch 112/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0707 - accuracy: 0.9763 - val_loss: 0.1191 - val_accuracy: 0.9664\n",
      "Epoch 113/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0706 - accuracy: 0.9760 - val_loss: 0.1162 - val_accuracy: 0.9671\n",
      "Epoch 114/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0703 - accuracy: 0.9759 - val_loss: 0.1160 - val_accuracy: 0.9667\n",
      "Epoch 115/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0694 - accuracy: 0.9761 - val_loss: 0.1175 - val_accuracy: 0.9659\n",
      "Epoch 116/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0691 - accuracy: 0.9766 - val_loss: 0.1157 - val_accuracy: 0.9673\n",
      "Epoch 117/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0685 - accuracy: 0.9766 - val_loss: 0.1173 - val_accuracy: 0.9653\n",
      "Epoch 118/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0671 - accuracy: 0.9772 - val_loss: 0.1169 - val_accuracy: 0.9677\n",
      "Epoch 119/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0658 - accuracy: 0.9777 - val_loss: 0.1161 - val_accuracy: 0.9675\n",
      "Epoch 120/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0670 - accuracy: 0.9770 - val_loss: 0.1156 - val_accuracy: 0.9675\n",
      "Epoch 121/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0673 - accuracy: 0.9771 - val_loss: 0.1175 - val_accuracy: 0.9673\n",
      "Epoch 122/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0654 - accuracy: 0.9777 - val_loss: 0.1170 - val_accuracy: 0.9676\n",
      "Epoch 123/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0644 - accuracy: 0.9782 - val_loss: 0.1185 - val_accuracy: 0.9674\n",
      "Epoch 124/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0647 - accuracy: 0.9775 - val_loss: 0.1169 - val_accuracy: 0.9670\n",
      "Epoch 125/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0629 - accuracy: 0.9787 - val_loss: 0.1191 - val_accuracy: 0.9679\n",
      "Epoch 126/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0641 - accuracy: 0.9786 - val_loss: 0.1207 - val_accuracy: 0.9671\n",
      "Epoch 127/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0637 - accuracy: 0.9779 - val_loss: 0.1154 - val_accuracy: 0.9681\n",
      "Epoch 128/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0623 - accuracy: 0.9786 - val_loss: 0.1176 - val_accuracy: 0.9668\n",
      "Epoch 129/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0634 - accuracy: 0.9780 - val_loss: 0.1199 - val_accuracy: 0.9674\n",
      "Epoch 130/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0633 - accuracy: 0.9782 - val_loss: 0.1190 - val_accuracy: 0.9676\n",
      "Epoch 131/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0614 - accuracy: 0.9788 - val_loss: 0.1156 - val_accuracy: 0.9686\n",
      "Epoch 132/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0599 - accuracy: 0.9791 - val_loss: 0.1175 - val_accuracy: 0.9673\n",
      "Epoch 133/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0601 - accuracy: 0.9791 - val_loss: 0.1210 - val_accuracy: 0.9676\n",
      "Epoch 134/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0600 - accuracy: 0.9791 - val_loss: 0.1191 - val_accuracy: 0.9671\n",
      "Epoch 135/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0593 - accuracy: 0.9789 - val_loss: 0.1192 - val_accuracy: 0.9671\n",
      "Epoch 136/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0579 - accuracy: 0.9799 - val_loss: 0.1178 - val_accuracy: 0.9681\n",
      "Epoch 137/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0584 - accuracy: 0.9802 - val_loss: 0.1226 - val_accuracy: 0.9670\n",
      "Epoch 138/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0579 - accuracy: 0.9798 - val_loss: 0.1188 - val_accuracy: 0.9675\n",
      "Epoch 139/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0569 - accuracy: 0.9805 - val_loss: 0.1185 - val_accuracy: 0.9674\n",
      "Epoch 140/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0572 - accuracy: 0.9806 - val_loss: 0.1215 - val_accuracy: 0.9673\n",
      "Epoch 141/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0574 - accuracy: 0.9800 - val_loss: 0.1168 - val_accuracy: 0.9679\n",
      "Epoch 142/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0554 - accuracy: 0.9810 - val_loss: 0.1184 - val_accuracy: 0.9678\n",
      "Epoch 143/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0559 - accuracy: 0.9802 - val_loss: 0.1179 - val_accuracy: 0.9678\n",
      "Epoch 144/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0555 - accuracy: 0.9807 - val_loss: 0.1184 - val_accuracy: 0.9679\n",
      "Epoch 145/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0550 - accuracy: 0.9814 - val_loss: 0.1187 - val_accuracy: 0.9680\n",
      "Epoch 146/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0541 - accuracy: 0.9816 - val_loss: 0.1193 - val_accuracy: 0.9675\n",
      "Epoch 147/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0535 - accuracy: 0.9815 - val_loss: 0.1176 - val_accuracy: 0.9686\n",
      "Epoch 148/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0549 - accuracy: 0.9806 - val_loss: 0.1223 - val_accuracy: 0.9676\n",
      "Epoch 149/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0540 - accuracy: 0.9813 - val_loss: 0.1177 - val_accuracy: 0.9691\n",
      "Epoch 150/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0526 - accuracy: 0.9819 - val_loss: 0.1195 - val_accuracy: 0.9686\n",
      "Epoch 151/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0544 - accuracy: 0.9806 - val_loss: 0.1218 - val_accuracy: 0.9670\n",
      "Epoch 152/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0522 - accuracy: 0.9818 - val_loss: 0.1223 - val_accuracy: 0.9679\n",
      "Epoch 153/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0529 - accuracy: 0.9820 - val_loss: 0.1218 - val_accuracy: 0.9673\n",
      "Epoch 154/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0526 - accuracy: 0.9818 - val_loss: 0.1208 - val_accuracy: 0.9682\n",
      "Epoch 155/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0515 - accuracy: 0.9824 - val_loss: 0.1205 - val_accuracy: 0.9678\n",
      "Epoch 156/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0510 - accuracy: 0.9823 - val_loss: 0.1206 - val_accuracy: 0.9678\n",
      "Epoch 157/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0514 - accuracy: 0.9822 - val_loss: 0.1229 - val_accuracy: 0.9681\n",
      "Epoch 158/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0511 - accuracy: 0.9815 - val_loss: 0.1214 - val_accuracy: 0.9690\n",
      "Epoch 159/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0505 - accuracy: 0.9823 - val_loss: 0.1222 - val_accuracy: 0.9680\n",
      "Epoch 160/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0506 - accuracy: 0.9830 - val_loss: 0.1241 - val_accuracy: 0.9683\n",
      "Epoch 161/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0503 - accuracy: 0.9823 - val_loss: 0.1207 - val_accuracy: 0.9680\n",
      "Epoch 162/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0499 - accuracy: 0.9831 - val_loss: 0.1244 - val_accuracy: 0.9679\n",
      "Epoch 163/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0481 - accuracy: 0.9836 - val_loss: 0.1233 - val_accuracy: 0.9689\n",
      "Epoch 164/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0491 - accuracy: 0.9830 - val_loss: 0.1229 - val_accuracy: 0.9680\n",
      "Epoch 165/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0491 - accuracy: 0.9828 - val_loss: 0.1218 - val_accuracy: 0.9682\n",
      "Epoch 166/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0485 - accuracy: 0.9833 - val_loss: 0.1232 - val_accuracy: 0.9674\n",
      "Epoch 167/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0487 - accuracy: 0.9827 - val_loss: 0.1237 - val_accuracy: 0.9684\n",
      "Epoch 168/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0485 - accuracy: 0.9831 - val_loss: 0.1213 - val_accuracy: 0.9678\n",
      "Epoch 169/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0466 - accuracy: 0.9839 - val_loss: 0.1228 - val_accuracy: 0.9684\n",
      "Epoch 170/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0474 - accuracy: 0.9831 - val_loss: 0.1226 - val_accuracy: 0.9689\n",
      "Epoch 171/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0472 - accuracy: 0.9837 - val_loss: 0.1214 - val_accuracy: 0.9677\n",
      "Epoch 172/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0481 - accuracy: 0.9832 - val_loss: 0.1203 - val_accuracy: 0.9679\n",
      "Epoch 173/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0456 - accuracy: 0.9838 - val_loss: 0.1237 - val_accuracy: 0.9681\n",
      "Epoch 174/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0463 - accuracy: 0.9840 - val_loss: 0.1238 - val_accuracy: 0.9688\n",
      "Epoch 175/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0452 - accuracy: 0.9845 - val_loss: 0.1246 - val_accuracy: 0.9683\n",
      "Epoch 176/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0453 - accuracy: 0.9842 - val_loss: 0.1206 - val_accuracy: 0.9684\n",
      "Epoch 177/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0451 - accuracy: 0.9846 - val_loss: 0.1251 - val_accuracy: 0.9681\n",
      "Epoch 178/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0449 - accuracy: 0.9847 - val_loss: 0.1227 - val_accuracy: 0.9685\n",
      "Epoch 179/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0450 - accuracy: 0.9846 - val_loss: 0.1258 - val_accuracy: 0.9682\n",
      "Epoch 180/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0451 - accuracy: 0.9845 - val_loss: 0.1228 - val_accuracy: 0.9675\n",
      "Epoch 181/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0437 - accuracy: 0.9845 - val_loss: 0.1242 - val_accuracy: 0.9686\n",
      "Epoch 182/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0439 - accuracy: 0.9845 - val_loss: 0.1236 - val_accuracy: 0.9689\n",
      "Epoch 183/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0435 - accuracy: 0.9848 - val_loss: 0.1238 - val_accuracy: 0.9683\n",
      "Epoch 184/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0441 - accuracy: 0.9848 - val_loss: 0.1268 - val_accuracy: 0.9686\n",
      "Epoch 185/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0436 - accuracy: 0.9848 - val_loss: 0.1244 - val_accuracy: 0.9686\n",
      "Epoch 186/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0427 - accuracy: 0.9848 - val_loss: 0.1257 - val_accuracy: 0.9685\n",
      "Epoch 187/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0428 - accuracy: 0.9845 - val_loss: 0.1249 - val_accuracy: 0.9682\n",
      "Epoch 188/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0425 - accuracy: 0.9850 - val_loss: 0.1240 - val_accuracy: 0.9695\n",
      "Epoch 189/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0420 - accuracy: 0.9848 - val_loss: 0.1275 - val_accuracy: 0.9686\n",
      "Epoch 190/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0437 - accuracy: 0.9846 - val_loss: 0.1240 - val_accuracy: 0.9678\n",
      "Epoch 191/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0422 - accuracy: 0.9850 - val_loss: 0.1277 - val_accuracy: 0.9684\n",
      "Epoch 192/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0430 - accuracy: 0.9849 - val_loss: 0.1275 - val_accuracy: 0.9690\n",
      "Epoch 193/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0419 - accuracy: 0.9855 - val_loss: 0.1265 - val_accuracy: 0.9680\n",
      "Epoch 194/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0421 - accuracy: 0.9848 - val_loss: 0.1276 - val_accuracy: 0.9685\n",
      "Epoch 195/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0410 - accuracy: 0.9854 - val_loss: 0.1264 - val_accuracy: 0.9680\n",
      "Epoch 196/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0420 - accuracy: 0.9851 - val_loss: 0.1317 - val_accuracy: 0.9679\n",
      "Epoch 197/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0406 - accuracy: 0.9857 - val_loss: 0.1281 - val_accuracy: 0.9685\n",
      "Epoch 198/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0405 - accuracy: 0.9855 - val_loss: 0.1258 - val_accuracy: 0.9683\n",
      "Epoch 199/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0401 - accuracy: 0.9857 - val_loss: 0.1257 - val_accuracy: 0.9686\n",
      "Epoch 200/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0404 - accuracy: 0.9858 - val_loss: 0.1272 - val_accuracy: 0.9678\n",
      "5/5 [==============================] - 1s 4ms/step\n",
      "\n",
      "Fold 9:\n",
      "X_train shape: (86692, 1, 1280)\n",
      "X_val shape: (9632, 1, 1280)\n",
      "Epoch 1/200\n",
      "43/43 [==============================] - 4s 22ms/step - loss: 1.2884 - accuracy: 0.5654 - val_loss: 0.9670 - val_accuracy: 0.6618\n",
      "Epoch 2/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.8067 - accuracy: 0.7378 - val_loss: 0.5953 - val_accuracy: 0.8253\n",
      "Epoch 3/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.5707 - accuracy: 0.8281 - val_loss: 0.4389 - val_accuracy: 0.8726\n",
      "Epoch 4/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.4723 - accuracy: 0.8591 - val_loss: 0.3622 - val_accuracy: 0.9011\n",
      "Epoch 5/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.4283 - accuracy: 0.8732 - val_loss: 0.3226 - val_accuracy: 0.9112\n",
      "Epoch 6/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3992 - accuracy: 0.8816 - val_loss: 0.2966 - val_accuracy: 0.9181\n",
      "Epoch 7/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3853 - accuracy: 0.8843 - val_loss: 0.2767 - val_accuracy: 0.9266\n",
      "Epoch 8/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3695 - accuracy: 0.8883 - val_loss: 0.2640 - val_accuracy: 0.9285\n",
      "Epoch 9/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3566 - accuracy: 0.8904 - val_loss: 0.2545 - val_accuracy: 0.9299\n",
      "Epoch 10/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3451 - accuracy: 0.8940 - val_loss: 0.2456 - val_accuracy: 0.9314\n",
      "Epoch 11/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3342 - accuracy: 0.8969 - val_loss: 0.2339 - val_accuracy: 0.9363\n",
      "Epoch 12/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3255 - accuracy: 0.8997 - val_loss: 0.2275 - val_accuracy: 0.9391\n",
      "Epoch 13/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3177 - accuracy: 0.9023 - val_loss: 0.2231 - val_accuracy: 0.9379\n",
      "Epoch 14/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3105 - accuracy: 0.9025 - val_loss: 0.2155 - val_accuracy: 0.9398\n",
      "Epoch 15/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3013 - accuracy: 0.9060 - val_loss: 0.2098 - val_accuracy: 0.9424\n",
      "Epoch 16/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.2924 - accuracy: 0.9088 - val_loss: 0.2078 - val_accuracy: 0.9399\n",
      "Epoch 17/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2841 - accuracy: 0.9126 - val_loss: 0.2004 - val_accuracy: 0.9429\n",
      "Epoch 18/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2756 - accuracy: 0.9149 - val_loss: 0.1948 - val_accuracy: 0.9439\n",
      "Epoch 19/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2669 - accuracy: 0.9182 - val_loss: 0.1891 - val_accuracy: 0.9459\n",
      "Epoch 20/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2607 - accuracy: 0.9199 - val_loss: 0.1876 - val_accuracy: 0.9453\n",
      "Epoch 21/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2526 - accuracy: 0.9217 - val_loss: 0.1815 - val_accuracy: 0.9483\n",
      "Epoch 22/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2446 - accuracy: 0.9247 - val_loss: 0.1780 - val_accuracy: 0.9480\n",
      "Epoch 23/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2411 - accuracy: 0.9266 - val_loss: 0.1735 - val_accuracy: 0.9481\n",
      "Epoch 24/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2325 - accuracy: 0.9281 - val_loss: 0.1702 - val_accuracy: 0.9500\n",
      "Epoch 25/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2261 - accuracy: 0.9308 - val_loss: 0.1673 - val_accuracy: 0.9500\n",
      "Epoch 26/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2208 - accuracy: 0.9331 - val_loss: 0.1634 - val_accuracy: 0.9511\n",
      "Epoch 27/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2125 - accuracy: 0.9349 - val_loss: 0.1619 - val_accuracy: 0.9515\n",
      "Epoch 28/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2069 - accuracy: 0.9368 - val_loss: 0.1583 - val_accuracy: 0.9516\n",
      "Epoch 29/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2032 - accuracy: 0.9377 - val_loss: 0.1545 - val_accuracy: 0.9526\n",
      "Epoch 30/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1963 - accuracy: 0.9391 - val_loss: 0.1517 - val_accuracy: 0.9530\n",
      "Epoch 31/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1947 - accuracy: 0.9401 - val_loss: 0.1522 - val_accuracy: 0.9521\n",
      "Epoch 32/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1877 - accuracy: 0.9427 - val_loss: 0.1492 - val_accuracy: 0.9539\n",
      "Epoch 33/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1844 - accuracy: 0.9431 - val_loss: 0.1452 - val_accuracy: 0.9555\n",
      "Epoch 34/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1793 - accuracy: 0.9453 - val_loss: 0.1437 - val_accuracy: 0.9561\n",
      "Epoch 35/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1755 - accuracy: 0.9463 - val_loss: 0.1420 - val_accuracy: 0.9567\n",
      "Epoch 36/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1744 - accuracy: 0.9467 - val_loss: 0.1418 - val_accuracy: 0.9565\n",
      "Epoch 37/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1690 - accuracy: 0.9471 - val_loss: 0.1389 - val_accuracy: 0.9567\n",
      "Epoch 38/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1649 - accuracy: 0.9490 - val_loss: 0.1400 - val_accuracy: 0.9573\n",
      "Epoch 39/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1623 - accuracy: 0.9502 - val_loss: 0.1371 - val_accuracy: 0.9586\n",
      "Epoch 40/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1611 - accuracy: 0.9503 - val_loss: 0.1353 - val_accuracy: 0.9582\n",
      "Epoch 41/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1572 - accuracy: 0.9513 - val_loss: 0.1357 - val_accuracy: 0.9578\n",
      "Epoch 42/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.1541 - accuracy: 0.9526 - val_loss: 0.1323 - val_accuracy: 0.9589\n",
      "Epoch 43/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1521 - accuracy: 0.9530 - val_loss: 0.1319 - val_accuracy: 0.9580\n",
      "Epoch 44/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1486 - accuracy: 0.9539 - val_loss: 0.1322 - val_accuracy: 0.9582\n",
      "Epoch 45/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1467 - accuracy: 0.9547 - val_loss: 0.1294 - val_accuracy: 0.9596\n",
      "Epoch 46/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1436 - accuracy: 0.9552 - val_loss: 0.1292 - val_accuracy: 0.9601\n",
      "Epoch 47/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1415 - accuracy: 0.9559 - val_loss: 0.1282 - val_accuracy: 0.9596\n",
      "Epoch 48/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1382 - accuracy: 0.9571 - val_loss: 0.1279 - val_accuracy: 0.9601\n",
      "Epoch 49/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1385 - accuracy: 0.9572 - val_loss: 0.1279 - val_accuracy: 0.9610\n",
      "Epoch 50/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1347 - accuracy: 0.9573 - val_loss: 0.1249 - val_accuracy: 0.9619\n",
      "Epoch 51/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1342 - accuracy: 0.9583 - val_loss: 0.1252 - val_accuracy: 0.9615\n",
      "Epoch 52/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1320 - accuracy: 0.9587 - val_loss: 0.1275 - val_accuracy: 0.9605\n",
      "Epoch 53/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1291 - accuracy: 0.9590 - val_loss: 0.1242 - val_accuracy: 0.9620\n",
      "Epoch 54/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1285 - accuracy: 0.9594 - val_loss: 0.1228 - val_accuracy: 0.9628\n",
      "Epoch 55/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1253 - accuracy: 0.9608 - val_loss: 0.1230 - val_accuracy: 0.9624\n",
      "Epoch 56/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1241 - accuracy: 0.9606 - val_loss: 0.1221 - val_accuracy: 0.9624\n",
      "Epoch 57/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1240 - accuracy: 0.9611 - val_loss: 0.1235 - val_accuracy: 0.9628\n",
      "Epoch 58/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1210 - accuracy: 0.9617 - val_loss: 0.1216 - val_accuracy: 0.9630\n",
      "Epoch 59/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1204 - accuracy: 0.9617 - val_loss: 0.1220 - val_accuracy: 0.9620\n",
      "Epoch 60/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1188 - accuracy: 0.9618 - val_loss: 0.1189 - val_accuracy: 0.9644\n",
      "Epoch 61/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1168 - accuracy: 0.9630 - val_loss: 0.1190 - val_accuracy: 0.9637\n",
      "Epoch 62/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1161 - accuracy: 0.9629 - val_loss: 0.1198 - val_accuracy: 0.9631\n",
      "Epoch 63/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1150 - accuracy: 0.9633 - val_loss: 0.1203 - val_accuracy: 0.9635\n",
      "Epoch 64/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1127 - accuracy: 0.9640 - val_loss: 0.1177 - val_accuracy: 0.9638\n",
      "Epoch 65/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1124 - accuracy: 0.9639 - val_loss: 0.1184 - val_accuracy: 0.9639\n",
      "Epoch 66/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1112 - accuracy: 0.9637 - val_loss: 0.1182 - val_accuracy: 0.9640\n",
      "Epoch 67/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1098 - accuracy: 0.9647 - val_loss: 0.1198 - val_accuracy: 0.9637\n",
      "Epoch 68/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1077 - accuracy: 0.9656 - val_loss: 0.1164 - val_accuracy: 0.9647\n",
      "Epoch 69/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1061 - accuracy: 0.9658 - val_loss: 0.1176 - val_accuracy: 0.9643\n",
      "Epoch 70/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1060 - accuracy: 0.9656 - val_loss: 0.1158 - val_accuracy: 0.9647\n",
      "Epoch 71/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1040 - accuracy: 0.9661 - val_loss: 0.1174 - val_accuracy: 0.9652\n",
      "Epoch 72/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1038 - accuracy: 0.9669 - val_loss: 0.1159 - val_accuracy: 0.9650\n",
      "Epoch 73/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1025 - accuracy: 0.9661 - val_loss: 0.1180 - val_accuracy: 0.9646\n",
      "Epoch 74/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1018 - accuracy: 0.9672 - val_loss: 0.1155 - val_accuracy: 0.9658\n",
      "Epoch 75/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1002 - accuracy: 0.9669 - val_loss: 0.1212 - val_accuracy: 0.9632\n",
      "Epoch 76/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1002 - accuracy: 0.9675 - val_loss: 0.1142 - val_accuracy: 0.9652\n",
      "Epoch 77/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0973 - accuracy: 0.9684 - val_loss: 0.1145 - val_accuracy: 0.9656\n",
      "Epoch 78/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0972 - accuracy: 0.9680 - val_loss: 0.1167 - val_accuracy: 0.9637\n",
      "Epoch 79/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0961 - accuracy: 0.9683 - val_loss: 0.1143 - val_accuracy: 0.9655\n",
      "Epoch 80/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0937 - accuracy: 0.9693 - val_loss: 0.1151 - val_accuracy: 0.9663\n",
      "Epoch 81/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0943 - accuracy: 0.9698 - val_loss: 0.1162 - val_accuracy: 0.9654\n",
      "Epoch 82/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0927 - accuracy: 0.9689 - val_loss: 0.1142 - val_accuracy: 0.9659\n",
      "Epoch 83/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0912 - accuracy: 0.9696 - val_loss: 0.1154 - val_accuracy: 0.9654\n",
      "Epoch 84/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0921 - accuracy: 0.9698 - val_loss: 0.1137 - val_accuracy: 0.9664\n",
      "Epoch 85/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0901 - accuracy: 0.9704 - val_loss: 0.1143 - val_accuracy: 0.9654\n",
      "Epoch 86/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0899 - accuracy: 0.9704 - val_loss: 0.1132 - val_accuracy: 0.9661\n",
      "Epoch 87/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0891 - accuracy: 0.9708 - val_loss: 0.1136 - val_accuracy: 0.9659\n",
      "Epoch 88/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0890 - accuracy: 0.9701 - val_loss: 0.1135 - val_accuracy: 0.9662\n",
      "Epoch 89/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0863 - accuracy: 0.9716 - val_loss: 0.1147 - val_accuracy: 0.9654\n",
      "Epoch 90/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0869 - accuracy: 0.9710 - val_loss: 0.1124 - val_accuracy: 0.9659\n",
      "Epoch 91/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0853 - accuracy: 0.9716 - val_loss: 0.1140 - val_accuracy: 0.9656\n",
      "Epoch 92/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0842 - accuracy: 0.9725 - val_loss: 0.1155 - val_accuracy: 0.9656\n",
      "Epoch 93/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0850 - accuracy: 0.9712 - val_loss: 0.1149 - val_accuracy: 0.9664\n",
      "Epoch 94/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0837 - accuracy: 0.9722 - val_loss: 0.1140 - val_accuracy: 0.9659\n",
      "Epoch 95/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0837 - accuracy: 0.9716 - val_loss: 0.1134 - val_accuracy: 0.9661\n",
      "Epoch 96/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0812 - accuracy: 0.9727 - val_loss: 0.1139 - val_accuracy: 0.9664\n",
      "Epoch 97/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0813 - accuracy: 0.9730 - val_loss: 0.1122 - val_accuracy: 0.9664\n",
      "Epoch 98/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0820 - accuracy: 0.9723 - val_loss: 0.1134 - val_accuracy: 0.9665\n",
      "Epoch 99/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0806 - accuracy: 0.9730 - val_loss: 0.1110 - val_accuracy: 0.9665\n",
      "Epoch 100/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0783 - accuracy: 0.9735 - val_loss: 0.1121 - val_accuracy: 0.9677\n",
      "Epoch 101/200\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.0779 - accuracy: 0.9736 - val_loss: 0.1133 - val_accuracy: 0.9675\n",
      "Epoch 102/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0781 - accuracy: 0.9738 - val_loss: 0.1160 - val_accuracy: 0.9671\n",
      "Epoch 103/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0767 - accuracy: 0.9742 - val_loss: 0.1130 - val_accuracy: 0.9673\n",
      "Epoch 104/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0778 - accuracy: 0.9735 - val_loss: 0.1133 - val_accuracy: 0.9672\n",
      "Epoch 105/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0767 - accuracy: 0.9738 - val_loss: 0.1158 - val_accuracy: 0.9670\n",
      "Epoch 106/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0749 - accuracy: 0.9750 - val_loss: 0.1152 - val_accuracy: 0.9671\n",
      "Epoch 107/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0756 - accuracy: 0.9746 - val_loss: 0.1122 - val_accuracy: 0.9671\n",
      "Epoch 108/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0727 - accuracy: 0.9751 - val_loss: 0.1127 - val_accuracy: 0.9675\n",
      "Epoch 109/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0734 - accuracy: 0.9748 - val_loss: 0.1141 - val_accuracy: 0.9671\n",
      "Epoch 110/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0723 - accuracy: 0.9759 - val_loss: 0.1120 - val_accuracy: 0.9679\n",
      "Epoch 111/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0722 - accuracy: 0.9751 - val_loss: 0.1137 - val_accuracy: 0.9676\n",
      "Epoch 112/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0703 - accuracy: 0.9763 - val_loss: 0.1124 - val_accuracy: 0.9677\n",
      "Epoch 113/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0709 - accuracy: 0.9758 - val_loss: 0.1132 - val_accuracy: 0.9686\n",
      "Epoch 114/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0699 - accuracy: 0.9763 - val_loss: 0.1178 - val_accuracy: 0.9668\n",
      "Epoch 115/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0691 - accuracy: 0.9761 - val_loss: 0.1145 - val_accuracy: 0.9680\n",
      "Epoch 116/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0697 - accuracy: 0.9767 - val_loss: 0.1157 - val_accuracy: 0.9673\n",
      "Epoch 117/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0681 - accuracy: 0.9766 - val_loss: 0.1123 - val_accuracy: 0.9683\n",
      "Epoch 118/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0669 - accuracy: 0.9770 - val_loss: 0.1139 - val_accuracy: 0.9682\n",
      "Epoch 119/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0672 - accuracy: 0.9772 - val_loss: 0.1152 - val_accuracy: 0.9682\n",
      "Epoch 120/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0662 - accuracy: 0.9771 - val_loss: 0.1152 - val_accuracy: 0.9676\n",
      "Epoch 121/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0660 - accuracy: 0.9775 - val_loss: 0.1132 - val_accuracy: 0.9684\n",
      "Epoch 122/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0659 - accuracy: 0.9771 - val_loss: 0.1134 - val_accuracy: 0.9682\n",
      "Epoch 123/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0661 - accuracy: 0.9773 - val_loss: 0.1164 - val_accuracy: 0.9679\n",
      "Epoch 124/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0658 - accuracy: 0.9776 - val_loss: 0.1145 - val_accuracy: 0.9683\n",
      "Epoch 125/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0654 - accuracy: 0.9776 - val_loss: 0.1163 - val_accuracy: 0.9671\n",
      "Epoch 126/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0645 - accuracy: 0.9780 - val_loss: 0.1148 - val_accuracy: 0.9679\n",
      "Epoch 127/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0636 - accuracy: 0.9782 - val_loss: 0.1137 - val_accuracy: 0.9685\n",
      "Epoch 128/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0631 - accuracy: 0.9784 - val_loss: 0.1135 - val_accuracy: 0.9681\n",
      "Epoch 129/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0628 - accuracy: 0.9781 - val_loss: 0.1134 - val_accuracy: 0.9681\n",
      "Epoch 130/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0636 - accuracy: 0.9778 - val_loss: 0.1134 - val_accuracy: 0.9684\n",
      "Epoch 131/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0613 - accuracy: 0.9793 - val_loss: 0.1145 - val_accuracy: 0.9678\n",
      "Epoch 132/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0611 - accuracy: 0.9787 - val_loss: 0.1164 - val_accuracy: 0.9681\n",
      "Epoch 133/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0614 - accuracy: 0.9790 - val_loss: 0.1171 - val_accuracy: 0.9677\n",
      "Epoch 134/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0611 - accuracy: 0.9791 - val_loss: 0.1162 - val_accuracy: 0.9683\n",
      "Epoch 135/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0605 - accuracy: 0.9789 - val_loss: 0.1139 - val_accuracy: 0.9680\n",
      "Epoch 136/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0593 - accuracy: 0.9799 - val_loss: 0.1183 - val_accuracy: 0.9688\n",
      "Epoch 137/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0602 - accuracy: 0.9793 - val_loss: 0.1178 - val_accuracy: 0.9681\n",
      "Epoch 138/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0590 - accuracy: 0.9795 - val_loss: 0.1145 - val_accuracy: 0.9679\n",
      "Epoch 139/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0582 - accuracy: 0.9800 - val_loss: 0.1196 - val_accuracy: 0.9684\n",
      "Epoch 140/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0590 - accuracy: 0.9798 - val_loss: 0.1158 - val_accuracy: 0.9692\n",
      "Epoch 141/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0566 - accuracy: 0.9805 - val_loss: 0.1180 - val_accuracy: 0.9690\n",
      "Epoch 142/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0571 - accuracy: 0.9803 - val_loss: 0.1157 - val_accuracy: 0.9693\n",
      "Epoch 143/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0566 - accuracy: 0.9802 - val_loss: 0.1166 - val_accuracy: 0.9690\n",
      "Epoch 144/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0570 - accuracy: 0.9802 - val_loss: 0.1173 - val_accuracy: 0.9694\n",
      "Epoch 145/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0543 - accuracy: 0.9813 - val_loss: 0.1162 - val_accuracy: 0.9692\n",
      "Epoch 146/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0550 - accuracy: 0.9806 - val_loss: 0.1174 - val_accuracy: 0.9689\n",
      "Epoch 147/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0559 - accuracy: 0.9806 - val_loss: 0.1208 - val_accuracy: 0.9681\n",
      "Epoch 148/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0551 - accuracy: 0.9810 - val_loss: 0.1207 - val_accuracy: 0.9686\n",
      "Epoch 149/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0551 - accuracy: 0.9809 - val_loss: 0.1168 - val_accuracy: 0.9692\n",
      "Epoch 150/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0547 - accuracy: 0.9812 - val_loss: 0.1189 - val_accuracy: 0.9699\n",
      "Epoch 151/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0539 - accuracy: 0.9811 - val_loss: 0.1186 - val_accuracy: 0.9692\n",
      "Epoch 152/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0532 - accuracy: 0.9811 - val_loss: 0.1176 - val_accuracy: 0.9695\n",
      "Epoch 153/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0534 - accuracy: 0.9808 - val_loss: 0.1173 - val_accuracy: 0.9695\n",
      "Epoch 154/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0533 - accuracy: 0.9815 - val_loss: 0.1174 - val_accuracy: 0.9699\n",
      "Epoch 155/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0524 - accuracy: 0.9816 - val_loss: 0.1192 - val_accuracy: 0.9690\n",
      "Epoch 156/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0524 - accuracy: 0.9820 - val_loss: 0.1183 - val_accuracy: 0.9696\n",
      "Epoch 157/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0520 - accuracy: 0.9822 - val_loss: 0.1176 - val_accuracy: 0.9698\n",
      "Epoch 158/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0510 - accuracy: 0.9821 - val_loss: 0.1182 - val_accuracy: 0.9696\n",
      "Epoch 159/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0510 - accuracy: 0.9823 - val_loss: 0.1175 - val_accuracy: 0.9699\n",
      "Epoch 160/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0517 - accuracy: 0.9818 - val_loss: 0.1192 - val_accuracy: 0.9695\n",
      "Epoch 161/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0514 - accuracy: 0.9823 - val_loss: 0.1206 - val_accuracy: 0.9692\n",
      "Epoch 162/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0502 - accuracy: 0.9824 - val_loss: 0.1189 - val_accuracy: 0.9694\n",
      "Epoch 163/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0488 - accuracy: 0.9831 - val_loss: 0.1202 - val_accuracy: 0.9689\n",
      "Epoch 164/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0485 - accuracy: 0.9833 - val_loss: 0.1205 - val_accuracy: 0.9695\n",
      "Epoch 165/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0486 - accuracy: 0.9826 - val_loss: 0.1194 - val_accuracy: 0.9696\n",
      "Epoch 166/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0492 - accuracy: 0.9826 - val_loss: 0.1195 - val_accuracy: 0.9690\n",
      "Epoch 167/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0481 - accuracy: 0.9829 - val_loss: 0.1201 - val_accuracy: 0.9696\n",
      "Epoch 168/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0483 - accuracy: 0.9832 - val_loss: 0.1175 - val_accuracy: 0.9696\n",
      "Epoch 169/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0472 - accuracy: 0.9834 - val_loss: 0.1186 - val_accuracy: 0.9702\n",
      "Epoch 170/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0474 - accuracy: 0.9835 - val_loss: 0.1206 - val_accuracy: 0.9699\n",
      "Epoch 171/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0482 - accuracy: 0.9831 - val_loss: 0.1228 - val_accuracy: 0.9700\n",
      "Epoch 172/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0472 - accuracy: 0.9835 - val_loss: 0.1189 - val_accuracy: 0.9703\n",
      "Epoch 173/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0472 - accuracy: 0.9835 - val_loss: 0.1192 - val_accuracy: 0.9700\n",
      "Epoch 174/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0468 - accuracy: 0.9838 - val_loss: 0.1185 - val_accuracy: 0.9704\n",
      "Epoch 175/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0468 - accuracy: 0.9835 - val_loss: 0.1220 - val_accuracy: 0.9698\n",
      "Epoch 176/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0460 - accuracy: 0.9836 - val_loss: 0.1212 - val_accuracy: 0.9694\n",
      "Epoch 177/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0449 - accuracy: 0.9841 - val_loss: 0.1215 - val_accuracy: 0.9703\n",
      "Epoch 178/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0448 - accuracy: 0.9841 - val_loss: 0.1253 - val_accuracy: 0.9694\n",
      "Epoch 179/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0445 - accuracy: 0.9845 - val_loss: 0.1216 - val_accuracy: 0.9696\n",
      "Epoch 180/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0446 - accuracy: 0.9838 - val_loss: 0.1202 - val_accuracy: 0.9700\n",
      "Epoch 181/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0435 - accuracy: 0.9848 - val_loss: 0.1221 - val_accuracy: 0.9698\n",
      "Epoch 182/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0443 - accuracy: 0.9839 - val_loss: 0.1232 - val_accuracy: 0.9696\n",
      "Epoch 183/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0450 - accuracy: 0.9843 - val_loss: 0.1221 - val_accuracy: 0.9695\n",
      "Epoch 184/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0444 - accuracy: 0.9841 - val_loss: 0.1215 - val_accuracy: 0.9702\n",
      "Epoch 185/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0447 - accuracy: 0.9842 - val_loss: 0.1280 - val_accuracy: 0.9694\n",
      "Epoch 186/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0426 - accuracy: 0.9849 - val_loss: 0.1253 - val_accuracy: 0.9696\n",
      "Epoch 187/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0430 - accuracy: 0.9850 - val_loss: 0.1236 - val_accuracy: 0.9701\n",
      "Epoch 188/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0424 - accuracy: 0.9853 - val_loss: 0.1257 - val_accuracy: 0.9698\n",
      "Epoch 189/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0437 - accuracy: 0.9848 - val_loss: 0.1234 - val_accuracy: 0.9695\n",
      "Epoch 190/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0423 - accuracy: 0.9847 - val_loss: 0.1232 - val_accuracy: 0.9693\n",
      "Epoch 191/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0431 - accuracy: 0.9850 - val_loss: 0.1227 - val_accuracy: 0.9708\n",
      "Epoch 192/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0423 - accuracy: 0.9849 - val_loss: 0.1223 - val_accuracy: 0.9704\n",
      "Epoch 193/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0416 - accuracy: 0.9853 - val_loss: 0.1268 - val_accuracy: 0.9699\n",
      "Epoch 194/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0417 - accuracy: 0.9857 - val_loss: 0.1291 - val_accuracy: 0.9694\n",
      "Epoch 195/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0410 - accuracy: 0.9856 - val_loss: 0.1281 - val_accuracy: 0.9692\n",
      "Epoch 196/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0423 - accuracy: 0.9854 - val_loss: 0.1249 - val_accuracy: 0.9699\n",
      "Epoch 197/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0408 - accuracy: 0.9854 - val_loss: 0.1255 - val_accuracy: 0.9696\n",
      "Epoch 198/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0413 - accuracy: 0.9859 - val_loss: 0.1239 - val_accuracy: 0.9696\n",
      "Epoch 199/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0414 - accuracy: 0.9853 - val_loss: 0.1251 - val_accuracy: 0.9693\n",
      "Epoch 200/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0399 - accuracy: 0.9862 - val_loss: 0.1249 - val_accuracy: 0.9694\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "\n",
      "Fold 10:\n",
      "X_train shape: (86692, 1, 1280)\n",
      "X_val shape: (9632, 1, 1280)\n",
      "Epoch 1/200\n",
      "43/43 [==============================] - 4s 21ms/step - loss: 1.2994 - accuracy: 0.5584 - val_loss: 0.9888 - val_accuracy: 0.6564\n",
      "Epoch 2/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.8210 - accuracy: 0.7311 - val_loss: 0.6137 - val_accuracy: 0.8236\n",
      "Epoch 3/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.5785 - accuracy: 0.8258 - val_loss: 0.4539 - val_accuracy: 0.8696\n",
      "Epoch 4/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.4810 - accuracy: 0.8583 - val_loss: 0.3691 - val_accuracy: 0.9033\n",
      "Epoch 5/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.4337 - accuracy: 0.8717 - val_loss: 0.3286 - val_accuracy: 0.9129\n",
      "Epoch 6/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.4051 - accuracy: 0.8812 - val_loss: 0.3005 - val_accuracy: 0.9196\n",
      "Epoch 7/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3873 - accuracy: 0.8828 - val_loss: 0.2806 - val_accuracy: 0.9250\n",
      "Epoch 8/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3706 - accuracy: 0.8871 - val_loss: 0.2672 - val_accuracy: 0.9290\n",
      "Epoch 9/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3603 - accuracy: 0.8894 - val_loss: 0.2556 - val_accuracy: 0.9332\n",
      "Epoch 10/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3486 - accuracy: 0.8924 - val_loss: 0.2465 - val_accuracy: 0.9343\n",
      "Epoch 11/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3361 - accuracy: 0.8961 - val_loss: 0.2369 - val_accuracy: 0.9357\n",
      "Epoch 12/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3281 - accuracy: 0.8983 - val_loss: 0.2291 - val_accuracy: 0.9383\n",
      "Epoch 13/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.3190 - accuracy: 0.9013 - val_loss: 0.2199 - val_accuracy: 0.9401\n",
      "Epoch 14/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3081 - accuracy: 0.9042 - val_loss: 0.2177 - val_accuracy: 0.9402\n",
      "Epoch 15/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3019 - accuracy: 0.9057 - val_loss: 0.2093 - val_accuracy: 0.9434\n",
      "Epoch 16/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2919 - accuracy: 0.9099 - val_loss: 0.2043 - val_accuracy: 0.9435\n",
      "Epoch 17/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2859 - accuracy: 0.9112 - val_loss: 0.1974 - val_accuracy: 0.9455\n",
      "Epoch 18/200\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.2771 - accuracy: 0.9145 - val_loss: 0.1953 - val_accuracy: 0.9452\n",
      "Epoch 19/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2704 - accuracy: 0.9162 - val_loss: 0.1877 - val_accuracy: 0.9482\n",
      "Epoch 20/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2616 - accuracy: 0.9183 - val_loss: 0.1828 - val_accuracy: 0.9490\n",
      "Epoch 21/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2559 - accuracy: 0.9212 - val_loss: 0.1797 - val_accuracy: 0.9495\n",
      "Epoch 22/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2473 - accuracy: 0.9236 - val_loss: 0.1744 - val_accuracy: 0.9506\n",
      "Epoch 23/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2428 - accuracy: 0.9247 - val_loss: 0.1701 - val_accuracy: 0.9515\n",
      "Epoch 24/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2374 - accuracy: 0.9267 - val_loss: 0.1676 - val_accuracy: 0.9519\n",
      "Epoch 25/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.2291 - accuracy: 0.9296 - val_loss: 0.1649 - val_accuracy: 0.9536\n",
      "Epoch 26/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2227 - accuracy: 0.9316 - val_loss: 0.1620 - val_accuracy: 0.9541\n",
      "Epoch 27/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2154 - accuracy: 0.9341 - val_loss: 0.1592 - val_accuracy: 0.9541\n",
      "Epoch 28/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2108 - accuracy: 0.9353 - val_loss: 0.1550 - val_accuracy: 0.9556\n",
      "Epoch 29/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2043 - accuracy: 0.9372 - val_loss: 0.1525 - val_accuracy: 0.9554\n",
      "Epoch 30/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2012 - accuracy: 0.9382 - val_loss: 0.1498 - val_accuracy: 0.9563\n",
      "Epoch 31/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1949 - accuracy: 0.9396 - val_loss: 0.1490 - val_accuracy: 0.9562\n",
      "Epoch 32/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1918 - accuracy: 0.9408 - val_loss: 0.1469 - val_accuracy: 0.9563\n",
      "Epoch 33/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1852 - accuracy: 0.9434 - val_loss: 0.1441 - val_accuracy: 0.9583\n",
      "Epoch 34/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1823 - accuracy: 0.9435 - val_loss: 0.1435 - val_accuracy: 0.9583\n",
      "Epoch 35/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1779 - accuracy: 0.9452 - val_loss: 0.1393 - val_accuracy: 0.9595\n",
      "Epoch 36/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1739 - accuracy: 0.9456 - val_loss: 0.1392 - val_accuracy: 0.9585\n",
      "Epoch 37/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1705 - accuracy: 0.9472 - val_loss: 0.1376 - val_accuracy: 0.9591\n",
      "Epoch 38/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1669 - accuracy: 0.9482 - val_loss: 0.1368 - val_accuracy: 0.9603\n",
      "Epoch 39/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1650 - accuracy: 0.9480 - val_loss: 0.1345 - val_accuracy: 0.9601\n",
      "Epoch 40/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1591 - accuracy: 0.9505 - val_loss: 0.1346 - val_accuracy: 0.9601\n",
      "Epoch 41/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1558 - accuracy: 0.9518 - val_loss: 0.1339 - val_accuracy: 0.9602\n",
      "Epoch 42/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1556 - accuracy: 0.9510 - val_loss: 0.1331 - val_accuracy: 0.9608\n",
      "Epoch 43/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1527 - accuracy: 0.9515 - val_loss: 0.1300 - val_accuracy: 0.9612\n",
      "Epoch 44/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1514 - accuracy: 0.9527 - val_loss: 0.1287 - val_accuracy: 0.9624\n",
      "Epoch 45/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1476 - accuracy: 0.9537 - val_loss: 0.1303 - val_accuracy: 0.9620\n",
      "Epoch 46/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1460 - accuracy: 0.9550 - val_loss: 0.1275 - val_accuracy: 0.9630\n",
      "Epoch 47/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1436 - accuracy: 0.9552 - val_loss: 0.1277 - val_accuracy: 0.9622\n",
      "Epoch 48/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1411 - accuracy: 0.9553 - val_loss: 0.1249 - val_accuracy: 0.9635\n",
      "Epoch 49/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1381 - accuracy: 0.9562 - val_loss: 0.1277 - val_accuracy: 0.9628\n",
      "Epoch 50/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1361 - accuracy: 0.9575 - val_loss: 0.1252 - val_accuracy: 0.9634\n",
      "Epoch 51/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1335 - accuracy: 0.9578 - val_loss: 0.1233 - val_accuracy: 0.9637\n",
      "Epoch 52/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1323 - accuracy: 0.9575 - val_loss: 0.1210 - val_accuracy: 0.9629\n",
      "Epoch 53/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1299 - accuracy: 0.9587 - val_loss: 0.1243 - val_accuracy: 0.9636\n",
      "Epoch 54/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1283 - accuracy: 0.9592 - val_loss: 0.1219 - val_accuracy: 0.9639\n",
      "Epoch 55/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1271 - accuracy: 0.9601 - val_loss: 0.1223 - val_accuracy: 0.9631\n",
      "Epoch 56/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1250 - accuracy: 0.9602 - val_loss: 0.1214 - val_accuracy: 0.9645\n",
      "Epoch 57/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1228 - accuracy: 0.9613 - val_loss: 0.1213 - val_accuracy: 0.9641\n",
      "Epoch 58/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1224 - accuracy: 0.9613 - val_loss: 0.1180 - val_accuracy: 0.9649\n",
      "Epoch 59/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1207 - accuracy: 0.9612 - val_loss: 0.1188 - val_accuracy: 0.9646\n",
      "Epoch 60/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1201 - accuracy: 0.9614 - val_loss: 0.1181 - val_accuracy: 0.9648\n",
      "Epoch 61/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1165 - accuracy: 0.9628 - val_loss: 0.1177 - val_accuracy: 0.9653\n",
      "Epoch 62/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1177 - accuracy: 0.9622 - val_loss: 0.1188 - val_accuracy: 0.9642\n",
      "Epoch 63/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1153 - accuracy: 0.9632 - val_loss: 0.1167 - val_accuracy: 0.9658\n",
      "Epoch 64/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1143 - accuracy: 0.9632 - val_loss: 0.1176 - val_accuracy: 0.9655\n",
      "Epoch 65/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1117 - accuracy: 0.9638 - val_loss: 0.1156 - val_accuracy: 0.9654\n",
      "Epoch 66/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1099 - accuracy: 0.9649 - val_loss: 0.1164 - val_accuracy: 0.9654\n",
      "Epoch 67/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1108 - accuracy: 0.9642 - val_loss: 0.1180 - val_accuracy: 0.9655\n",
      "Epoch 68/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1082 - accuracy: 0.9645 - val_loss: 0.1148 - val_accuracy: 0.9662\n",
      "Epoch 69/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.1060 - accuracy: 0.9657 - val_loss: 0.1151 - val_accuracy: 0.9654\n",
      "Epoch 70/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1065 - accuracy: 0.9656 - val_loss: 0.1137 - val_accuracy: 0.9670\n",
      "Epoch 71/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1052 - accuracy: 0.9662 - val_loss: 0.1138 - val_accuracy: 0.9657\n",
      "Epoch 72/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1049 - accuracy: 0.9660 - val_loss: 0.1131 - val_accuracy: 0.9662\n",
      "Epoch 73/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1026 - accuracy: 0.9673 - val_loss: 0.1137 - val_accuracy: 0.9666\n",
      "Epoch 74/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1029 - accuracy: 0.9663 - val_loss: 0.1138 - val_accuracy: 0.9651\n",
      "Epoch 75/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1016 - accuracy: 0.9670 - val_loss: 0.1138 - val_accuracy: 0.9665\n",
      "Epoch 76/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.1009 - accuracy: 0.9670 - val_loss: 0.1127 - val_accuracy: 0.9669\n",
      "Epoch 77/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0989 - accuracy: 0.9673 - val_loss: 0.1122 - val_accuracy: 0.9664\n",
      "Epoch 78/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0976 - accuracy: 0.9678 - val_loss: 0.1111 - val_accuracy: 0.9665\n",
      "Epoch 79/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0968 - accuracy: 0.9679 - val_loss: 0.1128 - val_accuracy: 0.9668\n",
      "Epoch 80/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0964 - accuracy: 0.9684 - val_loss: 0.1130 - val_accuracy: 0.9663\n",
      "Epoch 81/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0957 - accuracy: 0.9689 - val_loss: 0.1120 - val_accuracy: 0.9667\n",
      "Epoch 82/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0935 - accuracy: 0.9689 - val_loss: 0.1117 - val_accuracy: 0.9664\n",
      "Epoch 83/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0936 - accuracy: 0.9694 - val_loss: 0.1123 - val_accuracy: 0.9672\n",
      "Epoch 84/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0917 - accuracy: 0.9693 - val_loss: 0.1109 - val_accuracy: 0.9670\n",
      "Epoch 85/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0916 - accuracy: 0.9697 - val_loss: 0.1117 - val_accuracy: 0.9657\n",
      "Epoch 86/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0902 - accuracy: 0.9704 - val_loss: 0.1113 - val_accuracy: 0.9658\n",
      "Epoch 87/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0898 - accuracy: 0.9701 - val_loss: 0.1101 - val_accuracy: 0.9667\n",
      "Epoch 88/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0884 - accuracy: 0.9706 - val_loss: 0.1105 - val_accuracy: 0.9675\n",
      "Epoch 89/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0870 - accuracy: 0.9714 - val_loss: 0.1093 - val_accuracy: 0.9672\n",
      "Epoch 90/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0873 - accuracy: 0.9706 - val_loss: 0.1089 - val_accuracy: 0.9666\n",
      "Epoch 91/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0874 - accuracy: 0.9706 - val_loss: 0.1122 - val_accuracy: 0.9673\n",
      "Epoch 92/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0846 - accuracy: 0.9717 - val_loss: 0.1098 - val_accuracy: 0.9673\n",
      "Epoch 93/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0847 - accuracy: 0.9717 - val_loss: 0.1104 - val_accuracy: 0.9689\n",
      "Epoch 94/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0836 - accuracy: 0.9721 - val_loss: 0.1092 - val_accuracy: 0.9668\n",
      "Epoch 95/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0843 - accuracy: 0.9713 - val_loss: 0.1096 - val_accuracy: 0.9659\n",
      "Epoch 96/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0831 - accuracy: 0.9728 - val_loss: 0.1079 - val_accuracy: 0.9667\n",
      "Epoch 97/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0813 - accuracy: 0.9726 - val_loss: 0.1087 - val_accuracy: 0.9677\n",
      "Epoch 98/200\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.0811 - accuracy: 0.9721 - val_loss: 0.1087 - val_accuracy: 0.9669\n",
      "Epoch 99/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0801 - accuracy: 0.9730 - val_loss: 0.1115 - val_accuracy: 0.9675\n",
      "Epoch 100/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0793 - accuracy: 0.9730 - val_loss: 0.1114 - val_accuracy: 0.9675\n",
      "Epoch 101/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0791 - accuracy: 0.9740 - val_loss: 0.1106 - val_accuracy: 0.9677\n",
      "Epoch 102/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0803 - accuracy: 0.9734 - val_loss: 0.1084 - val_accuracy: 0.9683\n",
      "Epoch 103/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0763 - accuracy: 0.9741 - val_loss: 0.1091 - val_accuracy: 0.9683\n",
      "Epoch 104/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0761 - accuracy: 0.9741 - val_loss: 0.1107 - val_accuracy: 0.9684\n",
      "Epoch 105/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0762 - accuracy: 0.9743 - val_loss: 0.1112 - val_accuracy: 0.9676\n",
      "Epoch 106/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0756 - accuracy: 0.9742 - val_loss: 0.1091 - val_accuracy: 0.9688\n",
      "Epoch 107/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0751 - accuracy: 0.9745 - val_loss: 0.1086 - val_accuracy: 0.9682\n",
      "Epoch 108/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0745 - accuracy: 0.9742 - val_loss: 0.1071 - val_accuracy: 0.9685\n",
      "Epoch 109/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0735 - accuracy: 0.9753 - val_loss: 0.1099 - val_accuracy: 0.9679\n",
      "Epoch 110/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0733 - accuracy: 0.9752 - val_loss: 0.1119 - val_accuracy: 0.9679\n",
      "Epoch 111/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0726 - accuracy: 0.9757 - val_loss: 0.1078 - val_accuracy: 0.9669\n",
      "Epoch 112/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0706 - accuracy: 0.9763 - val_loss: 0.1078 - val_accuracy: 0.9682\n",
      "Epoch 113/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0707 - accuracy: 0.9757 - val_loss: 0.1074 - val_accuracy: 0.9686\n",
      "Epoch 114/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0696 - accuracy: 0.9767 - val_loss: 0.1097 - val_accuracy: 0.9685\n",
      "Epoch 115/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0705 - accuracy: 0.9767 - val_loss: 0.1105 - val_accuracy: 0.9682\n",
      "Epoch 116/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0704 - accuracy: 0.9760 - val_loss: 0.1069 - val_accuracy: 0.9695\n",
      "Epoch 117/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0702 - accuracy: 0.9763 - val_loss: 0.1073 - val_accuracy: 0.9682\n",
      "Epoch 118/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0688 - accuracy: 0.9759 - val_loss: 0.1126 - val_accuracy: 0.9675\n",
      "Epoch 119/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0682 - accuracy: 0.9771 - val_loss: 0.1101 - val_accuracy: 0.9681\n",
      "Epoch 120/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0672 - accuracy: 0.9769 - val_loss: 0.1094 - val_accuracy: 0.9680\n",
      "Epoch 121/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0659 - accuracy: 0.9773 - val_loss: 0.1086 - val_accuracy: 0.9682\n",
      "Epoch 122/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0673 - accuracy: 0.9765 - val_loss: 0.1098 - val_accuracy: 0.9682\n",
      "Epoch 123/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0662 - accuracy: 0.9776 - val_loss: 0.1090 - val_accuracy: 0.9685\n",
      "Epoch 124/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0656 - accuracy: 0.9778 - val_loss: 0.1101 - val_accuracy: 0.9682\n",
      "Epoch 125/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0643 - accuracy: 0.9776 - val_loss: 0.1109 - val_accuracy: 0.9680\n",
      "Epoch 126/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0632 - accuracy: 0.9780 - val_loss: 0.1092 - val_accuracy: 0.9679\n",
      "Epoch 127/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0635 - accuracy: 0.9781 - val_loss: 0.1094 - val_accuracy: 0.9688\n",
      "Epoch 128/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0633 - accuracy: 0.9781 - val_loss: 0.1078 - val_accuracy: 0.9684\n",
      "Epoch 129/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0634 - accuracy: 0.9782 - val_loss: 0.1128 - val_accuracy: 0.9671\n",
      "Epoch 130/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0613 - accuracy: 0.9792 - val_loss: 0.1095 - val_accuracy: 0.9686\n",
      "Epoch 131/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0625 - accuracy: 0.9787 - val_loss: 0.1110 - val_accuracy: 0.9689\n",
      "Epoch 132/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0606 - accuracy: 0.9788 - val_loss: 0.1095 - val_accuracy: 0.9683\n",
      "Epoch 133/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0613 - accuracy: 0.9786 - val_loss: 0.1119 - val_accuracy: 0.9681\n",
      "Epoch 134/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0612 - accuracy: 0.9794 - val_loss: 0.1125 - val_accuracy: 0.9673\n",
      "Epoch 135/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0607 - accuracy: 0.9793 - val_loss: 0.1099 - val_accuracy: 0.9679\n",
      "Epoch 136/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0591 - accuracy: 0.9791 - val_loss: 0.1074 - val_accuracy: 0.9688\n",
      "Epoch 137/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0578 - accuracy: 0.9804 - val_loss: 0.1115 - val_accuracy: 0.9682\n",
      "Epoch 138/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0586 - accuracy: 0.9795 - val_loss: 0.1122 - val_accuracy: 0.9680\n",
      "Epoch 139/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0577 - accuracy: 0.9804 - val_loss: 0.1097 - val_accuracy: 0.9681\n",
      "Epoch 140/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0594 - accuracy: 0.9790 - val_loss: 0.1101 - val_accuracy: 0.9680\n",
      "Epoch 141/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0581 - accuracy: 0.9796 - val_loss: 0.1110 - val_accuracy: 0.9683\n",
      "Epoch 142/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0549 - accuracy: 0.9812 - val_loss: 0.1107 - val_accuracy: 0.9679\n",
      "Epoch 143/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0573 - accuracy: 0.9801 - val_loss: 0.1125 - val_accuracy: 0.9673\n",
      "Epoch 144/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0562 - accuracy: 0.9809 - val_loss: 0.1108 - val_accuracy: 0.9677\n",
      "Epoch 145/200\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.0566 - accuracy: 0.9800 - val_loss: 0.1113 - val_accuracy: 0.9678\n",
      "Epoch 146/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0550 - accuracy: 0.9809 - val_loss: 0.1100 - val_accuracy: 0.9675\n",
      "Epoch 147/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0544 - accuracy: 0.9813 - val_loss: 0.1107 - val_accuracy: 0.9679\n",
      "Epoch 148/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0552 - accuracy: 0.9807 - val_loss: 0.1130 - val_accuracy: 0.9689\n",
      "Epoch 149/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0543 - accuracy: 0.9808 - val_loss: 0.1139 - val_accuracy: 0.9686\n",
      "Epoch 150/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0537 - accuracy: 0.9812 - val_loss: 0.1109 - val_accuracy: 0.9685\n",
      "Epoch 151/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0532 - accuracy: 0.9814 - val_loss: 0.1097 - val_accuracy: 0.9688\n",
      "Epoch 152/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0531 - accuracy: 0.9814 - val_loss: 0.1146 - val_accuracy: 0.9689\n",
      "Epoch 153/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0519 - accuracy: 0.9818 - val_loss: 0.1125 - val_accuracy: 0.9683\n",
      "Epoch 154/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0525 - accuracy: 0.9818 - val_loss: 0.1129 - val_accuracy: 0.9688\n",
      "Epoch 155/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0524 - accuracy: 0.9820 - val_loss: 0.1096 - val_accuracy: 0.9682\n",
      "Epoch 156/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0505 - accuracy: 0.9827 - val_loss: 0.1121 - val_accuracy: 0.9691\n",
      "Epoch 157/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0526 - accuracy: 0.9814 - val_loss: 0.1120 - val_accuracy: 0.9681\n",
      "Epoch 158/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0515 - accuracy: 0.9827 - val_loss: 0.1137 - val_accuracy: 0.9688\n",
      "Epoch 159/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0508 - accuracy: 0.9822 - val_loss: 0.1129 - val_accuracy: 0.9693\n",
      "Epoch 160/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0505 - accuracy: 0.9825 - val_loss: 0.1131 - val_accuracy: 0.9685\n",
      "Epoch 161/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0494 - accuracy: 0.9828 - val_loss: 0.1143 - val_accuracy: 0.9684\n",
      "Epoch 162/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0489 - accuracy: 0.9826 - val_loss: 0.1140 - val_accuracy: 0.9692\n",
      "Epoch 163/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0494 - accuracy: 0.9826 - val_loss: 0.1118 - val_accuracy: 0.9685\n",
      "Epoch 164/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0498 - accuracy: 0.9829 - val_loss: 0.1124 - val_accuracy: 0.9694\n",
      "Epoch 165/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0480 - accuracy: 0.9834 - val_loss: 0.1151 - val_accuracy: 0.9699\n",
      "Epoch 166/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0480 - accuracy: 0.9829 - val_loss: 0.1157 - val_accuracy: 0.9692\n",
      "Epoch 167/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0488 - accuracy: 0.9828 - val_loss: 0.1147 - val_accuracy: 0.9683\n",
      "Epoch 168/200\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0480 - accuracy: 0.9831 - val_loss: 0.1111 - val_accuracy: 0.9690\n",
      "Epoch 169/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0482 - accuracy: 0.9835 - val_loss: 0.1141 - val_accuracy: 0.9688\n",
      "Epoch 170/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0478 - accuracy: 0.9829 - val_loss: 0.1143 - val_accuracy: 0.9692\n",
      "Epoch 171/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0473 - accuracy: 0.9834 - val_loss: 0.1154 - val_accuracy: 0.9672\n",
      "Epoch 172/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0468 - accuracy: 0.9834 - val_loss: 0.1132 - val_accuracy: 0.9686\n",
      "Epoch 173/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0465 - accuracy: 0.9837 - val_loss: 0.1142 - val_accuracy: 0.9691\n",
      "Epoch 174/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0462 - accuracy: 0.9837 - val_loss: 0.1131 - val_accuracy: 0.9692\n",
      "Epoch 175/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0466 - accuracy: 0.9837 - val_loss: 0.1137 - val_accuracy: 0.9688\n",
      "Epoch 176/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0463 - accuracy: 0.9840 - val_loss: 0.1130 - val_accuracy: 0.9685\n",
      "Epoch 177/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0455 - accuracy: 0.9838 - val_loss: 0.1159 - val_accuracy: 0.9686\n",
      "Epoch 178/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0447 - accuracy: 0.9842 - val_loss: 0.1146 - val_accuracy: 0.9685\n",
      "Epoch 179/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0440 - accuracy: 0.9846 - val_loss: 0.1136 - val_accuracy: 0.9678\n",
      "Epoch 180/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0453 - accuracy: 0.9839 - val_loss: 0.1150 - val_accuracy: 0.9691\n",
      "Epoch 181/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0445 - accuracy: 0.9842 - val_loss: 0.1136 - val_accuracy: 0.9684\n",
      "Epoch 182/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0456 - accuracy: 0.9839 - val_loss: 0.1171 - val_accuracy: 0.9675\n",
      "Epoch 183/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0450 - accuracy: 0.9840 - val_loss: 0.1145 - val_accuracy: 0.9697\n",
      "Epoch 184/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0442 - accuracy: 0.9847 - val_loss: 0.1167 - val_accuracy: 0.9690\n",
      "Epoch 185/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0443 - accuracy: 0.9845 - val_loss: 0.1155 - val_accuracy: 0.9694\n",
      "Epoch 186/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0434 - accuracy: 0.9849 - val_loss: 0.1143 - val_accuracy: 0.9690\n",
      "Epoch 187/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0428 - accuracy: 0.9846 - val_loss: 0.1143 - val_accuracy: 0.9698\n",
      "Epoch 188/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0434 - accuracy: 0.9845 - val_loss: 0.1145 - val_accuracy: 0.9688\n",
      "Epoch 189/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0415 - accuracy: 0.9858 - val_loss: 0.1173 - val_accuracy: 0.9690\n",
      "Epoch 190/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0427 - accuracy: 0.9850 - val_loss: 0.1170 - val_accuracy: 0.9694\n",
      "Epoch 191/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0413 - accuracy: 0.9852 - val_loss: 0.1168 - val_accuracy: 0.9699\n",
      "Epoch 192/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0430 - accuracy: 0.9849 - val_loss: 0.1200 - val_accuracy: 0.9689\n",
      "Epoch 193/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0422 - accuracy: 0.9850 - val_loss: 0.1155 - val_accuracy: 0.9683\n",
      "Epoch 194/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0415 - accuracy: 0.9855 - val_loss: 0.1174 - val_accuracy: 0.9684\n",
      "Epoch 195/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0414 - accuracy: 0.9852 - val_loss: 0.1198 - val_accuracy: 0.9694\n",
      "Epoch 196/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0402 - accuracy: 0.9860 - val_loss: 0.1187 - val_accuracy: 0.9696\n",
      "Epoch 197/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0415 - accuracy: 0.9855 - val_loss: 0.1182 - val_accuracy: 0.9690\n",
      "Epoch 198/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0409 - accuracy: 0.9854 - val_loss: 0.1196 - val_accuracy: 0.9686\n",
      "Epoch 199/200\n",
      "43/43 [==============================] - 0s 9ms/step - loss: 0.0417 - accuracy: 0.9853 - val_loss: 0.1211 - val_accuracy: 0.9685\n",
      "Epoch 200/200\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0394 - accuracy: 0.9860 - val_loss: 0.1211 - val_accuracy: 0.9691\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "\n",
      "Train Over\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the dataset.\n",
    "dataset = load_and_preprocess_data()\n",
    "\n",
    "# Create a Pandas Excel writer\n",
    "excel_writer = pd.ExcelWriter('output/cv_labels.xlsx', engine='xlsxwriter')\n",
    "best_val_accuracy = 0.0\n",
    "best_model_path = ''\n",
    "folder_path = \"model/\"\n",
    "\n",
    "# StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(dataset.iloc[:, 3:], dataset['label'])):\n",
    "    print(f\"\\nFold {fold + 1}:\")\n",
    "\n",
    "    train_data, vali_data = dataset.iloc[train_index], dataset.iloc[val_index]\n",
    "\n",
    "    X_train = reshape_features(train_data.iloc[:, 3:])\n",
    "    X_val = reshape_features(vali_data.iloc[:, 3:])\n",
    "\n",
    "    print(f'X_train shape: {X_train.shape}')\n",
    "    print(f'X_val shape: {X_val.shape}')\n",
    "\n",
    "    gru_attention_model = md.GRUWithAttentionModel(input_shape=cfg.INPUT_SHAPE, num_classes=cfg.NUM_CLASSES)\n",
    "    gru_attention_model.compile_model()\n",
    "\n",
    "    history = gru_attention_model.train(\n",
    "        X_train,\n",
    "        train_data['label'],\n",
    "        X_val,\n",
    "        vali_data['label'],\n",
    "        batch_size=cfg.BATCH_SIZE,\n",
    "        epochs=cfg.EPOCHS\n",
    "    )\n",
    "\n",
    "    # Track and save only the best model\n",
    "    if history.history['val_accuracy'][-1] > best_val_accuracy:\n",
    "        best_val_accuracy = history.history['val_accuracy'][-1]\n",
    "        gru_attention_model.save_model(f'{folder_path}deepsub_new.h5')\n",
    "\n",
    "    # eval\n",
    "    val_predictions = gru_attention_model.model.predict(X_val, batch_size=cfg.BATCH_SIZE)\n",
    "    ground_truth_labels = vali_data['label'].values\n",
    "    predicted_labels = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "    # Export data to different sheets\n",
    "    export_data = pd.DataFrame({'GroundTruth': ground_truth_labels, 'PredictedLabels': predicted_labels})\n",
    "    sheet_name = f'fold{fold}'\n",
    "    export_data.to_excel(excel_writer, sheet_name=sheet_name, index=False)\n",
    "excel_writer.close()\n",
    "print(\"\\nTrain Over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCC</th>\n",
       "      <th>ACC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.953663</td>\n",
       "      <td>0.969791</td>\n",
       "      <td>0.882330</td>\n",
       "      <td>0.861123</td>\n",
       "      <td>0.871499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.953849</td>\n",
       "      <td>0.969895</td>\n",
       "      <td>0.945905</td>\n",
       "      <td>0.957866</td>\n",
       "      <td>0.948209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.953390</td>\n",
       "      <td>0.969584</td>\n",
       "      <td>0.975601</td>\n",
       "      <td>0.915527</td>\n",
       "      <td>0.937140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.954002</td>\n",
       "      <td>0.969999</td>\n",
       "      <td>0.883070</td>\n",
       "      <td>0.857063</td>\n",
       "      <td>0.869544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.956566</td>\n",
       "      <td>0.971657</td>\n",
       "      <td>0.977411</td>\n",
       "      <td>0.963762</td>\n",
       "      <td>0.970427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.955449</td>\n",
       "      <td>0.970930</td>\n",
       "      <td>0.879162</td>\n",
       "      <td>0.870081</td>\n",
       "      <td>0.874509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.953522</td>\n",
       "      <td>0.969684</td>\n",
       "      <td>0.876028</td>\n",
       "      <td>0.862424</td>\n",
       "      <td>0.869061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.950627</td>\n",
       "      <td>0.967816</td>\n",
       "      <td>0.879039</td>\n",
       "      <td>0.855708</td>\n",
       "      <td>0.866916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.953055</td>\n",
       "      <td>0.969373</td>\n",
       "      <td>0.979616</td>\n",
       "      <td>0.963825</td>\n",
       "      <td>0.971456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.952552</td>\n",
       "      <td>0.969061</td>\n",
       "      <td>0.879688</td>\n",
       "      <td>0.863517</td>\n",
       "      <td>0.871243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MCC       ACC  Precision    Recall        F1\n",
       "0  0.953663  0.969791   0.882330  0.861123  0.871499\n",
       "1  0.953849  0.969895   0.945905  0.957866  0.948209\n",
       "2  0.953390  0.969584   0.975601  0.915527  0.937140\n",
       "3  0.954002  0.969999   0.883070  0.857063  0.869544\n",
       "4  0.956566  0.971657   0.977411  0.963762  0.970427\n",
       "5  0.955449  0.970930   0.879162  0.870081  0.874509\n",
       "6  0.953522  0.969684   0.876028  0.862424  0.869061\n",
       "7  0.950627  0.967816   0.879039  0.855708  0.866916\n",
       "8  0.953055  0.969373   0.979616  0.963825  0.971456\n",
       "9  0.952552  0.969061   0.879688  0.863517  0.871243"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, matthews_corrcoef, confusion_matrix\n",
    "\n",
    "results = []\n",
    "for fold in range(10):\n",
    "    fold_data = pd.read_excel('output/cv_labels.xlsx', sheet_name=f'fold{fold}')\n",
    "\n",
    "    ground_truth_labels = fold_data['GroundTruth'].values\n",
    "    predicted_labels = fold_data['PredictedLabels'].values\n",
    "\n",
    "    mcc = matthews_corrcoef(ground_truth_labels, predicted_labels)\n",
    "    acc = accuracy_score(ground_truth_labels, predicted_labels) \n",
    "    precision = precision_score(ground_truth_labels, predicted_labels, zero_division=0, average='macro')\n",
    "    recall = recall_score(ground_truth_labels, predicted_labels, zero_division=0, average='macro')\n",
    "    f1 = f1_score(ground_truth_labels, predicted_labels, zero_division=0, average='macro')\n",
    "\n",
    "    results.append([mcc, acc, precision, recall, f1])\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=['MCC', 'ACC', 'Precision', 'Recall', 'F1'])\n",
    "\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCC</th>\n",
       "      <th>ACC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.953663</td>\n",
       "      <td>0.969791</td>\n",
       "      <td>0.88233</td>\n",
       "      <td>0.861123</td>\n",
       "      <td>0.871499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.953849</td>\n",
       "      <td>0.969895</td>\n",
       "      <td>0.945905</td>\n",
       "      <td>0.957866</td>\n",
       "      <td>0.948209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.95339</td>\n",
       "      <td>0.969584</td>\n",
       "      <td>0.975601</td>\n",
       "      <td>0.915527</td>\n",
       "      <td>0.93714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.954002</td>\n",
       "      <td>0.969999</td>\n",
       "      <td>0.88307</td>\n",
       "      <td>0.857063</td>\n",
       "      <td>0.869544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.956566</td>\n",
       "      <td>0.971657</td>\n",
       "      <td>0.977411</td>\n",
       "      <td>0.963762</td>\n",
       "      <td>0.970427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.955449</td>\n",
       "      <td>0.97093</td>\n",
       "      <td>0.879162</td>\n",
       "      <td>0.870081</td>\n",
       "      <td>0.874509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.953522</td>\n",
       "      <td>0.969684</td>\n",
       "      <td>0.876028</td>\n",
       "      <td>0.862424</td>\n",
       "      <td>0.869061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.950627</td>\n",
       "      <td>0.967816</td>\n",
       "      <td>0.879039</td>\n",
       "      <td>0.855708</td>\n",
       "      <td>0.866916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.953055</td>\n",
       "      <td>0.969373</td>\n",
       "      <td>0.979616</td>\n",
       "      <td>0.963825</td>\n",
       "      <td>0.971456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.952552</td>\n",
       "      <td>0.969061</td>\n",
       "      <td>0.879688</td>\n",
       "      <td>0.863517</td>\n",
       "      <td>0.871243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average  Std</th>\n",
       "      <td>0.954  0.002</td>\n",
       "      <td>0.97  0.001</td>\n",
       "      <td>0.916  0.047</td>\n",
       "      <td>0.897  0.048</td>\n",
       "      <td>0.905  0.046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         MCC           ACC      Precision         Recall  \\\n",
       "0                   0.953663      0.969791        0.88233       0.861123   \n",
       "1                   0.953849      0.969895       0.945905       0.957866   \n",
       "2                    0.95339      0.969584       0.975601       0.915527   \n",
       "3                   0.954002      0.969999        0.88307       0.857063   \n",
       "4                   0.956566      0.971657       0.977411       0.963762   \n",
       "5                   0.955449       0.97093       0.879162       0.870081   \n",
       "6                   0.953522      0.969684       0.876028       0.862424   \n",
       "7                   0.950627      0.967816       0.879039       0.855708   \n",
       "8                   0.953055      0.969373       0.979616       0.963825   \n",
       "9                   0.952552      0.969061       0.879688       0.863517   \n",
       "Average  Std  0.954  0.002  0.97  0.001  0.916  0.047  0.897  0.048   \n",
       "\n",
       "                          F1  \n",
       "0                   0.871499  \n",
       "1                   0.948209  \n",
       "2                    0.93714  \n",
       "3                   0.869544  \n",
       "4                   0.970427  \n",
       "5                   0.874509  \n",
       "6                   0.869061  \n",
       "7                   0.866916  \n",
       "8                   0.971456  \n",
       "9                   0.871243  \n",
       "Average  Std  0.905  0.046  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_values = df_results.mean()\n",
    "std_values = df_results.std()\n",
    "\n",
    "avg_std_row = round(mean_values,3).astype(str) + \"  \" + round(std_values,3).astype(str)\n",
    "\n",
    "avg_std_df = pd.DataFrame([avg_std_row], columns=df_results.columns, index=['Average  Std'])\n",
    "\n",
    "df_results_with_avg_std = pd.concat([df_results, avg_std_df])\n",
    "\n",
    "df_results_with_avg_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_with_avg_std.to_csv(\"cv_report.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Accuracy: 0.9698 (Std Dev: 0.0010)\n",
      "Average MCC: 0.9537 (Std Dev: 0.0015)\n",
      "Average Recall: 0.8971 (Std Dev: 0.0454)\n",
      "Average F1 Score: 0.9050 (Std Dev: 0.0433)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "all_accuracies, all_mccs, all_recalls, all_f1_scores, all_recall_macro = [], [], [], [], []\n",
    "\n",
    "# Loop through each fold\n",
    "for fold in range(10):\n",
    "    # Calculate metrics for each fold\n",
    "    accuracy = accuracy_score(all_ground_truth_labels[fold], all_predicted_labels[fold])\n",
    "    mcc = matthews_corrcoef(all_ground_truth_labels[fold], all_predicted_labels[fold])\n",
    "    recall = recall_score(all_ground_truth_labels[fold], all_predicted_labels[fold], average='macro')\n",
    "    f1 = f1_score(all_ground_truth_labels[fold], all_predicted_labels[fold], average='macro')\n",
    "    macro =  pd.DataFrame(classification_report(all_ground_truth_labels[fold], all_predicted_labels[fold], zero_division=0, output_dict=True)).T['recall']['macro avg']\n",
    "    \n",
    "    # Append metrics to lists\n",
    "    all_accuracies.append(accuracy)\n",
    "    all_mccs.append(mcc)\n",
    "    all_recalls.append(recall)\n",
    "    all_f1_scores.append(f1)\n",
    "    all_recall_macro.append(macro)\n",
    "\n",
    "# Calculate averages and standard deviations\n",
    "average_accuracy = np.mean(all_accuracies)\n",
    "std_accuracy = np.std(all_accuracies)\n",
    "\n",
    "average_mcc = np.mean(all_mccs)\n",
    "std_mcc = np.std(all_mccs)\n",
    "\n",
    "average_recall = np.mean(all_recalls)\n",
    "std_recall = np.std(all_recalls)\n",
    "\n",
    "average_f1 = np.mean(all_f1_scores)\n",
    "std_f1 = np.std(all_f1_scores)\n",
    "\n",
    "average_macro = np.mean(all_recall_macro)\n",
    "std_macro = np.std(all_recall_macro)\n",
    "\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy:.4f} (Std Dev: {std_accuracy:.4f})\")\n",
    "print(f\"Average MCC: {average_mcc:.4f} (Std Dev: {std_mcc:.4f})\")\n",
    "print(f\"Average Recall: {average_recall:.4f} (Std Dev: {std_recall:.4f})\")\n",
    "print(f\"Average F1 Score: {average_f1:.4f} (Std Dev: {std_f1:.4f})\")\n",
    "# print(f\"Average Recall Macro: {average_macro:.4f} (Std Dev: {std_macro:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.960685</td>\n",
       "      <td>0.945632</td>\n",
       "      <td>0.953099</td>\n",
       "      <td>4212.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.967294</td>\n",
       "      <td>0.981680</td>\n",
       "      <td>0.974434</td>\n",
       "      <td>10153.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.968545</td>\n",
       "      <td>0.941401</td>\n",
       "      <td>0.954780</td>\n",
       "      <td>785.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.967613</td>\n",
       "      <td>0.950327</td>\n",
       "      <td>0.958892</td>\n",
       "      <td>2295.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.971223</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>141.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.978776</td>\n",
       "      <td>0.981178</td>\n",
       "      <td>0.979975</td>\n",
       "      <td>1222.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.985612</td>\n",
       "      <td>0.895425</td>\n",
       "      <td>0.938356</td>\n",
       "      <td>153.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990244</td>\n",
       "      <td>0.995098</td>\n",
       "      <td>205.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.927083</td>\n",
       "      <td>0.951872</td>\n",
       "      <td>96.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.967246</td>\n",
       "      <td>0.967246</td>\n",
       "      <td>0.967246</td>\n",
       "      <td>0.967246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.977777</td>\n",
       "      <td>0.890375</td>\n",
       "      <td>0.917079</td>\n",
       "      <td>19265.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.967247</td>\n",
       "      <td>0.967246</td>\n",
       "      <td>0.967141</td>\n",
       "      <td>19265.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score       support\n",
       "0              0.960685  0.945632  0.953099   4212.000000\n",
       "1              0.967294  0.981680  0.974434  10153.000000\n",
       "2              0.968545  0.941401  0.954780    785.000000\n",
       "3              0.967613  0.950327  0.958892   2295.000000\n",
       "4              0.971223  0.957447  0.964286    141.000000\n",
       "5              0.978776  0.981178  0.979975   1222.000000\n",
       "6              1.000000  0.333333  0.500000      3.000000\n",
       "7              0.985612  0.895425  0.938356    153.000000\n",
       "8              1.000000  0.990244  0.995098    205.000000\n",
       "9              0.978022  0.927083  0.951872     96.000000\n",
       "accuracy       0.967246  0.967246  0.967246      0.967246\n",
       "macro avg      0.977777  0.890375  0.917079  19265.000000\n",
       "weighted avg   0.967247  0.967246  0.967141  19265.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata = pd.read_csv(\"./output/groundtruth_and_labels.csv\")\n",
    "from sklearn.metrics import classification_report\n",
    "blast_report = pd.DataFrame(classification_report(rawdata[\"GroundTruth\"], rawdata[\"PredictedLabels\"], zero_division=0, output_dict=True)).T\n",
    "blast_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ground_truth_labels = np.concatenate(all_ground_truth_labels)\n",
    "all_predicted_labels = np.concatenate(all_predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Classification Report:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.965575</td>\n",
       "      <td>0.952279</td>\n",
       "      <td>0.958881</td>\n",
       "      <td>21060.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.968355</td>\n",
       "      <td>0.983196</td>\n",
       "      <td>0.975719</td>\n",
       "      <td>50763.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.972718</td>\n",
       "      <td>0.945436</td>\n",
       "      <td>0.958883</td>\n",
       "      <td>3922.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.971023</td>\n",
       "      <td>0.955029</td>\n",
       "      <td>0.962960</td>\n",
       "      <td>11474.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.978663</td>\n",
       "      <td>0.973126</td>\n",
       "      <td>0.975887</td>\n",
       "      <td>707.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.986192</td>\n",
       "      <td>0.970054</td>\n",
       "      <td>0.978056</td>\n",
       "      <td>6111.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.972145</td>\n",
       "      <td>0.911227</td>\n",
       "      <td>0.940701</td>\n",
       "      <td>766.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.993097</td>\n",
       "      <td>0.980526</td>\n",
       "      <td>0.986771</td>\n",
       "      <td>1027.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>480.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.969779</td>\n",
       "      <td>0.969779</td>\n",
       "      <td>0.969779</td>\n",
       "      <td>0.969779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.962386</td>\n",
       "      <td>0.897802</td>\n",
       "      <td>0.920396</td>\n",
       "      <td>96324.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.969796</td>\n",
       "      <td>0.969779</td>\n",
       "      <td>0.969704</td>\n",
       "      <td>96324.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score       support\n",
       "0              0.965575  0.952279  0.958881  21060.000000\n",
       "1              0.968355  0.983196  0.975719  50763.000000\n",
       "2              0.972718  0.945436  0.958883   3922.000000\n",
       "3              0.971023  0.955029  0.962960  11474.000000\n",
       "4              0.978663  0.973126  0.975887    707.000000\n",
       "5              0.986192  0.970054  0.978056   6111.000000\n",
       "6              0.833333  0.357143  0.500000     14.000000\n",
       "7              0.972145  0.911227  0.940701    766.000000\n",
       "8              0.993097  0.980526  0.986771   1027.000000\n",
       "9              0.982759  0.950000  0.966102    480.000000\n",
       "accuracy       0.969779  0.969779  0.969779      0.969779\n",
       "macro avg      0.962386  0.897802  0.920396  96324.000000\n",
       "weighted avg   0.969796  0.969779  0.969704  96324.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nOverall Classification Report:\")\n",
    "report = pd.DataFrame(classification_report(all_ground_truth_labels, all_predicted_labels,output_dict=True)).T\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biobase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
