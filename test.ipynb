{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: DengRui\n",
    "Date: 2023-09-11 02:10:21\n",
    "LastEditors: DengRui\n",
    "LastEditTime: 2023-09-11 02:20:39\n",
    "FilePath: /DeepSub/dataprocess/get_train_test.ipynb\n",
    "Description:  get train and test data\n",
    "Copyright (c) 2023 by DengRui, All Rights Reserved. \n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import sys,os\n",
    "from tool.att import Attention\n",
    "from keras.models import load_model\n",
    "import esm\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import logging\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set gpu\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from fasta \n",
    "url = \"https://rest.uniprot.org/uniprotkb/P08499.fasta\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    fasta_data = response.text\n",
    "else:\n",
    "    fasta_data = \"Failed to download data. Status code: \" + str(response.status_code)\n",
    "    \n",
    "header, *sequence_lines = fasta_data.split('\\n')\n",
    "uniprot_id = header.split('|')[1]\n",
    "sequence = ''.join(sequence_lines)\n",
    "\n",
    "dataset = pd.DataFrame({'uniprot_id': [uniprot_id], 'seq': [sequence]})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esm2 embedding\n",
    "def get_rep_seq(sequences):\n",
    "\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(sequences)\n",
    "    batch_tokens = batch_tokens.to(device)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "    token_representations = results[\"representations\"][33]\n",
    "    sequence_representations = []\n",
    "    for i, tokens_len in enumerate(batch_lens):\n",
    "        sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "       \n",
    "    np_list = []\n",
    "\n",
    "    for i, ten in enumerate(sequence_representations):\n",
    "        ten=ten.cpu().detach().numpy()\n",
    "        np_list.append(ten)\n",
    "    res = pd.DataFrame(np_list)\n",
    "    res.columns = ['f'+str(i) for i in range (0,res.shape[1])]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "df_data = list(zip(dataset.uniprot_id.index,dataset.seq))\n",
    "\n",
    "# Run in batches\n",
    "stride =2\n",
    "num_iterations = len(df_data) // stride\n",
    "if len(df_data) % stride != 0:\n",
    "    num_iterations += 1\n",
    "    \n",
    "# Embedding\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(num_iterations)):\n",
    "    \n",
    "    start = i * stride\n",
    "    end = start + stride\n",
    "\n",
    "    current_data = df_data[start:end]\n",
    "\n",
    "    rep33 = get_rep_seq(sequences=current_data)\n",
    "    rep33['uniprot_id'] = dataset[start:end].uniprot_id.tolist()\n",
    "    cols = list(rep33.columns)\n",
    "    cols = [cols[-1]] + cols[:-1]\n",
    "    rep33 = rep33[cols]\n",
    "    all_results = pd.concat([all_results, rep33], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the predicted labels:\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "# Deepsub\n",
    "model = load_model(\"./model/deepsub_20240120.h5\",custom_objects={\"Attention\": Attention},compile=False)\n",
    "predicted = model.predict(np.array(all_results.iloc[:,1:]).reshape(all_results.shape[0],1,-1))\n",
    "predicted_labels = np.argmax(predicted, axis=1)\n",
    "label_map = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 10, 9: 12}\n",
    "y_test_transformed = [label_map[x] for x in predicted_labels]\n",
    "print(\"These are the predicted labels:\")\n",
    "print(y_test_transformed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
